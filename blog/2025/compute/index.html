<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures | Debjit Paul </title> <meta name="author" content="Debjit Paul"> <meta name="description" content="A comprehensive guide to counting FLOPs in LLM training, measuring Model FLOPs Utilization (MFU), and extending these concepts to Mixture-of-Experts architectures with a deep dive into OpenAI's GPT-OSS models."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://debjitpaul.github.io/blog/2025/compute/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Debjit</span> Paul </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures</h1> <p class="post-meta"> Created on September 25, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/flops"> <i class="fa-solid fa-hashtag fa-sm"></i> flops</a>   <a href="/blog/tag/mfu"> <i class="fa-solid fa-hashtag fa-sm"></i> mfu</a>   <a href="/blog/tag/llm-training"> <i class="fa-solid fa-hashtag fa-sm"></i> llm-training</a>   <a href="/blog/tag/moe"> <i class="fa-solid fa-hashtag fa-sm"></i> moe</a>   <a href="/blog/tag/gpt-oss"> <i class="fa-solid fa-hashtag fa-sm"></i> gpt-oss</a>   <a href="/blog/tag/computational-efficiency"> <i class="fa-solid fa-hashtag fa-sm"></i> computational-efficiency</a>   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   <a href="/blog/category/technical"> <i class="fa-solid fa-tag fa-sm"></i> technical</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>As Large Language Models (LLMs) continue to scale exponentially, understanding their computational requirements becomes crucial for researchers and practitioners alike. Whether you’re planning a training run, optimizing infrastructure costs, or comparing different architectures, <strong>FLOPs</strong> (Floating Point Operations) and <strong>MFU</strong> (Model FLOPs Utilization) are essential metrics that provide hardware-independent ways to measure and optimize computational efficiency.</p> <p>This post builds upon two excellent foundational resources: <strong><a href="https://www.adamcasson.com/posts/transformer-flops" rel="external nofollow noopener" target="_blank">Adam Casson’s comprehensive guide to Transformer FLOPs</a></strong> and <strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48" rel="external nofollow noopener" target="_blank">Pratish Raj’s practical guide to FLOPs in LLM training</a></strong>. While these posts masterfully cover dense Transformer architectures, this guide extends their insights to modern Mixture-of-Experts (MoE) architectures, with a particular focus on OpenAI’s recently released GPT-OSS models.</p> <p>In this comprehensive analysis, we’ll explore FLOP counting methodologies, dive deep into MFU calculations, and examine how these concepts apply to the latest generation of sparse models that are reshaping the efficiency landscape of large language models.</p> <h2 id="-what-are-flops-and-why-do-they-matter">🔢 What Are FLOPs and Why Do They Matter?</h2> <h3 id="defining-flops">Defining FLOPs</h3> <p>A <strong>FLOP</strong> (Floating Point Operation) represents a single computational operation like addition (<code class="language-plaintext highlighter-rouge">3.5 + 4.2</code>) or multiplication (<code class="language-plaintext highlighter-rouge">1.1 × 2.6</code>). When we talk about <strong>FLOPs</strong> (plural), we’re counting the total number of these atomic operations required for a specific task.</p> <p>For large-scale computations, we use:</p> <ul> <li> <strong>GFLOPs</strong> = 1 billion FLOPs (10⁹)</li> <li> <strong>TFLOPs</strong> = 1 trillion FLOPs (10¹²)</li> <li> <strong>PFLOPs</strong> = 1 quadrillion FLOPs (10¹⁵)</li> </ul> <h3 id="why-flops-matter">Why FLOPs Matter</h3> <p>FLOPs provide several crucial advantages:</p> <ol> <li> <strong>Hardware Independence</strong>: Unlike wall-clock time, FLOPs offer consistent measurements across different hardware configurations</li> <li> <strong>Reproducibility</strong>: Enable precise comparisons between different models and training setups</li> <li> <strong>Cost Estimation</strong>: Help predict computational costs and resource requirements</li> <li> <strong>Efficiency Analysis</strong>: Allow measurement of how well we utilize available hardware</li> </ol> <h2 id="-counting-flops-in-dense-transformers">📐 Counting FLOPs in Dense Transformers</h2> <h3 id="the-openai-method">The OpenAI Method</h3> <p>The seminal approach from OpenAI’s scaling laws paper provides a clean approximation:</p> <p><strong>FLOPs per token ≈ 6N</strong></p> <p>Where <code class="language-plaintext highlighter-rouge">N</code> is the number of non-embedding parameters. This factor of 6 accounts for:</p> <ul> <li> <strong>2×</strong> for the forward pass (multiply-accumulate operations)</li> <li> <strong>2×</strong> for the backward pass (gradients with respect to inputs)</li> <li> <strong>2×</strong> for the backward pass (gradients with respect to parameters)</li> </ul> <p>Let’s break down the forward pass components:</p> <table> <thead> <tr> <th>Operation</th> <th>Parameters</th> <th>FLOPs per Token</th> </tr> </thead> <tbody> <tr> <td><strong>Attention QKV</strong></td> <td><code class="language-plaintext highlighter-rouge">3 × d × d_model</code></td> <td><code class="language-plaintext highlighter-rouge">6 × L × d_model × d</code></td> </tr> <tr> <td><strong>Attention Scores</strong></td> <td>-</td> <td><code class="language-plaintext highlighter-rouge">4 × L × seq_len × d</code></td> </tr> <tr> <td><strong>Attention Project</strong></td> <td><code class="language-plaintext highlighter-rouge">d × d_model</code></td> <td><code class="language-plaintext highlighter-rouge">2 × L × d × d_model</code></td> </tr> <tr> <td><strong>Feedforward</strong></td> <td><code class="language-plaintext highlighter-rouge">8 × d_model²</code></td> <td><code class="language-plaintext highlighter-rouge">16 × L × d_model²</code></td> </tr> <tr> <td><strong>Total (approx)</strong></td> <td>-</td> <td><code class="language-plaintext highlighter-rouge">≈ 2 × N</code></td> </tr> </tbody> </table> <p>Where:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">L</code> = number of layers</li> <li> <code class="language-plaintext highlighter-rouge">seq_len</code> = sequence length</li> <li> <code class="language-plaintext highlighter-rouge">d_model</code> = hidden dimension</li> <li> <code class="language-plaintext highlighter-rouge">d</code> = attention dimension (<code class="language-plaintext highlighter-rouge">d_model</code> for most implementations)</li> </ul> <h3 id="the-deepmind-method">The DeepMind Method</h3> <p>DeepMind’s Chinchilla paper provides a more detailed accounting that includes embeddings, logits, and attention mechanics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">deepmind_flops_per_sequence</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">ff_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">DeepMind method for forward pass FLOPs counting</span><span class="sh">"""</span>
    <span class="n">d_attn</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
    <span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">ff_ratio</span>

    <span class="c1"># Components
</span>    <span class="n">embeddings</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="n">d_model</span>
    <span class="n">attn_qkv</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_softmax</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">attn_reduce</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_project</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_model</span>

    <span class="n">total_attn</span> <span class="o">=</span> <span class="n">attn_qkv</span> <span class="o">+</span> <span class="n">attn_logits</span> <span class="o">+</span> <span class="n">attn_softmax</span> <span class="o">+</span> <span class="n">attn_reduce</span> <span class="o">+</span> <span class="n">attn_project</span>
    <span class="n">feedforward</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">vocab_size</span>

    <span class="k">return</span> <span class="n">embeddings</span> <span class="o">+</span> <span class="n">n_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">total_attn</span> <span class="o">+</span> <span class="n">feedforward</span><span class="p">)</span> <span class="o">+</span> <span class="n">logits</span>
</code></pre></div></div> <h3 id="practical-example-gpt-3-scale-model">Practical Example: GPT-3 Scale Model</h3> <p>Let’s calculate FLOPs for a GPT-3 scale model:</p> <ul> <li> <strong>Parameters</strong>: 175B non-embedding</li> <li> <strong>Sequence length</strong>: 2048</li> <li> <strong>Batch size</strong>: 32</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using OpenAI approximation
</span><span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="mf">175e9</span>  <span class="c1"># 1.05 × 10^12 FLOPs
</span><span class="n">tokens_per_step</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">2048</span>  <span class="c1"># 65,536 tokens
</span><span class="n">flops_per_step</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">tokens_per_step</span>  <span class="c1"># 6.87 × 10^16 FLOPs
</span>
<span class="c1"># Forward + Backward
</span><span class="n">total_flops_per_step</span> <span class="o">=</span> <span class="n">flops_per_step</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># 1.37 × 10^17 FLOPs ≈ 137 PFLOPs
</span></code></pre></div></div> <h2 id="-model-flops-utilization-mfu-the-gold-standard">⚡ Model FLOPs Utilization (MFU): The Gold Standard</h2> <h3 id="understanding-mfu">Understanding MFU</h3> <p><strong>Model FLOPs Utilization (MFU)</strong> measures how efficiently we execute the theoretically necessary FLOPs for training, introduced in Google’s PaLM paper:</p> <p><strong>MFU = (Model FLOPs × Throughput) / Peak Hardware FLOPs</strong></p> <p>Where:</p> <ul> <li> <strong>Model FLOPs</strong>: Theoretical FLOPs per token (using OpenAI’s 6N approximation)</li> <li> <strong>Throughput</strong>: Observed tokens processed per second</li> <li> <strong>Peak Hardware FLOPs</strong>: Theoretical maximum FLOP/s of your hardware</li> </ul> <h3 id="mfu-vs-hardware-flops-utilization-hfu">MFU vs. Hardware FLOPs Utilization (HFU)</h3> <table> <thead> <tr> <th>Metric</th> <th><strong>MFU</strong></th> <th><strong>HFU</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Includes</strong></td> <td>Only necessary model computations</td> <td>All computations (including overheads)</td> </tr> <tr> <td><strong>Use Case</strong></td> <td>Fair comparison across setups</td> <td>Implementation efficiency</td> </tr> <tr> <td><strong>Affected by</strong></td> <td>Model architecture, batch size</td> <td>Memory management, communication</td> </tr> </tbody> </table> <h3 id="calculating-mfu-a-practical-example">Calculating MFU: A Practical Example</h3> <p>Consider training a 7B parameter model on 8×A100 GPUs:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model specifications
</span><span class="n">parameters</span> <span class="o">=</span> <span class="mf">7e9</span>
<span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">parameters</span>  <span class="c1"># 4.2 × 10^10
</span>
<span class="c1"># Hardware specifications
</span><span class="n">a100_peak_flops</span> <span class="o">=</span> <span class="mf">312e12</span>  <span class="c1"># 312 TFLOPs for bf16
</span><span class="n">total_peak_flops</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">a100_peak_flops</span>  <span class="c1"># 2.496 × 10^15
</span>
<span class="c1"># Measured throughput
</span><span class="n">tokens_per_second</span> <span class="o">=</span> <span class="mi">8000</span>

<span class="c1"># Calculate MFU
</span><span class="n">sustained_flops</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">tokens_per_second</span>  <span class="c1"># 3.36 × 10^14
</span><span class="n">mfu</span> <span class="o">=</span> <span class="n">sustained_flops</span> <span class="o">/</span> <span class="n">total_peak_flops</span>  <span class="c1"># 0.135 = 13.5%
</span></code></pre></div></div> <h3 id="typical-mfu-ranges">Typical MFU Ranges</h3> <p>Real-world MFU values vary significantly:</p> <ul> <li> <strong>Small models (&lt; 1B)</strong>: 10-30%</li> <li> <strong>Medium models (1B-10B)</strong>: 30-50%</li> <li> <strong>Large models (10B-100B)</strong>: 45-65%</li> <li> <strong>Very large models (100B+)</strong>: 50-70%</li> </ul> <p>Higher MFU in larger models is often due to better compute-to-communication ratios and improved memory bandwidth utilization.</p> <h2 id="-extending-to-mixture-of-experts-moe-architectures">🧠 Extending to Mixture-of-Experts (MoE) Architectures</h2> <h3 id="moe-fundamentals">MoE Fundamentals</h3> <p>Mixture-of-Experts architectures replace dense feedforward networks with a router and multiple expert networks. Only a subset of experts (typically 1-2 out of 8-64) are activated per token, dramatically reducing computational costs while maintaining or improving model quality.</p> <h3 id="moe-flop-counting-challenges">MoE FLOP Counting Challenges</h3> <p>Traditional FLOP counting becomes more nuanced with MoE:</p> <ol> <li> <strong>Routing Overhead</strong>: Additional FLOPs for expert selection</li> <li> <strong>Variable Computation</strong>: Different tokens may use different experts</li> <li> <strong>Load Balancing</strong>: Uneven expert utilization affects total FLOPs</li> <li> <strong>Communication Costs</strong>: Expert routing across devices</li> </ol> <h3 id="moe-flop-formula">MoE FLOP Formula</h3> <p>For a MoE layer replacing a dense feedforward network:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">moe_flops_per_token</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span> <span class="n">experts_per_token</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">load_balance_factor</span><span class="o">=</span><span class="mf">1.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Calculate FLOPs for MoE layer</span><span class="sh">"""</span>

    <span class="c1"># Router FLOPs (token → expert probabilities)
</span>    <span class="n">router_flops</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">num_experts</span>

    <span class="c1"># Expert FLOPs (only active experts)
</span>    <span class="n">expert_flops</span> <span class="o">=</span> <span class="n">experts_per_token</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Load balancing overhead
</span>    <span class="n">effective_expert_flops</span> <span class="o">=</span> <span class="n">expert_flops</span> <span class="o">*</span> <span class="n">load_balance_factor</span>

    <span class="k">return</span> <span class="n">router_flops</span> <span class="o">+</span> <span class="n">effective_expert_flops</span>

<span class="c1"># Example: 8 experts, top-2 routing
</span><span class="n">dense_ff_flops</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">*</span> <span class="n">d_model</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># 4x expansion, two linear layers
</span><span class="n">moe_ff_flops</span> <span class="o">=</span> <span class="nf">moe_flops_per_token</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">efficiency_gain</span> <span class="o">=</span> <span class="n">dense_ff_flops</span> <span class="o">/</span> <span class="n">moe_ff_flops</span>  <span class="c1"># Typically 2-4x
</span></code></pre></div></div> <h2 id="-case-study-openai-gpt-oss-models">🚀 Case Study: OpenAI GPT-OSS Models</h2> <p>OpenAI’s GPT-OSS models provide an excellent real-world example of modern MoE architectures with several innovative features:</p> <h3 id="gpt-oss-architecture-overview">GPT-OSS Architecture Overview</h3> <p><strong>GPT-OSS-120B</strong>:</p> <ul> <li> <strong>Total Parameters</strong>: 117B (5.1B active per token)</li> <li> <strong>Architecture</strong>: MoE with native MXFP4 quantization</li> <li> <strong>Activation</strong>: ~4.4% of total parameters per token</li> <li> <strong>Memory</strong>: Fits on single H100 (80GB)</li> </ul> <p><strong>GPT-OSS-20B</strong>:</p> <ul> <li> <strong>Total Parameters</strong>: 21B (3.6B active per token)</li> <li> <strong>Activation</strong>: ~17% of total parameters per token</li> <li> <strong>Memory</strong>: Runs in 16GB</li> </ul> <h3 id="flop-counting-for-gpt-oss">FLOP Counting for GPT-OSS</h3> <p>Let’s calculate FLOPs for GPT-OSS-120B:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gpt_oss_flops_calculation</span><span class="p">():</span>
    <span class="c1"># GPT-OSS-120B specifications (estimated)
</span>    <span class="n">total_params</span> <span class="o">=</span> <span class="mf">117e9</span>
    <span class="n">active_params_per_token</span> <span class="o">=</span> <span class="mf">5.1e9</span>

    <span class="c1"># Traditional approach (if it were dense)
</span>    <span class="n">dense_flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">total_params</span>  <span class="c1"># 702 GFLOPs
</span>
    <span class="c1"># MoE approach (actual)
</span>    <span class="c1"># Attention layers remain dense
</span>    <span class="n">attention_params</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mf">0.3</span>  <span class="c1"># Estimated 30%
</span>    <span class="n">attention_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">attention_params</span>

    <span class="c1"># MoE feedforward layers
</span>    <span class="n">moe_params_per_token</span> <span class="o">=</span> <span class="n">active_params_per_token</span> <span class="o">-</span> <span class="n">attention_params</span>
    <span class="n">moe_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">moe_params_per_token</span>

    <span class="c1"># Router overhead (small)
</span>    <span class="n">router_flops</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c1"># Estimated 0.1%
</span>
    <span class="n">total_flops_per_token</span> <span class="o">=</span> <span class="n">attention_flops</span> <span class="o">+</span> <span class="n">moe_flops</span> <span class="o">+</span> <span class="n">router_flops</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">dense_equivalent</span><span class="sh">'</span><span class="p">:</span> <span class="n">dense_flops_per_token</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">actual_moe</span><span class="sh">'</span><span class="p">:</span> <span class="n">total_flops_per_token</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">efficiency_gain</span><span class="sh">'</span><span class="p">:</span> <span class="n">dense_flops_per_token</span> <span class="o">/</span> <span class="n">total_flops_per_token</span>
    <span class="p">}</span>

<span class="n">results</span> <span class="o">=</span> <span class="nf">gpt_oss_flops_calculation</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dense equivalent: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">dense_equivalent</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">e</span><span class="si">}</span><span class="s"> FLOPs/token</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">MoE actual: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">actual_moe</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">e</span><span class="si">}</span><span class="s"> FLOPs/token</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Efficiency gain: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">efficiency_gain</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">x</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="mxfp4-quantization-impact">MXFP4 Quantization Impact</h3> <p>GPT-OSS uses native MXFP4 quantization for MoE layers:</p> <ul> <li> <strong>Memory</strong>: 4-bit storage vs 16-bit (4× reduction)</li> <li> <strong>Compute</strong>: Specialized kernels maintain computational efficiency</li> <li> <strong>Accuracy</strong>: Minimal degradation with proper scaling</li> </ul> <p>This affects FLOP counting considerations:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quantized_moe_flops</span><span class="p">(</span><span class="n">base_flops</span><span class="p">,</span> <span class="n">quantization_overhead</span><span class="o">=</span><span class="mf">1.05</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    MXFP4 might have slight computational overhead
    but significant memory bandwidth benefits
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">base_flops</span> <span class="o">*</span> <span class="n">quantization_overhead</span>
</code></pre></div></div> <h3 id="performance-analysis">Performance Analysis</h3> <p>Based on the GPT-OSS specifications, we can estimate performance characteristics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Estimated GPT-OSS-120B on H100
</span><span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="mf">5.1e9</span>  <span class="c1"># 30.6 GFLOPs (active parameters)
</span><span class="n">h100_peak</span> <span class="o">=</span> <span class="mf">1980e12</span>  <span class="c1"># ~2 PFLOPs for int4 operations
</span><span class="n">target_throughput</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># tokens/second (estimated)
</span>
<span class="n">required_flops_rate</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">target_throughput</span>  <span class="c1"># 3.06 TFLOPs/s
</span><span class="n">mfu</span> <span class="o">=</span> <span class="n">required_flops_rate</span> <span class="o">/</span> <span class="n">h100_peak</span>  <span class="c1"># 0.15%
</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Estimated MFU for GPT-OSS-120B: </span><span class="si">{</span><span class="n">mfu</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><em>Note: These are rough estimates. Actual performance depends on implementation details, memory bandwidth, and other factors.</em></p> <h2 id="-practical-considerations-and-optimizations">🔧 Practical Considerations and Optimizations</h2> <h3 id="memory-vs-compute-trade-offs">Memory vs. Compute Trade-offs</h3> <p>Modern LLM training involves several techniques that affect FLOP calculations:</p> <p><strong>Activation Checkpointing</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">checkpointing_flop_overhead</span><span class="p">(</span><span class="n">base_flops</span><span class="p">,</span> <span class="n">checkpoint_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Recompute activations during backward pass
    checkpoint_ratio: fraction of activations checkpointed
    </span><span class="sh">"""</span>
    <span class="n">recompute_flops</span> <span class="o">=</span> <span class="n">base_flops</span> <span class="o">*</span> <span class="n">checkpoint_ratio</span>
    <span class="k">return</span> <span class="n">base_flops</span> <span class="o">+</span> <span class="n">recompute_flops</span>  <span class="c1"># Up to 1.5× FLOPs
</span></code></pre></div></div> <p><strong>Gradient Accumulation</strong>:</p> <ul> <li>Doesn’t affect per-token FLOPs</li> <li>May affect MFU due to different memory access patterns</li> </ul> <h3 id="scaling-laws-and-flop-optimal-training">Scaling Laws and FLOP-Optimal Training</h3> <p>The relationship between model size, dataset size, and computational budget follows predictable scaling laws:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_optimal_scaling</span><span class="p">(</span><span class="n">compute_budget_flops</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Based on Chinchilla scaling laws
    Roughly: parameters ∝ compute^0.5, tokens ∝ compute^0.5
    </span><span class="sh">"""</span>
    <span class="n">optimal_params</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_budget_flops</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">optimal_tokens</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_budget_flops</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>

    <span class="k">return</span> <span class="n">optimal_params</span><span class="p">,</span> <span class="n">optimal_tokens</span>

<span class="c1"># Example: 10^23 FLOPs budget
</span><span class="n">params</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nf">compute_optimal_scaling</span><span class="p">(</span><span class="mf">1e23</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal: </span><span class="si">{</span><span class="n">params</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">B parameters, </span><span class="si">{</span><span class="n">tokens</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">B tokens</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-benchmarking-and-measurement-tools">📊 Benchmarking and Measurement Tools</h2> <h3 id="measuring-mfu-in-practice">Measuring MFU in Practice</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">measure_mfu</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Measure MFU during actual training</span><span class="sh">"""</span>
    <span class="n">device</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span>

    <span class="c1"># Create dummy batch
</span>    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50000</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Warm up
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">).</span><span class="n">loss</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">).</span><span class="n">loss</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="c1"># Calculate throughput
</span>    <span class="n">total_tokens</span> <span class="o">=</span> <span class="n">num_steps</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">throughput</span> <span class="o">=</span> <span class="n">total_tokens</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

    <span class="c1"># Calculate MFU
</span>    <span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">model</span><span class="p">.</span><span class="nf">num_parameters</span><span class="p">()</span>
    <span class="n">peak_flops</span> <span class="o">=</span> <span class="nf">get_device_peak_flops</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mfu</span> <span class="o">=</span> <span class="p">(</span><span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">throughput</span><span class="p">)</span> <span class="o">/</span> <span class="n">peak_flops</span>

    <span class="k">return</span> <span class="n">mfu</span><span class="p">,</span> <span class="n">throughput</span>
</code></pre></div></div> <h3 id="profiling-tools">Profiling Tools</h3> <ul> <li> <strong>PyTorch Profiler</strong>: Built-in FLOP counting</li> <li> <strong>DeepSpeed</strong>: MFU reporting in training logs</li> <li> <strong>Weights &amp; Biases</strong>: Integration with hardware metrics</li> <li> <strong>Custom counters</strong>: Framework-specific implementations</li> </ul> <h2 id="-future-directions-and-emerging-architectures">🔮 Future Directions and Emerging Architectures</h2> <h3 id="beyond-traditional-flops">Beyond Traditional FLOPs</h3> <p>As architectures evolve, FLOP counting methodologies must adapt:</p> <p><strong>Sparse Attention Patterns</strong>:</p> <ul> <li>Linear attention: O(n) instead of O(n²)</li> <li>Local attention: Reduced sequence length dependencies</li> <li>Mixture of attention heads: Different patterns per head</li> </ul> <p><strong>Conditional Computation</strong>:</p> <ul> <li>Early exit mechanisms</li> <li>Adaptive depth networks</li> <li>Token-wise routing</li> </ul> <p><strong>Hardware-Aware Metrics</strong>:</p> <ul> <li>Memory bandwidth utilization</li> <li>Integer operation efficiency</li> <li>Specialized accelerator metrics</li> </ul> <h3 id="the-road-ahead">The Road Ahead</h3> <p>The future of computational efficiency in LLMs likely involves:</p> <ol> <li> <strong>Multi-modal MoE</strong>: Extending sparse computation to different modalities</li> <li> <strong>Dynamic architectures</strong>: Runtime adaptation based on input complexity</li> <li> <strong>Hardware co-design</strong>: Models designed for specific accelerators</li> <li> <strong>Hybrid precision</strong>: Strategic use of different numerical formats</li> </ol> <h2 id="-conclusion">🏁 Conclusion</h2> <p>Understanding FLOPs and MFU is crucial for anyone working with large-scale language models. As we’ve seen:</p> <ul> <li> <strong>FLOPs provide hardware-independent measurement</strong> of computational requirements</li> <li> <strong>MFU enables fair comparison</strong> of training efficiency across different setups</li> <li> <strong>MoE architectures complicate</strong> but don’t invalidate these fundamental concepts</li> <li> <strong>Modern models like GPT-OSS</strong> showcase how innovative architectures and quantization can dramatically improve efficiency</li> </ul> <p>Whether you’re planning a training run, optimizing an existing system, or designing new architectures, these metrics provide essential insights into the computational reality of modern AI systems.</p> <p>As models continue to scale and new architectures emerge, the principles of FLOP counting and efficiency measurement will remain foundational tools for understanding and optimizing the computational landscape of artificial intelligence.</p> <hr> <h2 id="-acknowledgments">🙏 Acknowledgments</h2> <p>This blog post draws heavily from two excellent resources that provided the foundation for understanding FLOPs in LLM training.</p> <p><strong>Primary Inspirations:</strong></p> <ul> <li> <p><strong><a href="https://www.adamcasson.com/posts/transformer-flops" rel="external nofollow noopener" target="_blank">Adam Casson’s “Transformer FLOPs”</a></strong> - An exceptionally clear and comprehensive guide that masterfully explains FLOP counting methodologies, MFU calculations, and scaling behaviors. Much of the mathematical foundation and practical examples in this post are built upon Adam’s excellent work.</p> </li> <li> <p><strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48" rel="external nofollow noopener" target="_blank">Pratish Raj’s “FLOPs in LLM Training: The Ultimate Guide”</a></strong> - A practical and accessible guide that bridges the gap between theory and implementation, providing clear examples and real-world context for FLOP calculations.</p> </li> </ul> <p>Both posts were instrumental in shaping my understanding of computational efficiency in LLMs. This work extends their insights to MoE architectures and provides additional analysis of modern models like GPT-OSS, but the core concepts and methodologies owe much to these foundational resources.</p> <h2 id="-references-and-further-reading">📚 References and Further Reading</h2> <p><strong>Foundational Papers:</strong></p> <ul> <li> <strong><a href="https://arxiv.org/abs/2001.08361" rel="external nofollow noopener" target="_blank">OpenAI Scaling Laws</a></strong>: Kaplan et al. - Original FLOP counting methodology</li> <li> <strong><a href="https://arxiv.org/abs/2203.15556" rel="external nofollow noopener" target="_blank">DeepMind Chinchilla</a></strong>: Hoffmann et al. - Detailed FLOP accounting and compute-optimal training</li> <li> <strong><a href="https://arxiv.org/abs/2204.02311" rel="external nofollow noopener" target="_blank">Google PaLM</a></strong>: Chowdhery et al. - MFU methodology introduction</li> </ul> <p><strong>Key Blog Posts and Resources:</strong></p> <ul> <li> <strong><a href="https://www.adamcasson.com/posts/transformer-flops" rel="external nofollow noopener" target="_blank">Adam Casson’s Transformer FLOPs</a></strong>: Comprehensive FLOP analysis with interactive calculator</li> <li> <strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48" rel="external nofollow noopener" target="_blank">Pratish Raj’s FLOPs Guide</a></strong>: Practical guide to FLOPs in LLM training</li> <li> <strong><a href="https://github.com/openai/gpt-oss" rel="external nofollow noopener" target="_blank">GPT-OSS Repository</a></strong>: OpenAI’s open-source MoE implementation</li> </ul> <p><strong>Additional Technical Resources:</strong></p> <ul> <li> <strong><a href="https://kipp.ly/blog/transformer-inference-arithmetic/" rel="external nofollow noopener" target="_blank">Transformer Inference Arithmetic</a></strong>: Kipp Bradford’s analysis of inference costs</li> <li> <strong><a href="https://horace.io/brrr_intro.html" rel="external nofollow noopener" target="_blank">Making Deep Learning Go Brrrr</a></strong>: Horace He’s guide to optimization fundamentals</li> </ul> <hr> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">paul2024flops_mfu_moe</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">{https://debjitpaul.github.io/blog/}</span>
<span class="p">}</span>
</code></pre></div></div> <hr> <p><em>This analysis provides a comprehensive overview of computational efficiency in modern LLM training. For specific implementation details or model-specific calculations, always refer to the original papers and codebases.</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/reasoning-matter/">Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/">blog</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Debjit Paul. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>