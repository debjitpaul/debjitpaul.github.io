<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> FIPO: Fallacy-Informed Preference Optimization - Steering LLMs Toward Logically Sound Arguments | Debjit Paul </title> <meta name="author" content="Debjit Paul"> <meta name="description" content="🏆 Outstanding Paper Award Winner at NAACL 2025! Introducing FIPO, a novel framework that reduces logical fallacy errors in LLM-generated arguments by up to 17.5% through fallacy-informed preference optimization."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://debjitpaul.github.io/blog/2025/fipo-outstanding-paper-award/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Debjit</span> Paul </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">FIPO: Fallacy-Informed Preference Optimization - Steering LLMs Toward Logically Sound Arguments</h1> <p class="post-meta"> Created on January 15, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/fipo"> <i class="fa-solid fa-hashtag fa-sm"></i> fipo</a>   <a href="/blog/tag/logical-fallacies"> <i class="fa-solid fa-hashtag fa-sm"></i> logical-fallacies</a>   <a href="/blog/tag/preference-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> preference-optimization</a>   <a href="/blog/tag/argument-generation"> <i class="fa-solid fa-hashtag fa-sm"></i> argument-generation</a>   <a href="/blog/tag/outstanding-paper"> <i class="fa-solid fa-hashtag fa-sm"></i> outstanding-paper</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   <a href="/blog/category/awards"> <i class="fa-solid fa-tag fa-sm"></i> awards</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>🏆 <strong>Outstanding Paper Award Winner at NAACL 2025!</strong></p> <p>I’m thrilled to share that our paper “A Logical Fallacy-Informed Framework for Argument Generation” has received the <strong>Outstanding Paper Award</strong> at the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2025)!</p> <h2 id="-the-problem-llms-generate-fallacious-arguments">🚨 The Problem: LLMs Generate Fallacious Arguments</h2> <p>Despite their remarkable capabilities, Large Language Models (LLMs) still struggle with generating logically sound arguments, often producing content that contains <strong>logical fallacies</strong> - errors in reasoning that undermine the validity of an argument. Our preliminary study revealed a startling finding: <strong>21% of arguments generated by ChatGPT contain logical fallacies</strong>.</p> <p>Consider these examples of fallacious vs. logically sound arguments:</p> <table> <thead> <tr> <th> <strong>Topic</strong>: AI is bad for the job market (Support)</th> </tr> </thead> <tbody> <tr> <td>❌ <strong>Ad Hominem</strong>: “AI proponents are tech enthusiasts disconnected from real-world job market issues.”</td> </tr> <tr> <td>❌ <strong>Circular Reasoning</strong>: “AI is harmful to the employment sector because AI is bad for the job market.”</td> </tr> <tr> <td>❌ <strong>Appeal to Emotion</strong>: “AI will leave millions of families struggling and unemployed.”</td> </tr> <tr> <td>✅ <strong>Logically Sound</strong>: “AI automates tasks, reducing employment opportunities by replacing humans in manufacturing and administrative jobs.”</td> </tr> </tbody> </table> <h2 id="-our-solution-fallacy-informed-preference-optimization-fipo">💡 Our Solution: Fallacy-Informed Preference Optimization (FIPO)</h2> <p>We introduce <strong>FIPO</strong>, a novel framework that helps steer LLMs toward generating logically sound arguments by making them explicitly aware of logical fallacies during training.</p> <h3 id="key-innovation-weighted-classification-loss">Key Innovation: Weighted Classification Loss</h3> <p>FIPO combines traditional preference optimization with a <strong>weighted cross-entropy classification loss</strong> that penalizes the model based on fallacy frequency, applying stronger penalties for misclassifying more common fallacies.</p> <p>The FIPO loss function is:</p> <p><strong>L<sub>FIPO</sub>(π<sub>θ</sub>) = L<sub>CPO</sub>(π<sub>θ</sub>) + λL<sub>CLF</sub>(π<sub>θ</sub>)</strong></p> <p>Where the classification loss L<sub>CLF</sub> is defined as:</p> <table> <tbody> <tr> <td>**L<sub>CLF</sub>(π<sub>θ</sub>) = -E<sub>(t,s,y<sub>w</sub>,y<sub>l</sub>,k)∼D’</sub>[w<sub>0</sub> log P<sup>0</sup><sub>h<sub>θ</sub></sub>(y<sub>w</sub> </td> <td>t,s) + w<sub>k</sub> log P<sup>k</sup><sub>h<sub>θ</sub></sub>(y<sub>l</sub> </td> <td>t,s)]**</td> </tr> </tbody> </table> <p>Here:</p> <ul> <li> <strong>w<sub>k</sub></strong>: Weights based on fallacy frequency in training data</li> <li> <strong>P<sup>k</sup><sub>h<sub>θ</sub></sub></strong>: Probability of fallacy type k from classification head</li> <li> <strong>λ = 0.3</strong>: Balancing parameter (optimized through experiments)</li> </ul> <h2 id="-the-13-types-of-logical-fallacies">🧠 The 13 Types of Logical Fallacies</h2> <p>We define 13 categories of logical fallacies based on centuries of logical reasoning research dating back to Aristotle:</p> <h3 id="most-common-fallacies-in-our-dataset"><strong>Most Common Fallacies (in our dataset):</strong></h3> <ol> <li> <p><strong>Faulty Generalization (18.0%)</strong> - Drawing conclusions about all instances from limited examples</p> <ul> <li><em>Example: “I know someone who smoked cannabis and became successful. Therefore, everyone who smokes cannabis will be successful.”</em></li> </ul> </li> <li> <p><strong>Ad Hominem (12.3%)</strong> - Attacking the person making the argument rather than the argument itself</p> <ul> <li><em>Example: “Those climate scientists are just trying to get more funding for their research.”</em></li> </ul> </li> <li> <p><strong>Ad Populum (9.5%)</strong> - Argument based on what the majority believes</p> <ul> <li><em>Example: “Most people think this policy is good, so it must be correct.”</em></li> </ul> </li> <li> <p><strong>False Causality (8.8%)</strong> - Implying causal relationships without supporting evidence</p> <ul> <li><em>Example: “Ever since we installed those wind turbines, there have been more bird deaths in the area.”</em></li> </ul> </li> <li> <p><strong>Circular Reasoning (7.0%)</strong> - The conclusion comes back to the premise without proving itself</p> <ul> <li><em>Example: “We should trust the news because the news says we should trust it.”</em></li> </ul> </li> </ol> <h3 id="other-fallacy-types"><strong>Other Fallacy Types:</strong></h3> <ul> <li> <strong>Appeal to Emotion (6.8%)</strong> - Manipulating emotions rather than using logic</li> <li> <strong>Fallacy of Relevance (6.6%)</strong> - Introducing irrelevant information</li> <li> <strong>Fallacy of Logic (6.2%)</strong> - Errors in logical structure</li> <li> <strong>Intentional (5.8%)</strong> - Deliberately wrong arguments</li> <li> <strong>False Dilemma (5.8%)</strong> - Presenting only two options when many exist</li> <li> <strong>Fallacy of Extension (5.8%)</strong> - Attacking exaggerated versions of arguments</li> <li> <strong>Fallacy of Credibility (5.4%)</strong> - Attacking speaker’s character</li> <li> <strong>Equivocation (2.0%)</strong> - Using ambiguous language</li> </ul> <p><img src="https://via.placeholder.com/600x400/4CAF50/FFFFFF?text=Fallacy+Type+Distribution+Based+on+LOGIC+Dataset" alt="Fallacy Distribution"></p> <h2 id="-methodology-4-step-framework">🔬 Methodology: 4-Step Framework</h2> <p>Our approach involves four key steps:</p> <h3 id="step-1-supervised-fine-tuning-sft"><strong>Step 1: Supervised Fine-Tuning (SFT)</strong></h3> <p>Train the base model on the ExplaGraphs dataset containing topics, stances, and arguments.</p> <h3 id="step-2-preference-data-collection"><strong>Step 2: Preference Data Collection</strong></h3> <p>Generate fallacious arguments using ChatGPT for each original argument, creating preference pairs where:</p> <ul> <li> <strong>Preferred (y<sub>w</sub>)</strong>: Original logically sound argument</li> <li> <strong>Dispreferred (y<sub>l</sub>)</strong>: Generated fallacious argument with label k</li> </ul> <p>We generated <strong>7,872 fallacious arguments</strong> spanning all 13 fallacy types following the natural distribution found in real-world discourse.</p> <h3 id="step-3-preference-optimization"><strong>Step 3: Preference Optimization</strong></h3> <p>Apply standard preference optimization methods (DPO, PPO, CPO, KTO) using the preference dataset.</p> <h3 id="step-4-fipo-enhancement"><strong>Step 4: FIPO Enhancement</strong></h3> <p>Add our weighted classification loss to the best-performing preference method (CPO) to create FIPO.</p> <h2 id="-outstanding-results">📊 Outstanding Results</h2> <h3 id="dramatic-fallacy-reduction"><strong>Dramatic Fallacy Reduction</strong></h3> <table> <thead> <tr> <th>Model</th> <th>Baseline (SFT)</th> <th>Best Previous Method</th> <th><strong>FIPO</strong></th> <th><strong>Improvement</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Llama-2 (7B)</strong></td> <td>34.5%</td> <td>26.0% (PPO)</td> <td><strong>17.0%</strong></td> <td><strong>17.5% reduction</strong></td> </tr> <tr> <td><strong>Mistral (7B)</strong></td> <td>32.5%</td> <td>27.75% (KTO)</td> <td><strong>19.5%</strong></td> <td><strong>13.0% reduction</strong></td> </tr> </tbody> </table> <h3 id="quality-improvements-win-rate"><strong>Quality Improvements (Win-Rate)</strong></h3> <p>FIPO not only reduces fallacies but also maintains high argument quality:</p> <ul> <li> <strong>Human Evaluation Win-Rate</strong>: 46% (vs. 40.3% loss rate for CPO)</li> <li> <strong>GPT-4 Evaluation Win-Rate</strong>: 63.5% (highest among all methods)</li> <li> <strong>Significant reduction in “loss rate”</strong>: From 40.3% (CPO) to 23% (FIPO)</li> </ul> <h3 id="fallacy-specific-performance"><strong>Fallacy-Specific Performance</strong></h3> <p>FIPO excels particularly at reducing the most common fallacy types:</p> <table> <thead> <tr> <th>Fallacy Type</th> <th>Llama-2 SFT</th> <th>Llama-2 FIPO</th> <th><strong>Reduction</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Faulty Generalization</strong></td> <td>27.5%</td> <td><strong>7.0%</strong></td> <td><strong>-20.5%</strong></td> </tr> <tr> <td><strong>False Causality</strong></td> <td>2.5%</td> <td><strong>3.5%</strong></td> <td>Controlled</td> </tr> <tr> <td><strong>Appeal to Emotion</strong></td> <td>1.0%</td> <td><strong>2.5%</strong></td> <td>Controlled</td> </tr> </tbody> </table> <p>The weighted classification loss ensures the model focuses on the most problematic and frequent fallacies.</p> <h2 id="-human-evaluation-validation">🔍 Human Evaluation Validation</h2> <h3 id="gpt-4-reliability-verification"><strong>GPT-4 Reliability Verification</strong></h3> <p>We validated GPT-4’s ability to identify fallacies through human annotation:</p> <ul> <li> <strong>Randolph’s κ agreement</strong>: 0.640 (substantial agreement)</li> <li> <strong>Majority agreement ratio</strong>: 95.5% between annotators and GPT-4</li> </ul> <h3 id="comparative-analysis"><strong>Comparative Analysis</strong></h3> <p>200 arguments were independently classified by our team and compared with GPT-4:</p> <ul> <li>Strong alignment on most fallacy types</li> <li>Main discrepancy: Fallacy of Relevance detection</li> <li>Some confusion between Faulty Generalization and False Causality (expected due to subtle differences)</li> </ul> <h2 id="-ablation-studies-confirm-design-choices">🧪 Ablation Studies Confirm Design Choices</h2> <p>We validated our design through rigorous ablation studies:</p> <table> <thead> <tr> <th>Approach</th> <th>Fallacy Rate</th> <th><strong>Analysis</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Uniform Dataset Distribution</strong></td> <td>37.5%</td> <td>Natural distribution is crucial</td> </tr> <tr> <td><strong>Unweighted Cross-Entropy</strong></td> <td>29.0%</td> <td>Weighting by frequency is essential</td> </tr> <tr> <td><strong>FIPO (Full Method)</strong></td> <td><strong>17.0%</strong></td> <td><strong>Best performance</strong></td> </tr> </tbody> </table> <p>These results confirm that both the <strong>natural fallacy distribution</strong> and <strong>weighted classification loss</strong> are essential components of FIPO.</p> <h2 id="-generalization-out-of-domain-performance">🌍 Generalization: Out-of-Domain Performance</h2> <p>FIPO’s effectiveness extends beyond training data:</p> <p><strong>Debatepedia Dataset Results:</strong></p> <ul> <li> <strong>Fallacy Rate</strong>: 45% (second-best, close to KTO’s 44%)</li> <li> <strong>Win Rate</strong>: 62% (highest among all methods)</li> <li> <strong>Excellent at reducing</strong>: False Causality and Fallacy of Relevance</li> </ul> <p>This demonstrates FIPO’s ability to generalize to new domains and topics.</p> <h2 id="-implementation-details">💻 Implementation Details</h2> <h3 id="base-models"><strong>Base Models</strong></h3> <ul> <li> <strong>Llama-2 (7B)</strong> and <strong>Mistral (7B)</strong> </li> <li> <strong>LoRA fine-tuning</strong>: Reduces parameters from 7B to 8.3M (0.12%)</li> </ul> <h3 id="training-configuration"><strong>Training Configuration</strong></h3> <ul> <li> <strong>Classification loss weight (λ)</strong>: 0.3 (optimal balance)</li> <li> <strong>Fallacy weights (w<sub>k</sub>)</strong>: Based on natural frequency distribution</li> <li> <strong>Base method</strong>: CPO (best trade-off between win-rate and fallacy-rate)</li> </ul> <h3 id="key-hyperparameters"><strong>Key Hyperparameters</strong></h3> <ul> <li> <strong>Learning rate</strong>: 2×10⁻⁴</li> <li> <strong>LoRA rank</strong>: 16, α = 32</li> <li> <strong>Training epochs</strong>: 3</li> <li> <strong>β (CPO)</strong>: 0.1</li> </ul> <h2 id="-impact-and-future-directions">🔮 Impact and Future Directions</h2> <h3 id="immediate-impact"><strong>Immediate Impact</strong></h3> <ul> <li> <strong>First work</strong> to systematically address logical fallacies in argument generation</li> <li> <strong>Novel framework</strong> combining preference optimization with classification objectives</li> <li> <strong>Significant performance gains</strong> across multiple models and evaluation metrics</li> </ul> <h3 id="broader-implications"><strong>Broader Implications</strong></h3> <ul> <li> <strong>Trust and Safety</strong>: Reduces spread of logically flawed arguments</li> <li> <strong>Educational Applications</strong>: Could help teach logical reasoning</li> <li> <strong>Democratic Discourse</strong>: Promotes more sound public debate</li> </ul> <h3 id="future-research-directions"><strong>Future Research Directions</strong></h3> <ol> <li> <strong>Scaling to larger models</strong>: Apply FIPO to models &gt;10B parameters</li> <li> <strong>Multi-domain extension</strong>: Expand to scientific, legal, and technical arguments</li> <li> <strong>Real-time fallacy detection</strong>: Interactive systems for argument evaluation</li> <li> <strong>Cross-lingual fallacy analysis</strong>: Extend to non-English languages</li> <li> <strong>Integration with fact-checking</strong>: Combine logical and factual verification</li> </ol> <h2 id="-key-takeaways">🎯 Key Takeaways</h2> <ol> <li> <strong>LLMs have a significant logical fallacy problem</strong>: Even advanced models like ChatGPT generate fallacious arguments 21% of the time</li> <li> <strong>Explicit fallacy awareness helps</strong>: Making models aware of specific fallacy types significantly improves argument quality</li> <li> <strong>Weighted classification loss is crucial</strong>: Focusing on frequent fallacies yields better overall performance</li> <li> <strong>Preference optimization isn’t enough alone</strong>: Standard methods like DPO and PPO help but miss fine-grained fallacy distinctions</li> <li> <strong>FIPO provides substantial improvements</strong>: Up to 17.5% reduction in fallacy rates while maintaining argument quality</li> </ol> <h2 id="-resources-and-code">📚 Resources and Code</h2> <ul> <li> <strong>📄 Paper</strong>: <a href="https://aclanthology.org/2025.naacl-long.374.pdf" rel="external nofollow noopener" target="_blank">A Logical Fallacy-Informed Framework for Argument Generation</a> </li> <li> <strong>💻 Code &amp; Data</strong>: Available at <a href="https://github.com/lucamouchel/Logical-Fallacies" rel="external nofollow noopener" target="_blank">github.com/lucamouchel/Logical-Fallacies</a> </li> <li> <strong>🏆 Award Recognition</strong>: Outstanding Paper Award - NAACL 2025</li> </ul> <hr> <h2 id="-acknowledgments">👥 Acknowledgments</h2> <p>This work was a collaborative effort with amazing researchers:</p> <ul> <li> <strong>Luca Mouchel</strong> (Lead Author, Master Internship)</li> <li> <strong>Shaobo Cui</strong>, <strong>Robert West</strong>, <strong>Antoine Bosselut</strong>, <strong>Boi Faltings</strong> </li> <li><strong>EPFL, Switzerland</strong></li> </ul> <p>Special thanks to the <strong>ICT-48 Network of AI Research Excellence Center “TAILOR”</strong>, the <strong>Swiss National Science Foundation</strong>, and our other funding partners.</p> <hr> <h2 id="-citation">📖 Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mouchel-etal-2025-logical</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"A Logical Fallacy-Informed Framework for Argument Generation"</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">"Mouchel, Luca and Paul, Debjit and Cui, Shaobo and West, Robert and Bosselut, Antoine and Faltings, Boi"</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">"Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">"7296--7314"</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">"Association for Computational Linguistics"</span>
<span class="p">}</span>
</code></pre></div></div> <hr> <h2 id="-discussion-questions">🤔 Discussion Questions</h2> <p><strong>For Researchers:</strong></p> <ul> <li>How might FIPO be adapted for other types of reasoning tasks beyond argument generation?</li> <li>What other fine-grained classification objectives could improve preference optimization?</li> </ul> <p><strong>For Practitioners:</strong></p> <ul> <li>How can we integrate fallacy detection into real-world applications like social media platforms or educational tools?</li> <li>What are the computational trade-offs when adding classification heads to large language models?</li> </ul> <p><strong>For Society:</strong></p> <ul> <li>How do we balance improving logical reasoning with maintaining diverse perspectives in AI-generated content?</li> <li>What role should AI play in moderating online discourse and identifying fallacious arguments?</li> </ul> <hr> <p><em>This research represents a significant step toward more trustworthy and logically sound AI systems. As LLMs become increasingly prevalent in decision-making and public discourse, ensuring they generate well-reasoned arguments is not just a technical challenge—it’s a societal imperative.</em></p> <p><strong>🏆 Proud to have this work recognized with the Outstanding Paper Award at NAACL 2025!</strong></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/world-model/">What Children Know That AI Doesn't: Learning Through Experience, Not Text</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/compute/">Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/reasoning-matter/">Making Reasoning Matter: Measuring Faithfulness in Chain-of-Thought Reasoning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/">blog</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Debjit Paul. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>