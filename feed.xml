<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://debjitpaul.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debjitpaul.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-28T18:56:24+00:00</updated><id>https://debjitpaul.github.io/feed.xml</id><title type="html">blank</title><subtitle>Updating it, Work in Progress, My personal website</subtitle><entry><title type="html">Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures</title><link href="https://debjitpaul.github.io/blog/2025/compute/" rel="alternate" type="text/html" title="Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures"/><published>2025-09-28T00:00:00+00:00</published><updated>2025-09-28T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2025/compute</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2025/compute/"><![CDATA[<p>As Large Language Models (LLMs) continue to scale exponentially, understanding their computational requirements becomes crucial for researchers and practitioners alike. Whether you‚Äôre planning a training run, optimizing infrastructure costs, or comparing different architectures, <strong>FLOPs</strong> (Floating Point Operations) and <strong>MFU</strong> (Model FLOPs Utilization) are essential metrics that provide hardware-independent ways to measure and optimize computational efficiency.</p> <p>This post builds upon two excellent foundational resources: <strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs comprehensive guide to Transformer FLOPs</a></strong> and <strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs practical guide to FLOPs in LLM training</a></strong>. While these posts cover dense Transformer architectures, this post extends their insights to modern Mixture-of-Experts (MoE) architectures, with a particular focus on OpenAI‚Äôs recently released GPT-OSS models.</p> <p>In this comprehensive analysis, we‚Äôll explore FLOP counting methodologies, dive deep into MFU calculations, and examine how these concepts apply to the latest generation of sparse models that are reshaping the efficiency landscape of large language models.</p> <h2 id="-what-are-flops-and-why-do-they-matter">üî¢ What Are FLOPs and Why Do They Matter?</h2> <h3 id="defining-flops">Defining FLOPs</h3> <p>A <strong>FLOP</strong> (Floating Point Operation) represents a single computational operation like addition (<code class="language-plaintext highlighter-rouge">3.8 + 4.1</code>) or multiplication (<code class="language-plaintext highlighter-rouge">1.2 √ó 4.6</code>). When we talk about <strong>FLOPs</strong> (plural), we‚Äôre counting the total number of these atomic operations required for a specific task.</p> <p>For large-scale computations, we use:</p> <ul> <li><strong>GFLOPs</strong> = 1 billion FLOPs (10‚Åπ)</li> <li><strong>TFLOPs</strong> = 1 trillion FLOPs (10¬π¬≤)</li> <li><strong>PFLOPs</strong> = 1 quadrillion FLOPs (10¬π‚Åµ)</li> </ul> <h3 id="why-flops-matter">Why FLOPs Matter</h3> <p>FLOPs provide several crucial advantages:</p> <ol> <li><strong>Hardware Independence</strong>: Unlike wall-clock time, FLOPs offer consistent measurements across different hardware configurations</li> <li><strong>Reproducibility</strong>: Enable precise comparisons between different models and training setups</li> <li><strong>Cost Estimation</strong>: Help predict computational costs and resource requirements</li> <li><strong>Efficiency Analysis</strong>: Allow measurement of how well we utilize available hardware</li> </ol> <h2 id="-counting-flops-in-dense-transformers">üìê Counting FLOPs in Dense Transformers</h2> <h3 id="the-openai-method">The OpenAI Method</h3> <p>The seminal approach from OpenAI‚Äôs scaling laws paper [1] provides a clean approximation:</p> <p><strong>FLOPs per token ‚âà 6N</strong></p> <p>Where <code class="language-plaintext highlighter-rouge">N</code> is the number of non-embedding parameters. This factor of 6 accounts for:</p> <ul> <li><strong>2√ó</strong> for the forward pass (multiply-accumulate operations)</li> <li><strong>2√ó</strong> for the backward pass (gradients with respect to inputs)</li> <li><strong>2√ó</strong> for the backward pass (gradients with respect to parameters)</li> </ul> <p>This approximation, introduced by Kaplan et al. in their foundational work on neural language model scaling laws, provides a hardware-independent way to estimate computational requirements [1]. The elegance of this formula lies in its simplicity while capturing the essential computational structure of Transformer training.</p> <p>Let‚Äôs break down the forward pass components:</p> <table> <thead> <tr> <th>Operation</th> <th>Parameters</th> <th>FLOPs per Token</th> </tr> </thead> <tbody> <tr> <td><strong>Attention QKV</strong></td> <td><code class="language-plaintext highlighter-rouge">3 √ó d √ó d_model</code></td> <td><code class="language-plaintext highlighter-rouge">6 √ó L √ó d_model √ó d</code></td> </tr> <tr> <td><strong>Attention Scores</strong></td> <td>-</td> <td><code class="language-plaintext highlighter-rouge">4 √ó L √ó seq_len √ó d</code></td> </tr> <tr> <td><strong>Attention Project</strong></td> <td><code class="language-plaintext highlighter-rouge">d √ó d_model</code></td> <td><code class="language-plaintext highlighter-rouge">2 √ó L √ó d √ó d_model</code></td> </tr> <tr> <td><strong>Feedforward</strong></td> <td><code class="language-plaintext highlighter-rouge">8 √ó d_model¬≤</code></td> <td><code class="language-plaintext highlighter-rouge">16 √ó L √ó d_model¬≤</code></td> </tr> <tr> <td><strong>Total (approx)</strong></td> <td>-</td> <td><code class="language-plaintext highlighter-rouge">‚âà 2 √ó N</code></td> </tr> </tbody> </table> <p>Where:</p> <ul> <li><code class="language-plaintext highlighter-rouge">L</code> = number of layers</li> <li><code class="language-plaintext highlighter-rouge">seq_len</code> = sequence length</li> <li><code class="language-plaintext highlighter-rouge">d_model</code> = hidden dimension</li> <li><code class="language-plaintext highlighter-rouge">d</code> = attention dimension (<code class="language-plaintext highlighter-rouge">d_model</code> for most implementations)</li> </ul> <h3 id="the-deepmind-method">The DeepMind Method</h3> <p>DeepMind‚Äôs Chinchilla paper provides a more detailed accounting that includes embeddings, logits, and attention mechanics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">deepmind_flops_per_sequence</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">ff_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">DeepMind method for forward pass FLOPs counting</span><span class="sh">"""</span>
    <span class="n">d_attn</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
    <span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">ff_ratio</span>

    <span class="c1"># Components
</span>    <span class="n">embeddings</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="n">d_model</span>
    <span class="n">attn_qkv</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_softmax</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">attn_reduce</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_project</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_model</span>

    <span class="n">total_attn</span> <span class="o">=</span> <span class="n">attn_qkv</span> <span class="o">+</span> <span class="n">attn_logits</span> <span class="o">+</span> <span class="n">attn_softmax</span> <span class="o">+</span> <span class="n">attn_reduce</span> <span class="o">+</span> <span class="n">attn_project</span>
    <span class="n">feedforward</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">vocab_size</span>

    <span class="k">return</span> <span class="n">embeddings</span> <span class="o">+</span> <span class="n">n_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">total_attn</span> <span class="o">+</span> <span class="n">feedforward</span><span class="p">)</span> <span class="o">+</span> <span class="n">logits</span>
</code></pre></div></div> <h3 id="practical-example-gpt-3-scale-model">Practical Example: GPT-3 Scale Model</h3> <p>Let‚Äôs calculate FLOPs for a GPT-3 scale model:</p> <ul> <li><strong>Parameters</strong>: 175B non-embedding</li> <li><strong>Sequence length</strong>: 2048</li> <li><strong>Batch size</strong>: 32</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using OpenAI approximation
</span><span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="mf">175e9</span>  <span class="c1"># 1.05 √ó 10^12 FLOPs
</span><span class="n">tokens_per_step</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">2048</span>  <span class="c1"># 65,536 tokens
</span><span class="n">flops_per_step</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">tokens_per_step</span>  <span class="c1"># 6.87 √ó 10^16 FLOPs
</span>
<span class="c1"># Forward + Backward
</span><span class="n">total_flops_per_step</span> <span class="o">=</span> <span class="n">flops_per_step</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># 1.37 √ó 10^17 FLOPs ‚âà 137 PFLOPs
</span></code></pre></div></div> <h2 id="-model-flops-utilization-mfu-the-gold-standard">‚ö° Model FLOPs Utilization (MFU): The Gold Standard</h2> <h3 id="understanding-mfu">Understanding MFU</h3> <p><strong>Model FLOPs Utilization (MFU)</strong> measures how efficiently we execute the theoretically necessary FLOPs for training, introduced in Google‚Äôs PaLM paper:</p> <p><strong>MFU = (Model FLOPs √ó Throughput) / Peak Hardware FLOPs</strong></p> <p>Where:</p> <ul> <li><strong>Model FLOPs</strong>: Theoretical FLOPs per token (using OpenAI‚Äôs 6N approximation)</li> <li><strong>Throughput</strong>: Observed tokens processed per second</li> <li><strong>Peak Hardware FLOPs</strong>: Theoretical maximum FLOP/s of your hardware</li> </ul> <h3 id="mfu-vs-hardware-flops-utilization-hfu">MFU vs. Hardware FLOPs Utilization (HFU)</h3> <table> <thead> <tr> <th>Metric</th> <th><strong>MFU</strong></th> <th><strong>HFU</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Includes</strong></td> <td>Only necessary model computations</td> <td>All computations (including overheads)</td> </tr> <tr> <td><strong>Use Case</strong></td> <td>Fair comparison across setups</td> <td>Implementation efficiency</td> </tr> <tr> <td><strong>Affected by</strong></td> <td>Model architecture, batch size</td> <td>Memory management, communication</td> </tr> </tbody> </table> <h3 id="calculating-mfu-a-practical-example">Calculating MFU: A Practical Example</h3> <p>Consider training a 7B parameter model on 8√óA100 GPUs:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model specifications
</span><span class="n">parameters</span> <span class="o">=</span> <span class="mf">7e9</span>
<span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">parameters</span>  <span class="c1"># 4.2 √ó 10^10
</span>
<span class="c1"># Hardware specifications
</span><span class="n">a100_peak_flops</span> <span class="o">=</span> <span class="mf">312e12</span>  <span class="c1"># 312 TFLOPs for bf16
</span><span class="n">total_peak_flops</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">a100_peak_flops</span>  <span class="c1"># 2.496 √ó 10^15
</span>
<span class="c1"># Measured throughput
</span><span class="n">tokens_per_second</span> <span class="o">=</span> <span class="mi">8000</span>

<span class="c1"># Calculate MFU
</span><span class="n">sustained_flops</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">tokens_per_second</span>  <span class="c1"># 3.36 √ó 10^14
</span><span class="n">mfu</span> <span class="o">=</span> <span class="n">sustained_flops</span> <span class="o">/</span> <span class="n">total_peak_flops</span>  <span class="c1"># 0.135 = 13.5%
</span></code></pre></div></div> <h3 id="typical-mfu-ranges">Typical MFU Ranges</h3> <p>Real-world MFU values vary significantly:</p> <ul> <li><strong>Small models (&lt; 1B)</strong>: 10-30%</li> <li><strong>Medium models (1B-10B)</strong>: 30-50%</li> <li><strong>Large models (10B-100B)</strong>: 45-65%</li> <li><strong>Very large models (100B+)</strong>: 50-70%</li> </ul> <p>Higher MFU in larger models is often due to better compute-to-communication ratios and improved memory bandwidth utilization.</p> <h2 id="-extending-to-mixture-of-experts-moe-architectures">üß† Extending to Mixture-of-Experts (MoE) Architectures</h2> <h3 id="moe-fundamentals">MoE Fundamentals</h3> <p>Mixture-of-Experts architectures replace dense feedforward networks with a router and multiple expert networks. Only a subset of experts (typically 1-2 out of 8-64) are activated per token, dramatically reducing computational costs while maintaining or improving model quality.</p> <h3 id="moe-flop-counting-challenges">MoE FLOP Counting Challenges</h3> <p>Traditional FLOP counting becomes more nuanced with MoE:</p> <ol> <li><strong>Routing Overhead</strong>: Additional FLOPs for expert selection</li> <li><strong>Variable Computation</strong>: Different tokens may use different experts</li> <li><strong>Load Balancing</strong>: Uneven expert utilization affects total FLOPs</li> <li><strong>Communication Costs</strong>: Expert routing across devices</li> </ol> <h3 id="moe-flop-formula">MoE FLOP Formula</h3> <p>For a MoE layer replacing a dense feedforward network:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">moe_flops_per_token</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span> <span class="n">experts_per_token</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">load_balance_factor</span><span class="o">=</span><span class="mf">1.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Calculate FLOPs for MoE layer</span><span class="sh">"""</span>

    <span class="c1"># Router FLOPs (token ‚Üí expert probabilities)
</span>    <span class="n">router_flops</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">num_experts</span>

    <span class="c1"># Expert FLOPs (only active experts)
</span>    <span class="n">expert_flops</span> <span class="o">=</span> <span class="n">experts_per_token</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Load balancing overhead
</span>    <span class="n">effective_expert_flops</span> <span class="o">=</span> <span class="n">expert_flops</span> <span class="o">*</span> <span class="n">load_balance_factor</span>

    <span class="k">return</span> <span class="n">router_flops</span> <span class="o">+</span> <span class="n">effective_expert_flops</span>

<span class="c1"># Example: 8 experts, top-2 routing
</span><span class="n">dense_ff_flops</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">*</span> <span class="n">d_model</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># 4x expansion, two linear layers
</span><span class="n">moe_ff_flops</span> <span class="o">=</span> <span class="nf">moe_flops_per_token</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">efficiency_gain</span> <span class="o">=</span> <span class="n">dense_ff_flops</span> <span class="o">/</span> <span class="n">moe_ff_flops</span>  <span class="c1"># Typically 2-4x
</span></code></pre></div></div> <h2 id="-case-study-openai-gpt-oss-models">üöÄ Case Study: OpenAI GPT-OSS Models</h2> <p>OpenAI‚Äôs GPT-OSS models provide an excellent real-world example of modern MoE architectures with several innovative features:</p> <h3 id="gpt-oss-architecture-overview">GPT-OSS Architecture Overview</h3> <p><strong>GPT-OSS-120B</strong>:</p> <ul> <li><strong>Total Parameters</strong>: 117B (5.1B active per token)</li> <li><strong>Architecture</strong>: MoE with native MXFP4 quantization</li> <li><strong>Activation</strong>: ~4.4% of total parameters per token</li> <li><strong>Memory</strong>: Fits on single H100 (80GB)</li> </ul> <p><strong>GPT-OSS-20B</strong>:</p> <ul> <li><strong>Total Parameters</strong>: 21B (3.6B active per token)</li> <li><strong>Activation</strong>: ~17% of total parameters per token</li> <li><strong>Memory</strong>: Runs in 16GB</li> </ul> <h3 id="flop-counting-for-gpt-oss">FLOP Counting for GPT-OSS</h3> <p>Let‚Äôs calculate FLOPs for GPT-OSS-120B:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gpt_oss_flops_calculation</span><span class="p">():</span>
    <span class="c1"># GPT-OSS-120B specifications (estimated)
</span>    <span class="n">total_params</span> <span class="o">=</span> <span class="mf">117e9</span>
    <span class="n">active_params_per_token</span> <span class="o">=</span> <span class="mf">5.1e9</span>

    <span class="c1"># Traditional approach (if it were dense)
</span>    <span class="n">dense_flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">total_params</span>  <span class="c1"># 702 GFLOPs
</span>
    <span class="c1"># MoE approach (actual)
</span>    <span class="c1"># Attention layers remain dense
</span>    <span class="n">attention_params</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mf">0.3</span>  <span class="c1"># Estimated 30%
</span>    <span class="n">attention_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">attention_params</span>

    <span class="c1"># MoE feedforward layers
</span>    <span class="n">moe_params_per_token</span> <span class="o">=</span> <span class="n">active_params_per_token</span> <span class="o">-</span> <span class="n">attention_params</span>
    <span class="n">moe_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">moe_params_per_token</span>

    <span class="c1"># Router overhead (small)
</span>    <span class="n">router_flops</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c1"># Estimated 0.1%
</span>
    <span class="n">total_flops_per_token</span> <span class="o">=</span> <span class="n">attention_flops</span> <span class="o">+</span> <span class="n">moe_flops</span> <span class="o">+</span> <span class="n">router_flops</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">dense_equivalent</span><span class="sh">'</span><span class="p">:</span> <span class="n">dense_flops_per_token</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">actual_moe</span><span class="sh">'</span><span class="p">:</span> <span class="n">total_flops_per_token</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">efficiency_gain</span><span class="sh">'</span><span class="p">:</span> <span class="n">dense_flops_per_token</span> <span class="o">/</span> <span class="n">total_flops_per_token</span>
    <span class="p">}</span>

<span class="n">results</span> <span class="o">=</span> <span class="nf">gpt_oss_flops_calculation</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dense equivalent: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">dense_equivalent</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">e</span><span class="si">}</span><span class="s"> FLOPs/token</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">MoE actual: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">actual_moe</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">e</span><span class="si">}</span><span class="s"> FLOPs/token</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Efficiency gain: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">efficiency_gain</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">x</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="mxfp4-quantization-impact">MXFP4 Quantization Impact</h3> <p>GPT-OSS uses native MXFP4 quantization for MoE layers:</p> <ul> <li><strong>Memory</strong>: 4-bit storage vs 16-bit (4√ó reduction)</li> <li><strong>Compute</strong>: Specialized kernels maintain computational efficiency</li> <li><strong>Accuracy</strong>: Minimal degradation with proper scaling</li> </ul> <p>This affects FLOP counting considerations:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quantized_moe_flops</span><span class="p">(</span><span class="n">base_flops</span><span class="p">,</span> <span class="n">quantization_overhead</span><span class="o">=</span><span class="mf">1.05</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    MXFP4 might have slight computational overhead
    but significant memory bandwidth benefits
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">base_flops</span> <span class="o">*</span> <span class="n">quantization_overhead</span>
</code></pre></div></div> <h3 id="performance-analysis">Performance Analysis</h3> <p>Based on the GPT-OSS specifications, we can estimate performance characteristics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Estimated GPT-OSS-120B on H100
</span><span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="mf">5.1e9</span>  <span class="c1"># 30.6 GFLOPs (active parameters)
</span><span class="n">h100_peak</span> <span class="o">=</span> <span class="mf">1980e12</span>  <span class="c1"># ~2 PFLOPs for int4 operations
</span><span class="n">target_throughput</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># tokens/second (estimated)
</span>
<span class="n">required_flops_rate</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">target_throughput</span>  <span class="c1"># 3.06 TFLOPs/s
</span><span class="n">mfu</span> <span class="o">=</span> <span class="n">required_flops_rate</span> <span class="o">/</span> <span class="n">h100_peak</span>  <span class="c1"># 0.15%
</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Estimated MFU for GPT-OSS-120B: </span><span class="si">{</span><span class="n">mfu</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><em>Note: These are rough estimates. Actual performance depends on implementation details, memory bandwidth, and other factors.</em></p> <h2 id="-practical-considerations-and-optimizations">üîß Practical Considerations and Optimizations</h2> <h3 id="memory-vs-compute-trade-offs">Memory vs. Compute Trade-offs</h3> <p>Modern LLM training involves several techniques that affect FLOP calculations:</p> <p><strong>Activation Checkpointing</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">checkpointing_flop_overhead</span><span class="p">(</span><span class="n">base_flops</span><span class="p">,</span> <span class="n">checkpoint_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Recompute activations during backward pass
    checkpoint_ratio: fraction of activations checkpointed
    </span><span class="sh">"""</span>
    <span class="n">recompute_flops</span> <span class="o">=</span> <span class="n">base_flops</span> <span class="o">*</span> <span class="n">checkpoint_ratio</span>
    <span class="k">return</span> <span class="n">base_flops</span> <span class="o">+</span> <span class="n">recompute_flops</span>  <span class="c1"># Up to 1.5√ó FLOPs
</span></code></pre></div></div> <p><strong>Gradient Accumulation</strong>:</p> <ul> <li>Doesn‚Äôt affect per-token FLOPs</li> <li>May affect MFU due to different memory access patterns</li> </ul> <h3 id="scaling-laws-and-flop-optimal-training">Scaling Laws and FLOP-Optimal Training</h3> <p>The relationship between model size, dataset size, and computational budget follows predictable scaling laws:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_optimal_scaling</span><span class="p">(</span><span class="n">compute_budget_flops</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Based on Chinchilla scaling laws
    Roughly: parameters ‚àù compute^0.5, tokens ‚àù compute^0.5
    </span><span class="sh">"""</span>
    <span class="n">optimal_params</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_budget_flops</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">optimal_tokens</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_budget_flops</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>

    <span class="k">return</span> <span class="n">optimal_params</span><span class="p">,</span> <span class="n">optimal_tokens</span>

<span class="c1"># Example: 10^23 FLOPs budget
</span><span class="n">params</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nf">compute_optimal_scaling</span><span class="p">(</span><span class="mf">1e23</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal: </span><span class="si">{</span><span class="n">params</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">B parameters, </span><span class="si">{</span><span class="n">tokens</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">B tokens</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-benchmarking-and-measurement-tools">üìä Benchmarking and Measurement Tools</h2> <h3 id="measuring-mfu-in-practice">Measuring MFU in Practice</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">measure_mfu</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Measure MFU during actual training</span><span class="sh">"""</span>
    <span class="n">device</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span>

    <span class="c1"># Create dummy batch
</span>    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50000</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Warm up
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">).</span><span class="n">loss</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">).</span><span class="n">loss</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="c1"># Calculate throughput
</span>    <span class="n">total_tokens</span> <span class="o">=</span> <span class="n">num_steps</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">throughput</span> <span class="o">=</span> <span class="n">total_tokens</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

    <span class="c1"># Calculate MFU
</span>    <span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">model</span><span class="p">.</span><span class="nf">num_parameters</span><span class="p">()</span>
    <span class="n">peak_flops</span> <span class="o">=</span> <span class="nf">get_device_peak_flops</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mfu</span> <span class="o">=</span> <span class="p">(</span><span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">throughput</span><span class="p">)</span> <span class="o">/</span> <span class="n">peak_flops</span>

    <span class="k">return</span> <span class="n">mfu</span><span class="p">,</span> <span class="n">throughput</span>
</code></pre></div></div> <h3 id="profiling-tools">Profiling Tools</h3> <ul> <li><strong>PyTorch Profiler</strong>: Built-in FLOP counting</li> <li><strong>DeepSpeed</strong>: MFU reporting in training logs</li> <li><strong>Weights &amp; Biases</strong>: Integration with hardware metrics</li> <li><strong>Custom counters</strong>: Framework-specific implementations</li> </ul> <h2 id="-future-directions-and-emerging-architectures">üîÆ Future Directions and Emerging Architectures</h2> <h3 id="beyond-traditional-flops">Beyond Traditional FLOPs</h3> <p>As architectures evolve, FLOP counting methodologies must adapt:</p> <p><strong>Sparse Attention Patterns</strong>:</p> <ul> <li>Linear attention: O(n) instead of O(n¬≤)</li> <li>Local attention: Reduced sequence length dependencies</li> <li>Mixture of attention heads: Different patterns per head</li> </ul> <p><strong>Conditional Computation</strong>:</p> <ul> <li>Early exit mechanisms</li> <li>Adaptive depth networks</li> <li>Token-wise routing</li> </ul> <p><strong>Hardware-Aware Metrics</strong>:</p> <ul> <li>Memory bandwidth utilization</li> <li>Integer operation efficiency</li> <li>Specialized accelerator metrics</li> </ul> <h3 id="the-road-ahead">The Road Ahead</h3> <p>The future of computational efficiency in LLMs likely involves:</p> <ol> <li><strong>Multi-modal MoE</strong>: Extending sparse computation to different modalities</li> <li><strong>Dynamic architectures</strong>: Runtime adaptation based on input complexity</li> <li><strong>Hardware co-design</strong>: Models designed for specific accelerators</li> <li><strong>Hybrid precision</strong>: Strategic use of different numerical formats</li> </ol> <h2 id="-conclusion">üèÅ Conclusion</h2> <p>Understanding FLOPs and MFU is crucial for anyone working with large-scale language models. As we‚Äôve seen:</p> <ul> <li><strong>FLOPs provide hardware-independent measurement</strong> of computational requirements</li> <li><strong>MFU enables fair comparison</strong> of training efficiency across different setups</li> <li><strong>MoE architectures complicate</strong> but don‚Äôt invalidate these fundamental concepts</li> <li><strong>Modern models like GPT-OSS</strong> showcase how innovative architectures and quantization can dramatically improve efficiency</li> </ul> <p>Whether you‚Äôre planning a training run, optimizing an existing system, or designing new architectures, these metrics provide essential insights into the computational reality of modern AI systems.</p> <p>As models continue to scale and new architectures emerge, the principles of FLOP counting and efficiency measurement will remain foundational tools for understanding and optimizing the computational landscape of artificial intelligence.</p> <hr/> <h2 id="-acknowledgments">üôè Acknowledgments</h2> <p>This blog post draws heavily from two excellent resources that provided the foundation for understanding FLOPs in LLM training. I want to give full credit to these authors for their pioneering work:</p> <p><strong>Primary Inspirations:</strong></p> <ul> <li> <p><strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs ‚ÄúTransformer FLOPs‚Äù</a></strong> - An exceptionally clear and comprehensive guide that masterfully explains FLOP counting methodologies, MFU calculations, and scaling behaviors. Much of the mathematical foundation and practical examples in this post are built upon Adam‚Äôs excellent work.</p> </li> <li> <p><strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs ‚ÄúFLOPs in LLM Training: The Ultimate Guide‚Äù</a></strong> - A practical and accessible guide that bridges the gap between theory and implementation, providing clear examples and real-world context for FLOP calculations.</p> </li> </ul> <p>Both posts were instrumental in shaping my understanding of computational efficiency in LLMs. This work extends their insights to MoE architectures and provides additional analysis of modern models like GPT-OSS, but the core concepts and methodologies owe much to these foundational resources.</p> <h2 id="-references-and-further-reading">üìö References and Further Reading</h2> <p><strong>Foundational Papers:</strong></p> <ul> <li><strong>[1] <a href="https://arxiv.org/abs/2001.08361">OpenAI Scaling Laws</a></strong>: Kaplan, J., McCandlish, S., Henighan, T., et al. ‚ÄúScaling Laws for Neural Language Models‚Äù (2020) - Original FLOP counting methodology and the 6N approximation</li> <li><strong>[2] <a href="https://arxiv.org/abs/2203.15556">DeepMind Chinchilla</a></strong>: Hoffmann, J., Borgeaud, S., Mensch, A., et al. ‚ÄúTraining Compute-Optimal Large Language Models‚Äù (2022) - Detailed FLOP accounting and compute-optimal training</li> <li><strong>[3] <a href="https://arxiv.org/abs/2204.02311">Google PaLM</a></strong>: Chowdhery, A., Narang, S., Devlin, J., et al. ‚ÄúPaLM: Scaling Language Modeling with Pathways‚Äù (2022) - MFU methodology introduction</li> </ul> <p><strong>Key Blog Posts and Resources:</strong></p> <ul> <li><strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs Transformer FLOPs</a></strong>: Comprehensive FLOP analysis with interactive calculator</li> <li><strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs FLOPs Guide</a></strong>: Practical guide to FLOPs in LLM training</li> <li><strong><a href="https://github.com/openai/gpt-oss">GPT-OSS Repository</a></strong>: OpenAI‚Äôs open-source MoE implementation</li> </ul> <p><strong>Additional Technical Resources:</strong></p> <ul> <li><strong><a href="https://kipp.ly/blog/transformer-inference-arithmetic/">Transformer Inference Arithmetic</a></strong>: Kipp Bradford‚Äôs analysis of inference costs</li> <li><strong><a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr</a></strong>: Horace He‚Äôs guide to optimization fundamentals</li> </ul> <hr/> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">paul2024flops_mfu_moe</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">{https://debjitpaul.github.io/blog/}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/> <p><em>This analysis provides a comprehensive overview of computational efficiency in modern LLM training. For specific implementation details or model-specific calculations, always refer to the original papers and codebases.</em></p>]]></content><author><name></name></author><category term="research"/><category term="technical"/><category term="flops"/><category term="mfu"/><category term="llm-training"/><category term="moe"/><category term="gpt-oss"/><category term="computational-efficiency"/><category term="transformers"/><summary type="html"><![CDATA[A comprehensive guide to counting FLOPs in LLM training, measuring Model FLOPs Utilization (MFU), and extending these concepts to Mixture-of-Experts architectures with a deep dive into OpenAI's GPT-OSS models.]]></summary></entry><entry><title type="html">Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning</title><link href="https://debjitpaul.github.io/blog/2024/reasoning-matter/" rel="alternate" type="text/html" title="Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning"/><published>2024-02-23T00:00:00+00:00</published><updated>2024-02-23T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2024/reasoning-matter</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2024/reasoning-matter/"><![CDATA[<p>Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks when prompted to generate step-by-step explanations, known as Chain-of-Thought (CoT) reasoning. However, a critical question remains: <strong>Do these models actually use their generated reasoning steps to arrive at their final answers?</strong></p> <h2 id="the-problem">The Problem</h2> <p>While CoT prompting improves performance on many reasoning tasks, recent studies suggest that models might not always rely on their intermediate reasoning steps. This raises concerns about the <strong>faithfulness</strong> of the reasoning process - whether the generated explanations truly reflect the model‚Äôs decision-making process.</p> <h2 id="our-approach">Our Approach</h2> <p>In our paper <a href="https://arxiv.org/abs/2402.13950">‚ÄúMaking Reasoning Matter‚Äù</a>, we introduce a comprehensive framework to measure and improve reasoning faithfulness using <strong>causal mediation analysis</strong>.</p> <h3 id="key-contributions">Key Contributions</h3> <ol> <li><strong>Causal Framework</strong>: We develop a method to quantify how much intermediate reasoning steps causally influence final predictions</li> <li><strong>Extensive Evaluation</strong>: Analysis across 11 different language models on multiple reasoning tasks</li> <li><strong>Practical Improvements</strong>: Techniques to enhance reasoning faithfulness</li> </ol> <h2 id="key-findings">Key Findings</h2> <p>Our causal mediation analysis across different model families reveals several important insights:</p> <h3 id="model-behavior-varies-by-training-objective">Model Behavior Varies by Training Objective</h3> <ul> <li><strong>In-context learning</strong> and <strong>instruction-tuning</strong> improve alignment with reasoning chains</li> <li>Models trained with <strong>RLHF</strong> show more direct effects than indirect effects, suggesting potential issues with faithful reasoning</li> <li><strong>Larger models</strong> don‚Äôt automatically show better faithfulness</li> </ul> <h3 id="task-specific-patterns">Task-Specific Patterns</h3> <p>We evaluated on three types of reasoning tasks:</p> <ul> <li><strong>Mathematical Reasoning</strong> (GSM8K)</li> <li><strong>Strategic Reasoning</strong> (StrategyQA)</li> <li><strong>Causal Understanding</strong></li> </ul> <p>Results show that faithfulness varies significantly across task types, with mathematical reasoning showing different patterns compared to strategic reasoning tasks.</p> <h2 id="methodology">Methodology</h2> <p>Our approach uses <strong>causal mediation analysis</strong> to decompose the total effect of reasoning problems into:</p> <ol> <li><strong>Direct Effect</strong>: How much the problem directly influences the answer (bypassing reasoning)</li> <li><strong>Indirect Effect</strong>: How much the problem influences the answer through generated reasoning steps</li> </ol> <p>High indirect effect indicates faithful reasoning, while high direct effect suggests the model might be ignoring its reasoning steps.</p> <h2 id="implications">Implications</h2> <p>This work has important implications for:</p> <ul> <li><strong>Model Development</strong>: Understanding which training objectives promote faithful reasoning</li> <li><strong>Evaluation</strong>: Moving beyond accuracy to assess reasoning quality</li> <li><strong>Trust and Interpretability</strong>: Building more reliable and transparent AI systems</li> </ul> <h2 id="future-directions">Future Directions</h2> <p>Our findings open several avenues for future research:</p> <ul> <li>Developing training methods that promote faithfulness</li> <li>Creating better evaluation metrics for reasoning quality</li> <li>Understanding the relationship between model scale and reasoning faithfulness</li> </ul> <hr/> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">debjit2024frodo</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul and Robert West and Antoine Bosselut and Boi Faltings}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13950}</span><span class="p">,</span>
    <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
    <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Links:</strong></p> <ul> <li><a href="https://arxiv.org/abs/2402.13950">Paper on arXiv</a></li> <li><a href="/reasoningmatter/">Project Page</a></li> <li><a href="https://github.com/debjitpaul/reasoning-matter">Code Repository</a> <em>(if available)</em></li> </ul> <hr/> <p><em>This post is part of our ongoing research into making AI reasoning more transparent and reliable. For more updates on our work, follow our <a href="/blog/">research blog</a>.</em></p>]]></content><author><name></name></author><category term="research"/><category term="reasoning"/><category term="chain-of-thought"/><category term="LLMs"/><category term="faithfulness"/><category term="causal-analysis"/><summary type="html"><![CDATA[A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.]]></summary></entry><entry><title type="html">REFINER</title><link href="https://debjitpaul.github.io/blog/" rel="alternate" type="text/html" title="REFINER"/><published>2023-04-04T00:00:00+00:00</published><updated>2023-04-04T00:00:00+00:00</updated><id>https://debjitpaul.github.io/refiner</id><content type="html" xml:base="https://debjitpaul.github.io/blog/"><![CDATA[<div class="post"> <div class="header-bar"> <h1>Research Blog</h1> <h2>Feedback LLM to improve their performance</h2> </div> <div class="container featured-research" style="margin-bottom: 2rem;"> <h3 style="margin-bottom: 1rem; color: var(--global-theme-color);">üî¨ Featured Research Projects</h3> <div class="row mb-4"> <div class="col-md-6 mb-4"> <div class="card hoverable h-100"> <div class="card-body"> <h4 class="card-title">Making Reasoning Matter</h4> <p class="card-text">Measuring and improving faithfulness of chain-of-thought reasoning in large language models using causal mediation analysis.</p> <div class="research-highlights mb-3"> <h6>Key Contributions:</h6> <ul class="list-unstyled"> <li><i class="fas fa-check-circle text-success"></i> Causal framework for measuring reasoning faithfulness</li> <li><i class="fas fa-check-circle text-success"></i> Analysis across 11 different language models</li> <li><i class="fas fa-check-circle text-success"></i> Insights on training objectives and reasoning quality</li> </ul> </div> <div class="research-meta mb-2"> <span class="badge badge-primary">arXiv:2402.13950</span> <span class="badge badge-secondary">Chain-of-Thought</span> <span class="badge badge-secondary">Causal Analysis</span> </div> <div class="mt-3"> <a href="/reasoningmatter/" class="btn btn-primary btn-sm"> <i class="fas fa-info-circle"></i> Project Details </a> <a href="https://arxiv.org/abs/2402.13950" target="_blank" class="btn btn-outline-secondary btn-sm"> <i class="fas fa-external-link-alt"></i> Read Paper </a> </div> </div> </div> </div> <div class="col-md-6 mb-4"> <div class="card hoverable h-100"> <div class="card-body"> <h4 class="card-title">REFINER</h4> <p class="card-text">A framework for improving language models' reasoning through iterative feedback from critic models on intermediate representations.</p> <div class="research-highlights mb-3"> <h6>Key Features:</h6> <ul class="list-unstyled"> <li><i class="fas fa-check-circle text-success"></i> Generator-Critic architecture for iterative refinement</li> <li><i class="fas fa-check-circle text-success"></i> Structured feedback on reasoning steps</li> <li><i class="fas fa-check-circle text-success"></i> Human-compatible feedback integration</li> </ul> </div> <div class="research-meta mb-2"> <span class="badge badge-primary">arXiv:2304.01904</span> <span class="badge badge-secondary">Feedback Learning</span> <span class="badge badge-secondary">Reasoning</span> </div> <div class="mt-3"> <a href="/refiner/" class="btn btn-primary btn-sm"> <i class="fas fa-info-circle"></i> Project Details </a> <a href="https://arxiv.org/abs/2304.01904" target="_blank" class="btn btn-outline-secondary btn-sm"> <i class="fas fa-external-link-alt"></i> Read Paper </a> </div> </div> </div> </div> </div> <div class="row"> <div class="col-md-12"> <div class="card hoverable"> <div class="card-body"> <h5 class="card-title">üß† Research Focus Areas</h5> <div class="row"> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-brain text-primary"></i> AI Reasoning</h6> <p class="small text-muted">Chain-of-thought reasoning, causal inference, knowledge representation, and logical reasoning processes in AI systems.</p> </div> </div> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-comments text-info"></i> Feedback &amp; Learning</h6> <p class="small text-muted">Iterative feedback mechanisms, critic models, human-AI interaction, and preference optimization for better model alignment.</p> </div> </div> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-search text-success"></i> Model Faithfulness</h6> <p class="small text-muted">Measuring and improving the reliability, interpretability, and trustworthiness of AI decision-making processes.</p> </div> </div> </div> </div> </div> </div> </div> </div> <hr style="margin: 2rem 0;"/> <div class="tag-category-list mb-4"> <p><strong>Browse by topics:</strong></p> <ul class="p-0 m-0" style="display: flex; flex-wrap: wrap; list-style: none;"> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/moe" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> moe </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/gpt-oss" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> gpt-oss </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/transformers" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> transformers </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/faithfulness" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> faithfulness </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/code" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> code </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/causal-analysis" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> causal-analysis </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/chain-of-thought" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> chain-of-thought </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/category/external-services" class="badge badge-info"> <i class="fas fa-tag fa-sm"></i> external-services </a> </li> </ul> </div> <div class="container featured-posts mb-4"> <h4 style="margin-bottom: 1rem;">üìå Featured Posts</h4> <div class="row row-cols-2"> <div class="col mb-4"> <a href="/blog/2025/compute/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures</h5> <p class="card-text">A comprehensive guide to counting FLOPs in LLM training, measuring Model FLOPs Utilization (MFU), and extending these concepts to Mixture-of-Experts architectures with a deep dive into OpenAI's GPT-OSS models.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 14 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> September 28, 2025 </p> </div> </div> </a> </div> <div class="col mb-4"> <a href="/blog/2024/reasoning-matter/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning</h5> <p class="card-text">A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 3 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> February 23, 2024 </p> </div> </div> </a> </div> </div> </div> <hr/> <h4 style="margin-bottom: 1rem;">üìù Recent Posts</h4> <ul class="post-list"> </ul> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Feedback LLM to improve their performance]]></summary></entry></feed>