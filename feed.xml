<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://debjitpaul.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debjitpaul.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-28T22:44:27+00:00</updated><id>https://debjitpaul.github.io/feed.xml</id><title type="html">blank</title><subtitle>Updating it, Work in Progress, My personal website</subtitle><entry><title type="html">Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures</title><link href="https://debjitpaul.github.io/blog/2025/compute/" rel="alternate" type="text/html" title="Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures"/><published>2025-09-28T00:00:00+00:00</published><updated>2025-09-28T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2025/compute</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2025/compute/"><![CDATA[<p>As Large Language Models (LLMs) continue to scale exponentially, understanding their computational requirements becomes crucial for researchers and practitioners alike. Whether you‚Äôre planning a training run, optimizing infrastructure costs, or comparing different architectures, <strong>FLOPs</strong> (Floating Point Operations) and <strong>MFU</strong> (Model FLOPs Utilization) are essential metrics that provide hardware-independent ways to measure and optimize computational efficiency.</p> <p>This post builds upon two excellent foundational resources: <strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs comprehensive guide to Transformer FLOPs</a></strong> and <strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs practical guide to FLOPs in LLM training</a></strong>. While these posts cover dense Transformer architectures, this post extends their insights to modern Mixture-of-Experts (MoE) architectures, with a particular focus on OpenAI‚Äôs recently released GPT-OSS models.</p> <p>In this comprehensive analysis, we‚Äôll explore FLOP counting methodologies, dive deep into MFU calculations, and examine how these concepts apply to the latest generation of sparse models that are reshaping the efficiency landscape of large language models.</p> <h2 id="-what-are-flops-and-why-do-they-matter">üî¢ What Are FLOPs and Why Do They Matter?</h2> <h3 id="defining-flops">Defining FLOPs</h3> <p>A <strong>FLOP</strong> (Floating Point Operation) represents a single computational operation like addition (<code class="language-plaintext highlighter-rouge">3.8 + 4.1</code>) or multiplication (<code class="language-plaintext highlighter-rouge">1.2 √ó 4.6</code>). When we talk about <strong>FLOPs</strong> (plural), we‚Äôre counting the total number of these atomic operations required for a specific task.</p> <p>For large-scale computations, we use:</p> <ul> <li><strong>GFLOPs</strong> = 1 billion FLOPs (10‚Åπ)</li> <li><strong>TFLOPs</strong> = 1 trillion FLOPs (10¬π¬≤)</li> <li><strong>PFLOPs</strong> = 1 quadrillion FLOPs (10¬π‚Åµ)</li> </ul> <h3 id="why-flops-matter">Why FLOPs Matter</h3> <p>FLOPs provide several crucial advantages:</p> <ol> <li><strong>Hardware Independence</strong>: Unlike wall-clock time, FLOPs offer consistent measurements across different hardware configurations</li> <li><strong>Reproducibility</strong>: Enable precise comparisons between different models and training setups</li> <li><strong>Cost Estimation</strong>: Help predict computational costs and resource requirements</li> <li><strong>Efficiency Analysis</strong>: Allow measurement of how well we utilize available hardware</li> </ol> <h2 id="-counting-flops-in-dense-transformers">üìê Counting FLOPs in Dense Transformers</h2> <h3 id="the-openai-method">The OpenAI Method</h3> <p>The seminal approach from OpenAI‚Äôs scaling laws paper [1] provides a clean approximation:</p> <p><strong>FLOPs per token ‚âà 6N</strong></p> <p>Where <code class="language-plaintext highlighter-rouge">N</code> is the number of non-embedding parameters. This factor of 6 accounts for:</p> <ul> <li><strong>2√ó</strong> for the forward pass (multiply-accumulate operations)</li> <li><strong>2√ó</strong> for the backward pass (gradients with respect to inputs)</li> <li><strong>2√ó</strong> for the backward pass (gradients with respect to parameters)</li> </ul> <p>This approximation, introduced by Kaplan et al. in their foundational work on neural language model scaling laws, provides a hardware-independent way to estimate computational requirements [1]. The elegance of this formula lies in its simplicity while capturing the essential computational structure of Transformer training.</p> <p>Let‚Äôs break down the forward pass components:</p> <table> <thead> <tr> <th>Operation</th> <th>Parameters</th> <th>FLOPs per Token</th> </tr> </thead> <tbody> <tr> <td><strong>Attention QKV</strong></td> <td><code class="language-plaintext highlighter-rouge">3 √ó d √ó d_model</code></td> <td><code class="language-plaintext highlighter-rouge">6 √ó L √ó d_model √ó d</code></td> </tr> <tr> <td><strong>Attention Scores</strong></td> <td>-</td> <td><code class="language-plaintext highlighter-rouge">4 √ó L √ó seq_len √ó d</code></td> </tr> <tr> <td><strong>Attention Project</strong></td> <td><code class="language-plaintext highlighter-rouge">d √ó d_model</code></td> <td><code class="language-plaintext highlighter-rouge">2 √ó L √ó d √ó d_model</code></td> </tr> <tr> <td><strong>Feedforward</strong></td> <td><code class="language-plaintext highlighter-rouge">8 √ó d_model¬≤</code></td> <td><code class="language-plaintext highlighter-rouge">16 √ó L √ó d_model¬≤</code></td> </tr> <tr> <td><strong>Total (approx)</strong></td> <td>-</td> <td><code class="language-plaintext highlighter-rouge">‚âà 2 √ó N</code></td> </tr> </tbody> </table> <p>Where:</p> <ul> <li><code class="language-plaintext highlighter-rouge">L</code> = number of layers</li> <li><code class="language-plaintext highlighter-rouge">seq_len</code> = sequence length</li> <li><code class="language-plaintext highlighter-rouge">d_model</code> = hidden dimension</li> <li><code class="language-plaintext highlighter-rouge">d</code> = attention dimension (<code class="language-plaintext highlighter-rouge">d_model</code> for most implementations)</li> </ul> <h3 id="the-deepmind-method">The DeepMind Method</h3> <p>DeepMind‚Äôs Chinchilla paper provides a more detailed accounting that includes embeddings, logits, and attention mechanics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">deepmind_flops_per_sequence</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">ff_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">DeepMind method for forward pass FLOPs counting</span><span class="sh">"""</span>
    <span class="n">d_attn</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
    <span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">ff_ratio</span>

    <span class="c1"># Components
</span>    <span class="n">embeddings</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="n">d_model</span>
    <span class="n">attn_qkv</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_softmax</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">attn_reduce</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_project</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_model</span>

    <span class="n">total_attn</span> <span class="o">=</span> <span class="n">attn_qkv</span> <span class="o">+</span> <span class="n">attn_logits</span> <span class="o">+</span> <span class="n">attn_softmax</span> <span class="o">+</span> <span class="n">attn_reduce</span> <span class="o">+</span> <span class="n">attn_project</span>
    <span class="n">feedforward</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">vocab_size</span>

    <span class="k">return</span> <span class="n">embeddings</span> <span class="o">+</span> <span class="n">n_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">total_attn</span> <span class="o">+</span> <span class="n">feedforward</span><span class="p">)</span> <span class="o">+</span> <span class="n">logits</span>
</code></pre></div></div> <h3 id="practical-example-gpt-3-scale-model">Practical Example: GPT-3 Scale Model</h3> <p>Let‚Äôs calculate FLOPs for a GPT-3 scale model:</p> <ul> <li><strong>Parameters</strong>: 175B non-embedding</li> <li><strong>Sequence length</strong>: 2048</li> <li><strong>Batch size</strong>: 32</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using OpenAI approximation
</span><span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="mf">175e9</span>  <span class="c1"># 1.05 √ó 10^12 FLOPs
</span><span class="n">tokens_per_step</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">2048</span>  <span class="c1"># 65,536 tokens
</span><span class="n">flops_per_step</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">tokens_per_step</span>  <span class="c1"># 6.87 √ó 10^16 FLOPs
</span>
<span class="c1"># Forward + Backward
</span><span class="n">total_flops_per_step</span> <span class="o">=</span> <span class="n">flops_per_step</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># 1.37 √ó 10^17 FLOPs ‚âà 137 PFLOPs
</span></code></pre></div></div> <h2 id="-model-flops-utilization-mfu-the-gold-standard">‚ö° Model FLOPs Utilization (MFU): The Gold Standard</h2> <h3 id="understanding-mfu">Understanding MFU</h3> <p><strong>Model FLOPs Utilization (MFU)</strong> measures how efficiently we execute the theoretically necessary FLOPs for training, introduced in Google‚Äôs PaLM paper:</p> <p><strong>MFU = (Model FLOPs √ó Throughput) / Peak Hardware FLOPs</strong></p> <p>Where:</p> <ul> <li><strong>Model FLOPs</strong>: Theoretical FLOPs per token (using OpenAI‚Äôs 6N approximation)</li> <li><strong>Throughput</strong>: Observed tokens processed per second</li> <li><strong>Peak Hardware FLOPs</strong>: Theoretical maximum FLOP/s of your hardware</li> </ul> <h3 id="mfu-vs-hardware-flops-utilization-hfu">MFU vs. Hardware FLOPs Utilization (HFU)</h3> <table> <thead> <tr> <th>Metric</th> <th><strong>MFU</strong></th> <th><strong>HFU</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Includes</strong></td> <td>Only necessary model computations</td> <td>All computations (including overheads)</td> </tr> <tr> <td><strong>Use Case</strong></td> <td>Fair comparison across setups</td> <td>Implementation efficiency</td> </tr> <tr> <td><strong>Affected by</strong></td> <td>Model architecture, batch size</td> <td>Memory management, communication</td> </tr> </tbody> </table> <h3 id="calculating-mfu-a-practical-example">Calculating MFU: A Practical Example</h3> <p>Consider training a 7B parameter model on 8√óA100 GPUs:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model specifications
</span><span class="n">parameters</span> <span class="o">=</span> <span class="mf">7e9</span>
<span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">parameters</span>  <span class="c1"># 4.2 √ó 10^10
</span>
<span class="c1"># Hardware specifications
</span><span class="n">a100_peak_flops</span> <span class="o">=</span> <span class="mf">312e12</span>  <span class="c1"># 312 TFLOPs for bf16
</span><span class="n">total_peak_flops</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">a100_peak_flops</span>  <span class="c1"># 2.496 √ó 10^15
</span>
<span class="c1"># Measured throughput
</span><span class="n">tokens_per_second</span> <span class="o">=</span> <span class="mi">8000</span>

<span class="c1"># Calculate MFU
</span><span class="n">sustained_flops</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">tokens_per_second</span>  <span class="c1"># 3.36 √ó 10^14
</span><span class="n">mfu</span> <span class="o">=</span> <span class="n">sustained_flops</span> <span class="o">/</span> <span class="n">total_peak_flops</span>  <span class="c1"># 0.135 = 13.5%
</span></code></pre></div></div> <h3 id="typical-mfu-ranges">Typical MFU Ranges</h3> <p>Real-world MFU values vary significantly:</p> <ul> <li><strong>Small models (&lt; 1B)</strong>: 10-30%</li> <li><strong>Medium models (1B-10B)</strong>: 30-50%</li> <li><strong>Large models (10B-100B)</strong>: 45-65%</li> <li><strong>Very large models (100B+)</strong>: 50-70%</li> </ul> <p>Higher MFU in larger models is often due to better compute-to-communication ratios and improved memory bandwidth utilization.</p> <h2 id="-extending-to-mixture-of-experts-moe-architectures">üß† Extending to Mixture-of-Experts (MoE) Architectures</h2> <h3 id="moe-fundamentals">MoE Fundamentals</h3> <p>Mixture-of-Experts architectures replace dense feedforward networks with a router and multiple expert networks. Only a subset of experts (typically 1-2 out of 8-64) are activated per token, dramatically reducing computational costs while maintaining or improving model quality.</p> <h3 id="moe-flop-counting-challenges">MoE FLOP Counting Challenges</h3> <p>Traditional FLOP counting becomes more nuanced with MoE:</p> <ol> <li><strong>Routing Overhead</strong>: Additional FLOPs for expert selection</li> <li><strong>Variable Computation</strong>: Different tokens may use different experts</li> <li><strong>Load Balancing</strong>: Uneven expert utilization affects total FLOPs</li> <li><strong>Communication Costs</strong>: Expert routing across devices</li> </ol> <h3 id="moe-flop-formula">MoE FLOP Formula</h3> <p>For a MoE layer replacing a dense feedforward network:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">moe_flops_per_token</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span> <span class="n">experts_per_token</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">load_balance_factor</span><span class="o">=</span><span class="mf">1.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Calculate FLOPs for MoE layer</span><span class="sh">"""</span>

    <span class="c1"># Router FLOPs (token ‚Üí expert probabilities)
</span>    <span class="n">router_flops</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">num_experts</span>

    <span class="c1"># Expert FLOPs (only active experts)
</span>    <span class="n">expert_flops</span> <span class="o">=</span> <span class="n">experts_per_token</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Load balancing overhead
</span>    <span class="n">effective_expert_flops</span> <span class="o">=</span> <span class="n">expert_flops</span> <span class="o">*</span> <span class="n">load_balance_factor</span>

    <span class="k">return</span> <span class="n">router_flops</span> <span class="o">+</span> <span class="n">effective_expert_flops</span>

<span class="c1"># Example: 8 experts, top-2 routing
</span><span class="n">dense_ff_flops</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">*</span> <span class="n">d_model</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># 4x expansion, two linear layers
</span><span class="n">moe_ff_flops</span> <span class="o">=</span> <span class="nf">moe_flops_per_token</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">efficiency_gain</span> <span class="o">=</span> <span class="n">dense_ff_flops</span> <span class="o">/</span> <span class="n">moe_ff_flops</span>  <span class="c1"># Typically 2-4x
</span></code></pre></div></div> <h2 id="-case-study-openai-gpt-oss-models">üöÄ Case Study: OpenAI GPT-OSS Models</h2> <p>OpenAI‚Äôs GPT-OSS models provide an excellent real-world example of modern MoE architectures with several innovative features:</p> <h3 id="gpt-oss-architecture-overview">GPT-OSS Architecture Overview</h3> <p><strong>GPT-OSS-120B</strong>:</p> <ul> <li><strong>Total Parameters</strong>: 117B (5.1B active per token)</li> <li><strong>Architecture</strong>: MoE with native MXFP4 quantization</li> <li><strong>Activation</strong>: ~4.4% of total parameters per token</li> <li><strong>Memory</strong>: Fits on single H100 (80GB)</li> </ul> <p><strong>GPT-OSS-20B</strong>:</p> <ul> <li><strong>Total Parameters</strong>: 21B (3.6B active per token)</li> <li><strong>Activation</strong>: ~17% of total parameters per token</li> <li><strong>Memory</strong>: Runs in 16GB</li> </ul> <h3 id="flop-counting-for-gpt-oss">FLOP Counting for GPT-OSS</h3> <p>Let‚Äôs calculate FLOPs for GPT-OSS-120B:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gpt_oss_flops_calculation</span><span class="p">():</span>
    <span class="c1"># GPT-OSS-120B specifications (estimated)
</span>    <span class="n">total_params</span> <span class="o">=</span> <span class="mf">117e9</span>
    <span class="n">active_params_per_token</span> <span class="o">=</span> <span class="mf">5.1e9</span>

    <span class="c1"># Traditional approach (if it were dense)
</span>    <span class="n">dense_flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">total_params</span>  <span class="c1"># 702 GFLOPs
</span>
    <span class="c1"># MoE approach (actual)
</span>    <span class="c1"># Attention layers remain dense
</span>    <span class="n">attention_params</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mf">0.3</span>  <span class="c1"># Estimated 30%
</span>    <span class="n">attention_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">attention_params</span>

    <span class="c1"># MoE feedforward layers
</span>    <span class="n">moe_params_per_token</span> <span class="o">=</span> <span class="n">active_params_per_token</span> <span class="o">-</span> <span class="n">attention_params</span>
    <span class="n">moe_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">moe_params_per_token</span>

    <span class="c1"># Router overhead (small)
</span>    <span class="n">router_flops</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c1"># Estimated 0.1%
</span>
    <span class="n">total_flops_per_token</span> <span class="o">=</span> <span class="n">attention_flops</span> <span class="o">+</span> <span class="n">moe_flops</span> <span class="o">+</span> <span class="n">router_flops</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">dense_equivalent</span><span class="sh">'</span><span class="p">:</span> <span class="n">dense_flops_per_token</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">actual_moe</span><span class="sh">'</span><span class="p">:</span> <span class="n">total_flops_per_token</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">efficiency_gain</span><span class="sh">'</span><span class="p">:</span> <span class="n">dense_flops_per_token</span> <span class="o">/</span> <span class="n">total_flops_per_token</span>
    <span class="p">}</span>

<span class="n">results</span> <span class="o">=</span> <span class="nf">gpt_oss_flops_calculation</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dense equivalent: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">dense_equivalent</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">e</span><span class="si">}</span><span class="s"> FLOPs/token</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">MoE actual: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">actual_moe</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">e</span><span class="si">}</span><span class="s"> FLOPs/token</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Efficiency gain: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">efficiency_gain</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">x</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="mxfp4-quantization-impact">MXFP4 Quantization Impact</h3> <p>GPT-OSS uses native MXFP4 quantization for MoE layers:</p> <ul> <li><strong>Memory</strong>: 4-bit storage vs 16-bit (4√ó reduction)</li> <li><strong>Compute</strong>: Specialized kernels maintain computational efficiency</li> <li><strong>Accuracy</strong>: Minimal degradation with proper scaling</li> </ul> <p>This affects FLOP counting considerations:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quantized_moe_flops</span><span class="p">(</span><span class="n">base_flops</span><span class="p">,</span> <span class="n">quantization_overhead</span><span class="o">=</span><span class="mf">1.05</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    MXFP4 might have slight computational overhead
    but significant memory bandwidth benefits
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">base_flops</span> <span class="o">*</span> <span class="n">quantization_overhead</span>
</code></pre></div></div> <h3 id="performance-analysis">Performance Analysis</h3> <p>Based on the GPT-OSS specifications, we can estimate performance characteristics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Estimated GPT-OSS-120B on H100
</span><span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="mf">5.1e9</span>  <span class="c1"># 30.6 GFLOPs (active parameters)
</span><span class="n">h100_peak</span> <span class="o">=</span> <span class="mf">1980e12</span>  <span class="c1"># ~2 PFLOPs for int4 operations
</span><span class="n">target_throughput</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># tokens/second (estimated)
</span>
<span class="n">required_flops_rate</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">target_throughput</span>  <span class="c1"># 3.06 TFLOPs/s
</span><span class="n">mfu</span> <span class="o">=</span> <span class="n">required_flops_rate</span> <span class="o">/</span> <span class="n">h100_peak</span>  <span class="c1"># 0.15%
</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Estimated MFU for GPT-OSS-120B: </span><span class="si">{</span><span class="n">mfu</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><em>Note: These are rough estimates. Actual performance depends on implementation details, memory bandwidth, and other factors.</em></p> <h2 id="-practical-considerations-and-optimizations">üîß Practical Considerations and Optimizations</h2> <h3 id="memory-vs-compute-trade-offs">Memory vs. Compute Trade-offs</h3> <p>Modern LLM training involves several techniques that affect FLOP calculations:</p> <p><strong>Activation Checkpointing</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">checkpointing_flop_overhead</span><span class="p">(</span><span class="n">base_flops</span><span class="p">,</span> <span class="n">checkpoint_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Recompute activations during backward pass
    checkpoint_ratio: fraction of activations checkpointed
    </span><span class="sh">"""</span>
    <span class="n">recompute_flops</span> <span class="o">=</span> <span class="n">base_flops</span> <span class="o">*</span> <span class="n">checkpoint_ratio</span>
    <span class="k">return</span> <span class="n">base_flops</span> <span class="o">+</span> <span class="n">recompute_flops</span>  <span class="c1"># Up to 1.5√ó FLOPs
</span></code></pre></div></div> <p><strong>Gradient Accumulation</strong>:</p> <ul> <li>Doesn‚Äôt affect per-token FLOPs</li> <li>May affect MFU due to different memory access patterns</li> </ul> <h3 id="scaling-laws-and-flop-optimal-training">Scaling Laws and FLOP-Optimal Training</h3> <p>The relationship between model size, dataset size, and computational budget follows predictable scaling laws:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_optimal_scaling</span><span class="p">(</span><span class="n">compute_budget_flops</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Based on Chinchilla scaling laws
    Roughly: parameters ‚àù compute^0.5, tokens ‚àù compute^0.5
    </span><span class="sh">"""</span>
    <span class="n">optimal_params</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_budget_flops</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">optimal_tokens</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_budget_flops</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>

    <span class="k">return</span> <span class="n">optimal_params</span><span class="p">,</span> <span class="n">optimal_tokens</span>

<span class="c1"># Example: 10^23 FLOPs budget
</span><span class="n">params</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nf">compute_optimal_scaling</span><span class="p">(</span><span class="mf">1e23</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal: </span><span class="si">{</span><span class="n">params</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">B parameters, </span><span class="si">{</span><span class="n">tokens</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">B tokens</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-benchmarking-and-measurement-tools">üìä Benchmarking and Measurement Tools</h2> <h3 id="measuring-mfu-in-practice">Measuring MFU in Practice</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">measure_mfu</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Measure MFU during actual training</span><span class="sh">"""</span>
    <span class="n">device</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span>

    <span class="c1"># Create dummy batch
</span>    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50000</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Warm up
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">).</span><span class="n">loss</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">).</span><span class="n">loss</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="c1"># Calculate throughput
</span>    <span class="n">total_tokens</span> <span class="o">=</span> <span class="n">num_steps</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">throughput</span> <span class="o">=</span> <span class="n">total_tokens</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

    <span class="c1"># Calculate MFU
</span>    <span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">model</span><span class="p">.</span><span class="nf">num_parameters</span><span class="p">()</span>
    <span class="n">peak_flops</span> <span class="o">=</span> <span class="nf">get_device_peak_flops</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mfu</span> <span class="o">=</span> <span class="p">(</span><span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">throughput</span><span class="p">)</span> <span class="o">/</span> <span class="n">peak_flops</span>

    <span class="k">return</span> <span class="n">mfu</span><span class="p">,</span> <span class="n">throughput</span>
</code></pre></div></div> <h3 id="profiling-tools">Profiling Tools</h3> <ul> <li><strong>PyTorch Profiler</strong>: Built-in FLOP counting</li> <li><strong>DeepSpeed</strong>: MFU reporting in training logs</li> <li><strong>Weights &amp; Biases</strong>: Integration with hardware metrics</li> <li><strong>Custom counters</strong>: Framework-specific implementations</li> </ul> <h2 id="-future-directions-and-emerging-architectures">üîÆ Future Directions and Emerging Architectures</h2> <h3 id="beyond-traditional-flops">Beyond Traditional FLOPs</h3> <p>As architectures evolve, FLOP counting methodologies must adapt:</p> <p><strong>Sparse Attention Patterns</strong>:</p> <ul> <li>Linear attention: O(n) instead of O(n¬≤)</li> <li>Local attention: Reduced sequence length dependencies</li> <li>Mixture of attention heads: Different patterns per head</li> </ul> <p><strong>Conditional Computation</strong>:</p> <ul> <li>Early exit mechanisms</li> <li>Adaptive depth networks</li> <li>Token-wise routing</li> </ul> <p><strong>Hardware-Aware Metrics</strong>:</p> <ul> <li>Memory bandwidth utilization</li> <li>Integer operation efficiency</li> <li>Specialized accelerator metrics</li> </ul> <h3 id="the-road-ahead">The Road Ahead</h3> <p>The future of computational efficiency in LLMs likely involves:</p> <ol> <li><strong>Multi-modal MoE</strong>: Extending sparse computation to different modalities</li> <li><strong>Dynamic architectures</strong>: Runtime adaptation based on input complexity</li> <li><strong>Hardware co-design</strong>: Models designed for specific accelerators</li> <li><strong>Hybrid precision</strong>: Strategic use of different numerical formats</li> </ol> <h2 id="-conclusion">üèÅ Conclusion</h2> <p>Understanding FLOPs and MFU is crucial for anyone working with large-scale language models. As we‚Äôve seen:</p> <ul> <li><strong>FLOPs provide hardware-independent measurement</strong> of computational requirements</li> <li><strong>MFU enables fair comparison</strong> of training efficiency across different setups</li> <li><strong>MoE architectures complicate</strong> but don‚Äôt invalidate these fundamental concepts</li> <li><strong>Modern models like GPT-OSS</strong> showcase how innovative architectures and quantization can dramatically improve efficiency</li> </ul> <p>Whether you‚Äôre planning a training run, optimizing an existing system, or designing new architectures, these metrics provide essential insights into the computational reality of modern AI systems.</p> <p>As models continue to scale and new architectures emerge, the principles of FLOP counting and efficiency measurement will remain foundational tools for understanding and optimizing the computational landscape of artificial intelligence.</p> <hr/> <h2 id="-acknowledgments">üôè Acknowledgments</h2> <p>This blog post draws heavily from two excellent resources that provided the foundation for understanding FLOPs in LLM training. I want to give full credit to these authors for their pioneering work:</p> <p><strong>Primary Inspirations:</strong></p> <ul> <li> <p><strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs ‚ÄúTransformer FLOPs‚Äù</a></strong> - An exceptionally clear and comprehensive guide that masterfully explains FLOP counting methodologies, MFU calculations, and scaling behaviors. Much of the mathematical foundation and practical examples in this post are built upon Adam‚Äôs excellent work.</p> </li> <li> <p><strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs ‚ÄúFLOPs in LLM Training: The Ultimate Guide‚Äù</a></strong> - A practical and accessible guide that bridges the gap between theory and implementation, providing clear examples and real-world context for FLOP calculations.</p> </li> </ul> <p>Both posts were instrumental in shaping my understanding of computational efficiency in LLMs. This work extends their insights to MoE architectures and provides additional analysis of modern models like GPT-OSS, but the core concepts and methodologies owe much to these foundational resources.</p> <h2 id="-references-and-further-reading">üìö References and Further Reading</h2> <p><strong>Foundational Papers:</strong></p> <ul> <li><strong>[1] <a href="https://arxiv.org/abs/2001.08361">OpenAI Scaling Laws</a></strong>: Kaplan, J., McCandlish, S., Henighan, T., et al. ‚ÄúScaling Laws for Neural Language Models‚Äù (2020) - Original FLOP counting methodology and the 6N approximation</li> <li><strong>[2] <a href="https://arxiv.org/abs/2203.15556">DeepMind Chinchilla</a></strong>: Hoffmann, J., Borgeaud, S., Mensch, A., et al. ‚ÄúTraining Compute-Optimal Large Language Models‚Äù (2022) - Detailed FLOP accounting and compute-optimal training</li> <li><strong>[3] <a href="https://arxiv.org/abs/2204.02311">Google PaLM</a></strong>: Chowdhery, A., Narang, S., Devlin, J., et al. ‚ÄúPaLM: Scaling Language Modeling with Pathways‚Äù (2022) - MFU methodology introduction</li> </ul> <p><strong>Key Blog Posts and Resources:</strong></p> <ul> <li><strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs Transformer FLOPs</a></strong>: Comprehensive FLOP analysis with interactive calculator</li> <li><strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs FLOPs Guide</a></strong>: Practical guide to FLOPs in LLM training</li> <li><strong><a href="https://github.com/openai/gpt-oss">GPT-OSS Repository</a></strong>: OpenAI‚Äôs open-source MoE implementation</li> </ul> <p><strong>Additional Technical Resources:</strong></p> <ul> <li><strong><a href="https://kipp.ly/blog/transformer-inference-arithmetic/">Transformer Inference Arithmetic</a></strong>: Kipp Bradford‚Äôs analysis of inference costs</li> <li><strong><a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr</a></strong>: Horace He‚Äôs guide to optimization fundamentals</li> </ul> <hr/> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">paul2024flops_mfu_moe</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">{https://debjitpaul.github.io/blog/}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/> <p><em>This analysis provides a comprehensive overview of computational efficiency in modern LLM training. For specific implementation details or model-specific calculations, always refer to the original papers and codebases.</em></p>]]></content><author><name></name></author><category term="research"/><category term="technical"/><category term="flops"/><category term="mfu"/><category term="llm-training"/><category term="moe"/><category term="gpt-oss"/><category term="computational-efficiency"/><category term="transformers"/><summary type="html"><![CDATA[A comprehensive guide to counting FLOPs in LLM training, measuring Model FLOPs Utilization (MFU), and extending these concepts to Mixture-of-Experts architectures with a deep dive into OpenAI's GPT-OSS models.]]></summary></entry><entry><title type="html">FIPO: Fallacy-Informed Preference Optimization - Steering LLMs Toward Logically Sound Arguments</title><link href="https://debjitpaul.github.io/blog/2025/fipo-outstanding-paper-award/" rel="alternate" type="text/html" title="FIPO: Fallacy-Informed Preference Optimization - Steering LLMs Toward Logically Sound Arguments"/><published>2025-01-15T00:00:00+00:00</published><updated>2025-01-15T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2025/fipo-outstanding-paper-award</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2025/fipo-outstanding-paper-award/"><![CDATA[<p>üèÜ <strong>Outstanding Paper Award Winner at NAACL 2025!</strong></p> <p>I‚Äôm thrilled to share that our paper ‚ÄúA Logical Fallacy-Informed Framework for Argument Generation‚Äù has received the <strong>Outstanding Paper Award</strong> at the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2025)!</p> <h2 id="-the-problem-llms-generate-fallacious-arguments">üö® The Problem: LLMs Generate Fallacious Arguments</h2> <p>Despite their remarkable capabilities, Large Language Models (LLMs) still struggle with generating logically sound arguments, often producing content that contains <strong>logical fallacies</strong> - errors in reasoning that undermine the validity of an argument. Our preliminary study revealed a startling finding: <strong>21% of arguments generated by ChatGPT contain logical fallacies</strong>.</p> <p>Consider these examples of fallacious vs. logically sound arguments:</p> <table> <thead> <tr> <th><strong>Topic</strong>: AI is bad for the job market (Support)</th> </tr> </thead> <tbody> <tr> <td>‚ùå <strong>Ad Hominem</strong>: ‚ÄúAI proponents are tech enthusiasts disconnected from real-world job market issues.‚Äù</td> </tr> <tr> <td>‚ùå <strong>Circular Reasoning</strong>: ‚ÄúAI is harmful to the employment sector because AI is bad for the job market.‚Äù</td> </tr> <tr> <td>‚ùå <strong>Appeal to Emotion</strong>: ‚ÄúAI will leave millions of families struggling and unemployed.‚Äù</td> </tr> <tr> <td>‚úÖ <strong>Logically Sound</strong>: ‚ÄúAI automates tasks, reducing employment opportunities by replacing humans in manufacturing and administrative jobs.‚Äù</td> </tr> </tbody> </table> <h2 id="-our-solution-fallacy-informed-preference-optimization-fipo">üí° Our Solution: Fallacy-Informed Preference Optimization (FIPO)</h2> <p>We introduce <strong>FIPO</strong>, a novel framework that helps steer LLMs toward generating logically sound arguments by making them explicitly aware of logical fallacies during training.</p> <h3 id="key-innovation-weighted-classification-loss">Key Innovation: Weighted Classification Loss</h3> <p>FIPO combines traditional preference optimization with a <strong>weighted cross-entropy classification loss</strong> that penalizes the model based on fallacy frequency, applying stronger penalties for misclassifying more common fallacies.</p> <p>The FIPO loss function is:</p> <p><strong>L<sub>FIPO</sub>(œÄ<sub>Œ∏</sub>) = L<sub>CPO</sub>(œÄ<sub>Œ∏</sub>) + ŒªL<sub>CLF</sub>(œÄ<sub>Œ∏</sub>)</strong></p> <p>Where the classification loss L<sub>CLF</sub> is defined as:</p> <table> <tbody> <tr> <td>**L<sub>CLF</sub>(œÄ<sub>Œ∏</sub>) = -E<sub>(t,s,y<sub>w</sub>,y<sub>l</sub>,k)‚àºD‚Äô</sub>[w<sub>0</sub> log P<sup>0</sup><sub>h<sub>Œ∏</sub></sub>(y<sub>w</sub></td> <td>t,s) + w<sub>k</sub> log P<sup>k</sup><sub>h<sub>Œ∏</sub></sub>(y<sub>l</sub></td> <td>t,s)]**</td> </tr> </tbody> </table> <p>Here:</p> <ul> <li><strong>w<sub>k</sub></strong>: Weights based on fallacy frequency in training data</li> <li><strong>P<sup>k</sup><sub>h<sub>Œ∏</sub></sub></strong>: Probability of fallacy type k from classification head</li> <li><strong>Œª = 0.3</strong>: Balancing parameter (optimized through experiments)</li> </ul> <h2 id="-the-13-types-of-logical-fallacies">üß† The 13 Types of Logical Fallacies</h2> <p>We define 13 categories of logical fallacies based on centuries of logical reasoning research dating back to Aristotle:</p> <h3 id="most-common-fallacies-in-our-dataset"><strong>Most Common Fallacies (in our dataset):</strong></h3> <ol> <li> <p><strong>Faulty Generalization (18.0%)</strong> - Drawing conclusions about all instances from limited examples</p> <ul> <li><em>Example: ‚ÄúI know someone who smoked cannabis and became successful. Therefore, everyone who smokes cannabis will be successful.‚Äù</em></li> </ul> </li> <li> <p><strong>Ad Hominem (12.3%)</strong> - Attacking the person making the argument rather than the argument itself</p> <ul> <li><em>Example: ‚ÄúThose climate scientists are just trying to get more funding for their research.‚Äù</em></li> </ul> </li> <li> <p><strong>Ad Populum (9.5%)</strong> - Argument based on what the majority believes</p> <ul> <li><em>Example: ‚ÄúMost people think this policy is good, so it must be correct.‚Äù</em></li> </ul> </li> <li> <p><strong>False Causality (8.8%)</strong> - Implying causal relationships without supporting evidence</p> <ul> <li><em>Example: ‚ÄúEver since we installed those wind turbines, there have been more bird deaths in the area.‚Äù</em></li> </ul> </li> <li> <p><strong>Circular Reasoning (7.0%)</strong> - The conclusion comes back to the premise without proving itself</p> <ul> <li><em>Example: ‚ÄúWe should trust the news because the news says we should trust it.‚Äù</em></li> </ul> </li> </ol> <h3 id="other-fallacy-types"><strong>Other Fallacy Types:</strong></h3> <ul> <li><strong>Appeal to Emotion (6.8%)</strong> - Manipulating emotions rather than using logic</li> <li><strong>Fallacy of Relevance (6.6%)</strong> - Introducing irrelevant information</li> <li><strong>Fallacy of Logic (6.2%)</strong> - Errors in logical structure</li> <li><strong>Intentional (5.8%)</strong> - Deliberately wrong arguments</li> <li><strong>False Dilemma (5.8%)</strong> - Presenting only two options when many exist</li> <li><strong>Fallacy of Extension (5.8%)</strong> - Attacking exaggerated versions of arguments</li> <li><strong>Fallacy of Credibility (5.4%)</strong> - Attacking speaker‚Äôs character</li> <li><strong>Equivocation (2.0%)</strong> - Using ambiguous language</li> </ul> <p><img src="https://via.placeholder.com/600x400/4CAF50/FFFFFF?text=Fallacy+Type+Distribution+Based+on+LOGIC+Dataset" alt="Fallacy Distribution"/></p> <h2 id="-methodology-4-step-framework">üî¨ Methodology: 4-Step Framework</h2> <p>Our approach involves four key steps:</p> <h3 id="step-1-supervised-fine-tuning-sft"><strong>Step 1: Supervised Fine-Tuning (SFT)</strong></h3> <p>Train the base model on the ExplaGraphs dataset containing topics, stances, and arguments.</p> <h3 id="step-2-preference-data-collection"><strong>Step 2: Preference Data Collection</strong></h3> <p>Generate fallacious arguments using ChatGPT for each original argument, creating preference pairs where:</p> <ul> <li><strong>Preferred (y<sub>w</sub>)</strong>: Original logically sound argument</li> <li><strong>Dispreferred (y<sub>l</sub>)</strong>: Generated fallacious argument with label k</li> </ul> <p>We generated <strong>7,872 fallacious arguments</strong> spanning all 13 fallacy types following the natural distribution found in real-world discourse.</p> <h3 id="step-3-preference-optimization"><strong>Step 3: Preference Optimization</strong></h3> <p>Apply standard preference optimization methods (DPO, PPO, CPO, KTO) using the preference dataset.</p> <h3 id="step-4-fipo-enhancement"><strong>Step 4: FIPO Enhancement</strong></h3> <p>Add our weighted classification loss to the best-performing preference method (CPO) to create FIPO.</p> <h2 id="-outstanding-results">üìä Outstanding Results</h2> <h3 id="dramatic-fallacy-reduction"><strong>Dramatic Fallacy Reduction</strong></h3> <table> <thead> <tr> <th>Model</th> <th>Baseline (SFT)</th> <th>Best Previous Method</th> <th><strong>FIPO</strong></th> <th><strong>Improvement</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Llama-2 (7B)</strong></td> <td>34.5%</td> <td>26.0% (PPO)</td> <td><strong>17.0%</strong></td> <td><strong>17.5% reduction</strong></td> </tr> <tr> <td><strong>Mistral (7B)</strong></td> <td>32.5%</td> <td>27.75% (KTO)</td> <td><strong>19.5%</strong></td> <td><strong>13.0% reduction</strong></td> </tr> </tbody> </table> <h3 id="quality-improvements-win-rate"><strong>Quality Improvements (Win-Rate)</strong></h3> <p>FIPO not only reduces fallacies but also maintains high argument quality:</p> <ul> <li><strong>Human Evaluation Win-Rate</strong>: 46% (vs. 40.3% loss rate for CPO)</li> <li><strong>GPT-4 Evaluation Win-Rate</strong>: 63.5% (highest among all methods)</li> <li><strong>Significant reduction in ‚Äúloss rate‚Äù</strong>: From 40.3% (CPO) to 23% (FIPO)</li> </ul> <h3 id="fallacy-specific-performance"><strong>Fallacy-Specific Performance</strong></h3> <p>FIPO excels particularly at reducing the most common fallacy types:</p> <table> <thead> <tr> <th>Fallacy Type</th> <th>Llama-2 SFT</th> <th>Llama-2 FIPO</th> <th><strong>Reduction</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Faulty Generalization</strong></td> <td>27.5%</td> <td><strong>7.0%</strong></td> <td><strong>-20.5%</strong></td> </tr> <tr> <td><strong>False Causality</strong></td> <td>2.5%</td> <td><strong>3.5%</strong></td> <td>Controlled</td> </tr> <tr> <td><strong>Appeal to Emotion</strong></td> <td>1.0%</td> <td><strong>2.5%</strong></td> <td>Controlled</td> </tr> </tbody> </table> <p>The weighted classification loss ensures the model focuses on the most problematic and frequent fallacies.</p> <h2 id="-human-evaluation-validation">üîç Human Evaluation Validation</h2> <h3 id="gpt-4-reliability-verification"><strong>GPT-4 Reliability Verification</strong></h3> <p>We validated GPT-4‚Äôs ability to identify fallacies through human annotation:</p> <ul> <li><strong>Randolph‚Äôs Œ∫ agreement</strong>: 0.640 (substantial agreement)</li> <li><strong>Majority agreement ratio</strong>: 95.5% between annotators and GPT-4</li> </ul> <h3 id="comparative-analysis"><strong>Comparative Analysis</strong></h3> <p>200 arguments were independently classified by our team and compared with GPT-4:</p> <ul> <li>Strong alignment on most fallacy types</li> <li>Main discrepancy: Fallacy of Relevance detection</li> <li>Some confusion between Faulty Generalization and False Causality (expected due to subtle differences)</li> </ul> <h2 id="-ablation-studies-confirm-design-choices">üß™ Ablation Studies Confirm Design Choices</h2> <p>We validated our design through rigorous ablation studies:</p> <table> <thead> <tr> <th>Approach</th> <th>Fallacy Rate</th> <th><strong>Analysis</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Uniform Dataset Distribution</strong></td> <td>37.5%</td> <td>Natural distribution is crucial</td> </tr> <tr> <td><strong>Unweighted Cross-Entropy</strong></td> <td>29.0%</td> <td>Weighting by frequency is essential</td> </tr> <tr> <td><strong>FIPO (Full Method)</strong></td> <td><strong>17.0%</strong></td> <td><strong>Best performance</strong></td> </tr> </tbody> </table> <p>These results confirm that both the <strong>natural fallacy distribution</strong> and <strong>weighted classification loss</strong> are essential components of FIPO.</p> <h2 id="-generalization-out-of-domain-performance">üåç Generalization: Out-of-Domain Performance</h2> <p>FIPO‚Äôs effectiveness extends beyond training data:</p> <p><strong>Debatepedia Dataset Results:</strong></p> <ul> <li><strong>Fallacy Rate</strong>: 45% (second-best, close to KTO‚Äôs 44%)</li> <li><strong>Win Rate</strong>: 62% (highest among all methods)</li> <li><strong>Excellent at reducing</strong>: False Causality and Fallacy of Relevance</li> </ul> <p>This demonstrates FIPO‚Äôs ability to generalize to new domains and topics.</p> <h2 id="-implementation-details">üíª Implementation Details</h2> <h3 id="base-models"><strong>Base Models</strong></h3> <ul> <li><strong>Llama-2 (7B)</strong> and <strong>Mistral (7B)</strong></li> <li><strong>LoRA fine-tuning</strong>: Reduces parameters from 7B to 8.3M (0.12%)</li> </ul> <h3 id="training-configuration"><strong>Training Configuration</strong></h3> <ul> <li><strong>Classification loss weight (Œª)</strong>: 0.3 (optimal balance)</li> <li><strong>Fallacy weights (w<sub>k</sub>)</strong>: Based on natural frequency distribution</li> <li><strong>Base method</strong>: CPO (best trade-off between win-rate and fallacy-rate)</li> </ul> <h3 id="key-hyperparameters"><strong>Key Hyperparameters</strong></h3> <ul> <li><strong>Learning rate</strong>: 2√ó10‚Åª‚Å¥</li> <li><strong>LoRA rank</strong>: 16, Œ± = 32</li> <li><strong>Training epochs</strong>: 3</li> <li><strong>Œ≤ (CPO)</strong>: 0.1</li> </ul> <h2 id="-impact-and-future-directions">üîÆ Impact and Future Directions</h2> <h3 id="immediate-impact"><strong>Immediate Impact</strong></h3> <ul> <li><strong>First work</strong> to systematically address logical fallacies in argument generation</li> <li><strong>Novel framework</strong> combining preference optimization with classification objectives</li> <li><strong>Significant performance gains</strong> across multiple models and evaluation metrics</li> </ul> <h3 id="broader-implications"><strong>Broader Implications</strong></h3> <ul> <li><strong>Trust and Safety</strong>: Reduces spread of logically flawed arguments</li> <li><strong>Educational Applications</strong>: Could help teach logical reasoning</li> <li><strong>Democratic Discourse</strong>: Promotes more sound public debate</li> </ul> <h3 id="future-research-directions"><strong>Future Research Directions</strong></h3> <ol> <li><strong>Scaling to larger models</strong>: Apply FIPO to models &gt;10B parameters</li> <li><strong>Multi-domain extension</strong>: Expand to scientific, legal, and technical arguments</li> <li><strong>Real-time fallacy detection</strong>: Interactive systems for argument evaluation</li> <li><strong>Cross-lingual fallacy analysis</strong>: Extend to non-English languages</li> <li><strong>Integration with fact-checking</strong>: Combine logical and factual verification</li> </ol> <h2 id="-key-takeaways">üéØ Key Takeaways</h2> <ol> <li><strong>LLMs have a significant logical fallacy problem</strong>: Even advanced models like ChatGPT generate fallacious arguments 21% of the time</li> <li><strong>Explicit fallacy awareness helps</strong>: Making models aware of specific fallacy types significantly improves argument quality</li> <li><strong>Weighted classification loss is crucial</strong>: Focusing on frequent fallacies yields better overall performance</li> <li><strong>Preference optimization isn‚Äôt enough alone</strong>: Standard methods like DPO and PPO help but miss fine-grained fallacy distinctions</li> <li><strong>FIPO provides substantial improvements</strong>: Up to 17.5% reduction in fallacy rates while maintaining argument quality</li> </ol> <h2 id="-resources-and-code">üìö Resources and Code</h2> <ul> <li><strong>üìÑ Paper</strong>: <a href="https://aclanthology.org/2025.naacl-long.374.pdf">A Logical Fallacy-Informed Framework for Argument Generation</a></li> <li><strong>üíª Code &amp; Data</strong>: Available at <a href="https://github.com/lucamouchel/Logical-Fallacies">github.com/lucamouchel/Logical-Fallacies</a></li> <li><strong>üèÜ Award Recognition</strong>: Outstanding Paper Award - NAACL 2025</li> </ul> <hr/> <h2 id="-acknowledgments">üë• Acknowledgments</h2> <p>This work was a collaborative effort with amazing researchers:</p> <ul> <li><strong>Luca Mouchel</strong> (Lead Author, Master Internship)</li> <li><strong>Shaobo Cui</strong>, <strong>Robert West</strong>, <strong>Antoine Bosselut</strong>, <strong>Boi Faltings</strong></li> <li><strong>EPFL, Switzerland</strong></li> </ul> <p>Special thanks to the <strong>ICT-48 Network of AI Research Excellence Center ‚ÄúTAILOR‚Äù</strong>, the <strong>Swiss National Science Foundation</strong>, and our other funding partners.</p> <hr/> <h2 id="-citation">üìñ Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mouchel-etal-2025-logical</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"A Logical Fallacy-Informed Framework for Argument Generation"</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">"Mouchel, Luca and Paul, Debjit and Cui, Shaobo and West, Robert and Bosselut, Antoine and Faltings, Boi"</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">"Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">"7296--7314"</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">"Association for Computational Linguistics"</span>
<span class="p">}</span>
</code></pre></div></div> <hr/> <h2 id="-discussion-questions">ü§î Discussion Questions</h2> <p><strong>For Researchers:</strong></p> <ul> <li>How might FIPO be adapted for other types of reasoning tasks beyond argument generation?</li> <li>What other fine-grained classification objectives could improve preference optimization?</li> </ul> <p><strong>For Practitioners:</strong></p> <ul> <li>How can we integrate fallacy detection into real-world applications like social media platforms or educational tools?</li> <li>What are the computational trade-offs when adding classification heads to large language models?</li> </ul> <p><strong>For Society:</strong></p> <ul> <li>How do we balance improving logical reasoning with maintaining diverse perspectives in AI-generated content?</li> <li>What role should AI play in moderating online discourse and identifying fallacious arguments?</li> </ul> <hr/> <p><em>This research represents a significant step toward more trustworthy and logically sound AI systems. As LLMs become increasingly prevalent in decision-making and public discourse, ensuring they generate well-reasoned arguments is not just a technical challenge‚Äîit‚Äôs a societal imperative.</em></p> <p><strong>üèÜ Proud to have this work recognized with the Outstanding Paper Award at NAACL 2025!</strong></p>]]></content><author><name></name></author><category term="research"/><category term="awards"/><category term="fipo"/><category term="logical-fallacies"/><category term="preference-optimization"/><category term="argument-generation"/><category term="outstanding-paper"/><summary type="html"><![CDATA[üèÜ Outstanding Paper Award Winner at NAACL 2025! Introducing FIPO, a novel framework that reduces logical fallacy errors in LLM-generated arguments by up to 17.5% through fallacy-informed preference optimization.]]></summary></entry><entry><title type="html">Making Reasoning Matter: Measuring Faithfulness in Chain-of-Thought Reasoning</title><link href="https://debjitpaul.github.io/blog/2024/reasoning-matter/" rel="alternate" type="text/html" title="Making Reasoning Matter: Measuring Faithfulness in Chain-of-Thought Reasoning"/><published>2024-02-23T00:00:00+00:00</published><updated>2024-02-23T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2024/reasoning-matter</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2024/reasoning-matter/"><![CDATA[<p>Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks when prompted to generate step-by-step explanations, known as Chain-of-Thought (CoT) reasoning. However, a critical question remains: <strong>Do these models actually use their generated reasoning steps to arrive at their final answers?</strong></p> <h2 id="the-problem">The Problem</h2> <p>While CoT prompting improves performance on many reasoning tasks, recent studies suggest that models might not always rely on their intermediate reasoning steps. This raises concerns about the <strong>faithfulness</strong> of the reasoning process - whether the generated explanations truly reflect the model‚Äôs decision-making process.</p> <h2 id="our-approach">Our Approach</h2> <p>In our paper <a href="https://arxiv.org/abs/2402.13950">‚ÄúMaking Reasoning Matter‚Äù</a>, we introduce a comprehensive framework to measure and improve reasoning faithfulness using <strong>causal mediation analysis</strong>.</p> <h3 id="key-contributions">Key Contributions</h3> <ol> <li><strong>Causal Framework</strong>: We develop a method to quantify how much intermediate reasoning steps causally influence final predictions</li> <li><strong>Extensive Evaluation</strong>: Analysis across 11 different language models on multiple reasoning tasks</li> <li><strong>Practical Improvements</strong>: Techniques to enhance reasoning faithfulness</li> </ol> <h2 id="key-findings">Key Findings</h2> <p>Our causal mediation analysis across different model families reveals several important insights:</p> <h3 id="model-behavior-varies-by-training-objective">Model Behavior Varies by Training Objective</h3> <ul> <li><strong>In-context learning</strong> and <strong>instruction-tuning</strong> improve alignment with reasoning chains</li> <li>Models trained with <strong>RLHF</strong> show more direct effects than indirect effects, suggesting potential issues with faithful reasoning</li> <li><strong>Larger models</strong> don‚Äôt automatically show better faithfulness</li> </ul> <h3 id="task-specific-patterns">Task-Specific Patterns</h3> <p>We evaluated on three types of reasoning tasks:</p> <ul> <li><strong>Mathematical Reasoning</strong> (GSM8K)</li> <li><strong>Strategic Reasoning</strong> (StrategyQA)</li> <li><strong>Causal Understanding</strong></li> </ul> <p>Results show that faithfulness varies significantly across task types, with mathematical reasoning showing different patterns compared to strategic reasoning tasks.</p> <h2 id="methodology">Methodology</h2> <p>Our approach uses <strong>causal mediation analysis</strong> to decompose the total effect of reasoning problems into:</p> <ol> <li><strong>Direct Effect</strong>: How much the problem directly influences the answer (bypassing reasoning)</li> <li><strong>Indirect Effect</strong>: How much the problem influences the answer through generated reasoning steps</li> </ol> <p>High indirect effect indicates faithful reasoning, while high direct effect suggests the model might be ignoring its reasoning steps.</p> <h2 id="implications">Implications</h2> <p>This work has important implications for:</p> <ul> <li><strong>Model Development</strong>: Understanding which training objectives promote faithful reasoning</li> <li><strong>Evaluation</strong>: Moving beyond accuracy to assess reasoning quality</li> <li><strong>Trust and Interpretability</strong>: Building more reliable and transparent AI systems</li> </ul> <h2 id="future-directions">Future Directions</h2> <p>Our findings open several avenues for future research:</p> <ul> <li>Developing training methods that promote faithfulness</li> <li>Creating better evaluation metrics for reasoning quality</li> <li>Understanding the relationship between model scale and reasoning faithfulness</li> </ul> <hr/> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">debjit2024frodo</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul and Robert West and Antoine Bosselut and Boi Faltings}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13950}</span><span class="p">,</span>
    <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
    <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Links:</strong></p> <ul> <li><a href="https://arxiv.org/abs/2402.13950">Paper on arXiv</a></li> <li><a href="/reasoningmatter/">Project Page</a></li> <li><a href="https://github.com/debjitpaul/reasoning-matter">Code Repository</a> <em>(if available)</em></li> </ul> <hr/> <p><em>This post is part of our ongoing research into making AI reasoning more transparent and reliable. For more updates on our work, follow our <a href="/blog/">research blog</a>.</em></p>]]></content><author><name></name></author><category term="research"/><category term="reasoning"/><category term="chain-of-thought"/><category term="LLMs"/><category term="faithfulness"/><category term="causal-analysis"/><summary type="html"><![CDATA[A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.]]></summary></entry><entry><title type="html">REFINER</title><link href="https://debjitpaul.github.io/blog/" rel="alternate" type="text/html" title="REFINER"/><published>2023-04-04T00:00:00+00:00</published><updated>2023-04-04T00:00:00+00:00</updated><id>https://debjitpaul.github.io/refiner</id><content type="html" xml:base="https://debjitpaul.github.io/blog/"><![CDATA[<div class="post"> <div class="header-bar"> <h1>Research Blog</h1> <h2>Feedback LLM to improve their performance</h2> </div> <div class="container featured-research" style="margin-bottom: 2rem;"> <h3 style="margin-bottom: 1rem; color: var(--global-theme-color);">üî¨ Featured Research Projects</h3> <div class="row mb-4"> <div class="col-md-6 mb-4"> <div class="card hoverable h-100"> <div class="card-body"> <h4 class="card-title">Making Reasoning Matter</h4> <p class="card-text">Measuring and improving faithfulness of chain-of-thought reasoning in large language models using causal mediation analysis.</p> <div class="research-highlights mb-3"> <h6>Key Contributions:</h6> <ul class="list-unstyled"> <li><i class="fas fa-check-circle text-success"></i> Causal framework for measuring reasoning faithfulness</li> <li><i class="fas fa-check-circle text-success"></i> Analysis across 11 different language models</li> <li><i class="fas fa-check-circle text-success"></i> Insights on training objectives and reasoning quality</li> </ul> </div> <div class="research-meta mb-2"> <span class="badge badge-primary">arXiv:2402.13950</span> <span class="badge badge-secondary">Chain-of-Thought</span> <span class="badge badge-secondary">Causal Analysis</span> </div> <div class="mt-3"> <a href="/reasoningmatter/" class="btn btn-primary btn-sm"> <i class="fas fa-info-circle"></i> Project Details </a> <a href="https://arxiv.org/abs/2402.13950" target="_blank" class="btn btn-outline-secondary btn-sm"> <i class="fas fa-external-link-alt"></i> Read Paper </a> </div> </div> </div> </div> <div class="col-md-6 mb-4"> <div class="card hoverable h-100"> <div class="card-body"> <h4 class="card-title">REFINER</h4> <p class="card-text">A framework for improving language models' reasoning through iterative feedback from critic models on intermediate representations.</p> <div class="research-highlights mb-3"> <h6>Key Features:</h6> <ul class="list-unstyled"> <li><i class="fas fa-check-circle text-success"></i> Generator-Critic architecture for iterative refinement</li> <li><i class="fas fa-check-circle text-success"></i> Structured feedback on reasoning steps</li> <li><i class="fas fa-check-circle text-success"></i> Human-compatible feedback integration</li> </ul> </div> <div class="research-meta mb-2"> <span class="badge badge-primary">arXiv:2304.01904</span> <span class="badge badge-secondary">Feedback Learning</span> <span class="badge badge-secondary">Reasoning</span> </div> <div class="mt-3"> <a href="/refiner/" class="btn btn-primary btn-sm"> <i class="fas fa-info-circle"></i> Project Details </a> <a href="https://arxiv.org/abs/2304.01904" target="_blank" class="btn btn-outline-secondary btn-sm"> <i class="fas fa-external-link-alt"></i> Read Paper </a> </div> </div> </div> </div> </div> <div class="row"> <div class="col-md-12"> <div class="card hoverable"> <div class="card-body"> <h5 class="card-title">üß† Research Focus Areas</h5> <div class="row"> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-brain text-primary"></i> AI Reasoning</h6> <p class="small text-muted">Chain-of-thought reasoning, causal inference, knowledge representation, and logical reasoning processes in AI systems.</p> </div> </div> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-comments text-info"></i> Feedback &amp; Learning</h6> <p class="small text-muted">Iterative feedback mechanisms, critic models, human-AI interaction, and preference optimization for better model alignment.</p> </div> </div> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-search text-success"></i> Model Faithfulness</h6> <p class="small text-muted">Measuring and improving the reliability, interpretability, and trustworthiness of AI decision-making processes.</p> </div> </div> </div> </div> </div> </div> </div> </div> <hr style="margin: 2rem 0;"/> <div class="tag-category-list mb-4"> <p><strong>Browse by topics:</strong></p> <ul class="p-0 m-0" style="display: flex; flex-wrap: wrap; list-style: none;"> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/moe" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> moe </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/gpt-oss" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> gpt-oss </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/transformers" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> transformers </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/faithfulness" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> faithfulness </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/code" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> code </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/causal-analysis" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> causal-analysis </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/chain-of-thought" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> chain-of-thought </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/category/external-services" class="badge badge-info"> <i class="fas fa-tag fa-sm"></i> external-services </a> </li> </ul> </div> <div class="container featured-posts mb-4"> <h4 style="margin-bottom: 1rem;">üìå Featured Posts</h4> <div class="row row-cols-3"> <div class="col mb-4"> <a href="/blog/2025/compute/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures</h5> <p class="card-text">A comprehensive guide to counting FLOPs in LLM training, measuring Model FLOPs Utilization (MFU), and extending these concepts to Mixture-of-Experts architectures with a deep dive into OpenAI's GPT-OSS models.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 14 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> September 28, 2025 </p> </div> </div> </a> </div> <div class="col mb-4"> <a href="/blog/2025/fipo-outstanding-paper-award/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">FIPO: Fallacy-Informed Preference Optimization - Steering LLMs Toward Logically Sound Arguments</h5> <p class="card-text">üèÜ Outstanding Paper Award Winner at NAACL 2025! Introducing FIPO, a novel framework that reduces logical fallacy errors in LLM-generated arguments by up to 17.5% through fallacy-informed preference optimization.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 10 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> January 15, 2025 </p> </div> </div> </a> </div> <div class="col mb-4"> <a href="/blog/2024/reasoning-matter/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">Making Reasoning Matter: Measuring Faithfulness in Chain-of-Thought Reasoning</h5> <p class="card-text">A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 3 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> February 23, 2024 </p> </div> </div> </a> </div> </div> </div> <hr/> <h4 style="margin-bottom: 1rem;">üìù Recent Posts</h4> <ul class="post-list"> </ul> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Feedback LLM to improve their performance]]></summary></entry></feed>