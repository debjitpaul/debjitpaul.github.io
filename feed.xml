<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://debjitpaul.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debjitpaul.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-02T00:20:51+00:00</updated><id>https://debjitpaul.github.io/feed.xml</id><title type="html">blank</title><subtitle>Updating it, Work in Progress, My personal website</subtitle><entry><title type="html">What Children Know That AI Doesn‚Äôt: Learning Through Experience, Not Text</title><link href="https://debjitpaul.github.io/blog/2025/world-model/" rel="alternate" type="text/html" title="What Children Know That AI Doesn‚Äôt: Learning Through Experience, Not Text"/><published>2025-10-01T00:00:00+00:00</published><updated>2025-10-01T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2025/world-model</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2025/world-model/"><![CDATA[<p>A toddler drops a toy. It falls. They drop it again. By the third time, they predict the fall before releasing. Drop a helium balloon, and watch their delighted confusion‚Äîit floats upward instead. This simple interaction contains a profound lesson: the child is building a causal model of gravity through direct experience, learning from genuine surprise when reality violates predictions.</p> <p>Now consider GPT-4. It has ‚Äúread‚Äù millions of descriptions of falling objects and can generate fluent text: ‚Äúwhen you drop something, it falls due to gravity.‚Äù But it has never experienced releasing an object, never felt acceleration, never been surprised by a balloon floating. It predicts what would be said about dropping objects, not what would actually happen.</p> <p>This distinction‚Äîbetween linguistic pattern matching and genuine causal understanding grounded in experience‚Äîreveals why current AI, despite impressive capabilities, lacks true intelligence. Jean Piaget‚Äôs theory of cognitive development offers a framework for understanding what‚Äôs missing.</p> <p>Yet something remarkable is happening. After decades dominated by learning from human-generated text and data, AI is beginning to rediscover what children have always known: true understanding comes from experience. As Silver and Sutton (2025) argue, we‚Äôre entering an ‚ÄúEra of Experience‚Äù where AI will finally learn the way children do‚Äîthrough interaction, surprise, and genuine engagement with reality.<sup id="fnref:15"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <h2 id="piagets-core-insight-knowledge-is-constructed-through-experience">Piaget‚Äôs Core Insight: Knowledge is Constructed Through Experience</h2> <p>Swiss psychologist Jean Piaget (1896-1980) made a revolutionary claim: children don‚Äôt passively absorb knowledge. They actively construct understanding through interaction with the world.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">2</a></sup> Knowledge isn‚Äôt transmitted‚Äîit‚Äôs built by the learner through experience, experimentation, and adaptation.</p> <p>This has immediate implications for AI. Current machine learning follows a transmission model: massive datasets encode patterns, models absorb them through optimization. But a child doesn‚Äôt need millions of examples to understand gravity‚Äîthey construct the concept through dozens of hands-on experiments, generalizing from minimal data by building causal models, not memorizing correlations.</p> <h3 id="schemas-testable-mental-models">Schemas: Testable Mental Models</h3> <p>Piaget introduced schemas‚Äîorganized patterns representing our understanding of the world.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> A schema isn‚Äôt stored knowledge; it‚Äôs an active framework for interpreting experiences, making predictions, and recognizing when reality violates expectations.</p> <p>A child‚Äôs schema for ‚Äúdog‚Äù includes: four legs, fur, barks. When they encounter a cat, their dog schema fails to predict its behavior (meows, acts aloof), triggering learning. The schema is testable against reality.</p> <p>Modern AI has representations‚Äîword embeddings, neural activations‚Äîbut these lack causal structure, predictive power, and testability. They capture statistical patterns, not causal understanding.</p> <h3 id="two-modes-of-learning-assimilation-and-accommodation">Two Modes of Learning: Assimilation and Accommodation</h3> <p><strong>Assimilation</strong>: Fitting new experiences into existing schemas. Seeing a new dog breed: ‚Äúthat‚Äôs also a dog.‚Äù</p> <p><strong>Accommodation</strong>: Restructuring schemas when experiences don‚Äôt fit. Meeting a cat: the child must split their schema‚Äîdogs AND cats, both with four legs but different behaviors.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p> <p>Most neural networks only assimilate‚Äîfitting new data within existing capacity. When truly novel information arrives, they fail catastrophically, overfit, or suffer catastrophic forgetting. Few AI systems have mechanisms for graceful accommodation‚Äîrecognizing when their world model is fundamentally wrong and restructuring it while preserving valuable knowledge.</p> <h2 id="the-four-stages-from-action-to-abstraction">The Four Stages: From Action to Abstraction</h2> <p>Piaget proposed that cognitive development progresses through four distinct, universal stages, each characterized by fundamentally different ways of thinking and interacting with the world.<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">5</a></sup></p> <h3 id="stage-1-sensorimotor-0-2-years">Stage 1: Sensorimotor (0-2 years)</h3> <p>Children learn through direct sensory experience and motor action.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> Key achievement: object permanence‚Äîthings exist even when unseen. This requires repeated experimentation, sensorimotor feedback, and surprise learning. Infants master causality and object permanence by manipulating their surroundings and learning from the consequences.<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">7</a></sup></p> <p>For AI: This is the embodiment problem. Systems trained on text or static images lack sensorimotor grounding, action-consequence loops, and physical intuition. They reason about physics they‚Äôve never experienced.</p> <h3 id="stage-2-preoperational-2-7-years">Stage 2: Preoperational (2-7 years)</h3> <p>Symbolic thinking emerges‚Äîusing words to represent objects not present.<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> But limitations remain: difficulty reversing operations mentally, focusing on one aspect while ignoring others (centration), inability to take others‚Äô perspectives (egocentrism).</p> <p>For AI: This is where LLMs operate. They manipulate symbols fluently but show systematic limitations‚Äîsensitivity to surface features, difficulty with logical reversibility, brittle performance when prompts change slightly.</p> <h3 id="stage-3-concrete-operational-7-11-years">Stage 3: Concrete Operational (7-11 years)</h3> <p>Logical reasoning develops‚Äîbut only with concrete, tangible examples.<sup id="fnref:8"><a href="#fn:8" class="footnote" rel="footnote" role="doc-noteref">9</a></sup> Children can classify hierarchically, understand conservation (quantity remains constant despite changes in appearance), reverse operations mentally. But they struggle with abstract concepts and hypothetical reasoning.</p> <p>For AI: Current systems rarely reach this stage. LLMs sometimes simulate logical inference through pattern matching but lack stable, consistent logical frameworks.</p> <h3 id="stage-4-formal-operational-12-years">Stage 4: Formal Operational (12+ years)</h3> <p>Abstract reasoning, hypothesis testing, scientific thinking.<sup id="fnref:9"><a href="#fn:9" class="footnote" rel="footnote" role="doc-noteref">10</a></sup> Understanding abstract concepts, reasoning about hypotheticals, thinking about thinking (metacognition).</p> <p>For AI: The unreached goal. Even frontier systems show brittle, domain-specific reasoning that doesn‚Äôt transfer robustly.</p> <h2 id="three-critical-gaps">Three Critical Gaps</h2> <h3 id="gap-1-experience-vs-description">Gap 1: Experience vs. Description</h3> <p>Piaget: Knowledge comes from acting on the world and observing consequences. A child learns ‚Äúhot‚Äù by experiencing heat, connecting sensation to cause. Cognitive development emerges from the interaction between the child‚Äôs actions and the environment‚Äôs responses.<sup id="fnref:10"><a href="#fn:10" class="footnote" rel="footnote" role="doc-noteref">11</a></sup></p> <p>Current AI: Learns from descriptions of others‚Äô experiences. An LLM has read ‚Äúfire is hot‚Äù millions of times but never experienced heat. It predicts what people say about heat, not what heat actually feels like or causes.</p> <p>As Alan Turing observed, true machine intelligence requires ‚Äúmachines that can learn from experience, where experience means things that actually happened.‚Äù<sup id="fnref:11"><a href="#fn:11" class="footnote" rel="footnote" role="doc-noteref">12</a></sup> Current LLMs lack this experiential grounding‚Äîthey absorb descriptions, not reality.</p> <h3 id="gap-2-predicting-reality-vs-predicting-words">Gap 2: Predicting Reality vs. Predicting Words</h3> <p><strong>World Models predict what will happen in reality</strong>. A child‚Äôs world model: ‚ÄúIf I release this, it will fall.‚Äù This prediction is tested against reality. When surprised (balloon floats), the model updates.</p> <p><strong>LLMs predict what would be said about reality</strong>. They model P(next words | previous words), not P(outcomes | actions). When their linguistic prediction is wrong, they receive no feedback from reality‚Äîonly more text descriptions, which themselves might be incorrect.</p> <p>Consider:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Child: "If I let go, this will fall"
‚Üí Action: Releases object
‚Üí Reality: Object falls
‚Üí Update: Prediction confirmed

LLM: "Objects fall when dropped"
‚Üí Action: None
‚Üí Reality: No interaction
‚Üí Update: Only from more text
</code></pre></div></div> <p>The LLM‚Äôs knowledge is untested and untestable against physical reality.</p> <h3 id="gap-3-learning-from-surprise">Gap 3: Learning from Surprise</h3> <p><strong>Piaget‚Äôs deepest insight</strong>: Children learn most when reality violates predictions. The balloon floats (violating gravity schema), ice melts (violating object permanence), the friendly dog bites (violating behavioral schema). Surprise triggers accommodation‚Äîthe unexpected outcome signals ‚Äúmy model is wrong.‚Äù<sup id="fnref:12"><a href="#fn:12" class="footnote" rel="footnote" role="doc-noteref">13</a></sup></p> <p><strong>Current AI cannot be genuinely surprised by reality</strong>. LLMs never interact with the physical world. When deployed, if predictions are wrong, they receive no reality-based feedback‚Äîonly human corrections encoded as more text. When an LLM confidently predicts something physically impossible, reality never corrects it.</p> <p>This is perhaps the most critical limitation: without the ability to be surprised by reality, AI cannot truly learn from experience.</p> <h2 id="what-this-means-for-building-better-ai">What This Means for Building Better AI</h2> <p>Piaget‚Äôs framework suggests several principles:</p> <p><strong>Start with embodiment</strong>: Intelligence should be grounded in sensorimotor experience before abstract reasoning. Train agents in physical environments with action-consequence feedback.</p> <p><strong>Enable genuine surprise</strong>: Systems need to form explicit predictions about reality, test them through action, recognize failures, and use surprise as a learning signal.</p> <p><strong>Support accommodation</strong>: Systems need structured representations that can be updated incrementally or restructured fundamentally, avoiding catastrophic forgetting.</p> <p><strong>Progress through stages</strong>: Follow Piaget‚Äôs progression‚Äîsensorimotor grounding, then symbolic representation, concrete logic, and finally abstract reasoning. Skipping stages creates surface-level capabilities with fundamental limitations.</p> <h2 id="the-coming-paradigm-shift-from-human-data-to-experience">The Coming Paradigm Shift: From Human Data to Experience</h2> <p>The history of AI reveals a striking pattern. As Silver and Sutton document, we‚Äôve moved through distinct eras: early symbolic AI, the ‚ÄúEra of Simulation‚Äù (where reinforcement learning agents mastered games like chess, Go, and StarCraft through self-play), and most recently, the ‚ÄúEra of Human Data‚Äù (where large language models trained on massive text corpora achieved unprecedented breadth of capabilities).<sup id="fnref:15:1"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p><img src="/assets/img/ai_eras_timeline.png" alt="Evolution of AI paradigms from simulation to human data to experience"/></p> <p><em>Figure: A chronology of dominant AI paradigms showing the transition from the Era of Simulation (narrow domains, clear rewards) through the Era of Human Data (broad capabilities from text) to the emerging Era of Experience (experiential learning at scale). Adapted from Silver &amp; Sutton (2025).<sup id="fnref:15:2"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></em></p> <p>Each transition brought gains and losses. The Era of Simulation produced agents like AlphaZero that discovered fundamentally new strategies‚Äîknowledge that went beyond human understanding. But these systems were confined to narrow domains with clear reward signals. The Era of Human Data achieved remarkable generality but lost the ability to self-discover knowledge. As Silver and Sutton observe: ‚Äúsomething was lost in this transition: an agent‚Äôs ability to self-discover its own knowledge.‚Äù<sup id="fnref:15:3"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>Current AI faces a fundamental ceiling: you cannot learn what humans don‚Äôt know by reading what humans have written. In domains like mathematics, coding, and science, we‚Äôre rapidly exhausting high-quality human data. More critically, breakthrough insights‚Äînew theorems, technologies, scientific discoveries‚Äîlie beyond current human understanding and cannot be captured in existing training data.<sup id="fnref:15:4"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>The solution? A return to experiential learning, but now applied to open-ended, real-world problems rather than isolated games.</p> <h3 id="the-four-pillars-of-experiential-ai">The Four Pillars of Experiential AI</h3> <p>Silver and Sutton identify four critical dimensions that distinguish experiential learning from the current human-data paradigm:<sup id="fnref:15:5"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p><strong>1. Streams of Experience</strong>: Rather than short conversational episodes, agents will inhabit continuous streams spanning months or years‚Äîlike human development. A health agent could monitor patterns over time, adapting recommendations based on long-term trends. An educational agent could track language learning progress across years, continuously refining its teaching methods.</p> <p><strong>2. Grounded Actions and Observations</strong>: Instead of only text-based dialogue, agents will interact through rich sensorimotor channels‚Äîcontrolling digital tools, operating laboratory equipment, monitoring real-world sensors. This mirrors Piaget‚Äôs emphasis on sensorimotor grounding in cognitive development.</p> <p><strong>3. Reality-Based Rewards</strong>: Moving beyond human preferences and judgments, agents will receive feedback directly from the environment. A scientific agent pursuing climate goals would use actual CO‚ÇÇ measurements, not human opinions about experimental designs. This grounds learning in reality rather than human prejudgment‚Äîallowing agents to discover approaches that humans might underappreciate or fail to recognize.</p> <p><strong>4. Experience-Grounded Reasoning</strong>: Rather than imitating human chains of thought, agents will develop their own methods of reasoning, grounded in and tested against reality. AlphaProof demonstrated this by learning to prove mathematical theorems in ways fundamentally different from human mathematicians‚Äîgenerating 100 million formal proofs after exposure to just 100,000 human-created ones, ultimately achieving medal performance at the International Mathematical Olympiad.<sup id="fnref:15:6"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup><sup id="fnref:20"><a href="#fn:20" class="footnote" rel="footnote" role="doc-noteref">14</a></sup></p> <h3 id="early-signs-when-experience-surpasses-imitation">Early Signs: When Experience Surpasses Imitation</h3> <p>We‚Äôre already seeing glimpses of this transition. AlphaProof didn‚Äôt just learn mathematics from human examples‚Äîit learned by doing mathematics, generating its own proofs through interaction with a formal proving system. Similarly, DeepSeek‚Äôs recent work shows how ‚Äúrather than explicitly teaching the model on how to solve a problem, we simply provide it with the right incentives, and it autonomously develops advanced problem-solving strategies.‚Äù<sup id="fnref:15:7"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>This mirrors exactly how children learn. A toddler isn‚Äôt taught a theory of gravity‚Äîthey construct it through dozens of dropping experiments. Similarly, these AI systems construct mathematical understanding through thousands of proof attempts, learning from success and failure.</p> <h3 id="the-piaget-rl-connection-why-this-matters">The Piaget-RL Connection: Why This Matters</h3> <p>The convergence between Piaget‚Äôs developmental psychology and modern reinforcement learning is profound:</p> <table> <thead> <tr> <th>Piaget‚Äôs Concepts</th> <th>RL/Experiential AI Equivalent</th> </tr> </thead> <tbody> <tr> <td>Schemas</td> <td>World models that predict consequences of actions</td> </tr> <tr> <td>Assimilation/Accommodation</td> <td>Value function updates vs. model restructuring</td> </tr> <tr> <td>Sensorimotor stage</td> <td>Grounded interaction with environments through actions and observations</td> </tr> <tr> <td>Object permanence</td> <td>Learning temporal persistence and causality through experience</td> </tr> <tr> <td>Learning from surprise</td> <td>Prediction error as learning signal</td> </tr> <tr> <td>Action ‚Üí Consequence</td> <td>Agent actions receive environment feedback</td> </tr> </tbody> </table> <p>Both frameworks recognize that intelligence is not knowledge transfer but knowledge construction through experience. The key insight: cognitive structures emerge from the repeated cycle of prediction, action, observation, and adaptation.</p> <h2 id="current-work-and-future-directions">Current Work and Future Directions</h2> <p>Recent research begins incorporating these principles. Del Ser et al. (2025) propose a framework integrating six key areas essential for enabling AI systems to develop structured, adaptive representations inspired by Piaget‚Äôs cognitive development theory:<sup id="fnref:13"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">15</a></sup></p> <ol> <li><strong>Physics-informed learning</strong>: Embedding physical laws and constraints</li> <li><strong>Neurosymbolic AI</strong>: Combining neural pattern recognition with symbolic reasoning</li> <li><strong>Causal inference</strong>: Modeling interventions and counterfactuals</li> <li><strong>Open-world machine learning</strong>: Handling novelty and continual learning</li> <li><strong>Human-in-the-loop</strong>: Guided development with human feedback</li> <li><strong>Responsible AI</strong>: Ethical constraints and safety considerations</li> </ol> <p>These directions address Piagetian concerns: grounding in physical reality, structured causal representations, accommodation mechanisms, and experiential learning loops.</p> <p>The convergence is striking. Both Piaget‚Äôs developmental psychology and modern reinforcement learning research point to the same insight: intelligence emerges from the interaction between the agent‚Äôs actions and the environment‚Äôs responses. Classic RL concepts‚Äîvalue functions, exploration strategies, world models, temporal abstraction‚Äîprovide computational mechanisms for what Piaget observed in children: building internal models, testing predictions, learning from surprise, and reasoning over increasingly long timescales.<sup id="fnref:15:8"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>Recent systems like Anthropic‚Äôs Claude with computer use, Google‚Äôs Project Mariner, and OpenAI‚Äôs Operator represent early steps toward agents that interact with digital environments through the same interfaces humans use‚Äîclicking, typing, navigating‚Äîrather than just conversing.<sup id="fnref:15:9"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> These are the first stirrings of genuine sensorimotor grounding in AI.</p> <h2 id="conclusion-the-hard-truth-about-intelligence">Conclusion: The Hard Truth About Intelligence</h2> <p>Current AI development has optimized for benchmark performance and scaling‚Äîbigger models, more data. But both Piaget‚Äôs framework and emerging research reveal this approach‚Äôs fundamental limitations:</p> <p><strong>Quantity of text data ‚â† Quality of experiential learning</strong></p> <p>A child constructs rich causal understanding from limited but highly informative experiences‚Äîeach grounded in reality, tied to action and consequence, testable through prediction and surprise. Training on trillions of text tokens provides vast linguistic patterns but no sensorimotor grounding, no action-consequence loops, no reality-based surprise.</p> <p>The challenge isn‚Äôt primarily technical‚Äîit‚Äôs philosophical. We want AI to be precocious, to skip development stages and emerge fully formed. But intelligence doesn‚Äôt work that way. Piaget demonstrated that genuine understanding is built slowly through countless interactions, mistakes, accommodation, and gradual construction of ever-more-sophisticated schemas.<sup id="fnref:14"><a href="#fn:14" class="footnote" rel="footnote" role="doc-noteref">16</a></sup></p> <p>Yet there‚Äôs reason for optimism. The ‚ÄúEra of Experience‚Äù isn‚Äôt speculative‚Äîit‚Äôs beginning now. Systems are starting to learn through interaction with proving systems, code execution environments, and digital interfaces. The scale of experiential data is already eclipsing human-generated data in specific domains, and this trend will accelerate.<sup id="fnref:15:10"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>The path forward is clear: If we want AI that truly understands reality rather than mimicking descriptions of it, we must let it learn through genuine experience‚Äîhands-on interaction, prediction, surprise, and patient construction of causal models grounded in reality itself.</p> <p>As Alan Turing recognized and Piaget formalized, as Silver and Sutton now champion: machines that truly think must learn from experience‚Äîfrom things that actually happen, not just from reading about what happened.<sup id="fnref:11:1"><a href="#fn:11" class="footnote" rel="footnote" role="doc-noteref">12</a></sup><sup id="fnref:15:11"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>The exciting truth is that we‚Äôre not just theorizing about this future. We‚Äôre building it, one interaction at a time, watching AI systems rediscover what every toddler dropping a toy already knows: the world is the best teacher, and experience is the only path to genuine understanding.</p> <hr/> <h2 id="references">References</h2> <h3 id="additional-reading">Additional Reading</h3> <ul> <li>Boden, M. A. (1979). <em>Piaget</em>. Fontana/Collins.</li> <li>Brainerd, C. J. (1978). <em>Piaget‚Äôs Theory of Intelligence</em>. Prentice-Hall.</li> <li>Ginsburg, H., &amp; Opper, S. (1988). <em>Piaget‚Äôs Theory of Intellectual Development</em> (3rd ed.). Prentice-Hall.</li> <li>Lake, B. M., Ullman, T. D., Tenenbaum, J. B., &amp; Gershman, S. J. (2017). ‚ÄúBuilding machines that learn and think like people.‚Äù <em>Behavioral and Brain Sciences</em>, 40, e253.</li> <li>Marcus, G. (2018). ‚ÄúDeep Learning: A Critical Appraisal.‚Äù <em>arXiv preprint arXiv:1801.00631</em>.</li> <li>Wikipedia contributors. (2025). ‚ÄúPiaget‚Äôs theory of cognitive development.‚Äù <em>Wikipedia, The Free Encyclopedia</em>. Retrieved from https://en.wikipedia.org/wiki/Piaget‚Äôs_theory_of_cognitive_development</li> </ul> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:15"> <p>Silver, D., &amp; Sutton, R. S. (2025). ‚ÄúWelcome to the Era of Experience.‚Äù In <em>Designing an Intelligence</em>. MIT Press. Retrieved from https://storage.googleapis.com/deepmind-media/Era-of-Experience%20/The%20Era%20of%20Experience%20Paper.pdf¬†<a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:15:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a>¬†<a href="#fnref:15:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a>¬†<a href="#fnref:15:3" class="reversefootnote" role="doc-backlink">&#8617;<sup>4</sup></a>¬†<a href="#fnref:15:4" class="reversefootnote" role="doc-backlink">&#8617;<sup>5</sup></a>¬†<a href="#fnref:15:5" class="reversefootnote" role="doc-backlink">&#8617;<sup>6</sup></a>¬†<a href="#fnref:15:6" class="reversefootnote" role="doc-backlink">&#8617;<sup>7</sup></a>¬†<a href="#fnref:15:7" class="reversefootnote" role="doc-backlink">&#8617;<sup>8</sup></a>¬†<a href="#fnref:15:8" class="reversefootnote" role="doc-backlink">&#8617;<sup>9</sup></a>¬†<a href="#fnref:15:9" class="reversefootnote" role="doc-backlink">&#8617;<sup>10</sup></a>¬†<a href="#fnref:15:10" class="reversefootnote" role="doc-backlink">&#8617;<sup>11</sup></a>¬†<a href="#fnref:15:11" class="reversefootnote" role="doc-backlink">&#8617;<sup>12</sup></a></p> </li> <li id="fn:1"> <p>Piaget, J. (1954). <em>The Construction of Reality in the Child</em>. Basic Books. [Original work published 1937]¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>Piaget, J. (1952). <em>The Origins of Intelligence in Children</em>. International Universities Press.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>Piaget, J. (1970). <em>Genetic Epistemology</em>. Columbia University Press.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>Piaget, J. (1936). <em>The Origins of Intelligence in the Child</em>. Routledge &amp; Kegan Paul. (See also: Simply Psychology. (2025). ‚ÄúPiaget‚Äôs Theory and Stages of Cognitive Development.‚Äù Retrieved from https://www.simplypsychology.org/piaget.html)¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5"> <p>Piaget, J., &amp; Inhelder, B. (1969). <em>The Psychology of the Child</em>. Basic Books.¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6"> <p>National Center for Biotechnology Information. (2023). ‚ÄúCognitive Development.‚Äù <em>StatPearls</em>. Retrieved from https://www.ncbi.nlm.nih.gov/books/NBK537095/¬†<a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7"> <p>Piaget, J. (1959). <em>The Language and Thought of the Child</em> (3rd ed.). Routledge &amp; Kegan Paul.¬†<a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:8"> <p>Piaget, J., &amp; Inhelder, B. (1956). <em>The Child‚Äôs Conception of Space</em>. Routledge &amp; Kegan Paul.¬†<a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:9"> <p>Inhelder, B., &amp; Piaget, J. (1958). <em>The Growth of Logical Thinking from Childhood to Adolescence</em>. Basic Books.¬†<a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:10"> <p>Piaget, J. (1970). <em>Piaget‚Äôs Theory</em>. In P. H. Mussen (Ed.), <em>Carmichael‚Äôs Manual of Child Psychology</em> (Vol. 1, pp. 703-732). Wiley.¬†<a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:11"> <p>Turing, A. M. (1950). ‚ÄúComputing Machinery and Intelligence.‚Äù <em>Mind</em>, 59(236), 433-460.¬†<a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a>¬†<a href="#fnref:11:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p> </li> <li id="fn:12"> <p>Piaget, J. (1985). <em>The Equilibration of Cognitive Structures: The Central Problem of Intellectual Development</em>. University of Chicago Press.¬†<a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:20"> <p>Masoom, H., et al. (2024). ‚ÄúAI achieves silver-medal standard solving International Mathematical Olympiad problems.‚Äù DeepMind. Retrieved from https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/¬†<a href="#fnref:20" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:13"> <p>Del Ser, J., Lobo, J. L., M√ºller, H., &amp; Holzinger, A. (2025). ‚ÄúWorld Models in Artificial Intelligence: Sensing, Learning, and Reasoning Like a Child.‚Äù <em>arXiv preprint arXiv:2503.15168</em>. Retrieved from https://arxiv.org/abs/2503.15168¬†<a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:14"> <p>Flavell, J. H. (1963). <em>The Developmental Psychology of Jean Piaget</em>. Van Nostrand.¬†<a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="research"/><category term="ai-theory"/><category term="cognitive-development"/><category term="piaget"/><category term="ai-reasoning"/><category term="world-models"/><category term="experiential-learning"/><category term="reinforcement-learning"/><summary type="html"><![CDATA[Jean Piaget revealed how children construct understanding through action, surprise, and adaptation. His insights expose fundamental gaps in current AI‚Äîsystems that predict words, not reality. But a new paradigm is emerging.]]></summary></entry><entry><title type="html">Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures</title><link href="https://debjitpaul.github.io/blog/2025/compute/" rel="alternate" type="text/html" title="Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures"/><published>2025-09-28T00:00:00+00:00</published><updated>2025-09-28T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2025/compute</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2025/compute/"><![CDATA[<p>As Large Language Models (LLMs) continue to scale exponentially, understanding their computational requirements becomes crucial for researchers and practitioners alike. Whether you‚Äôre planning a training run, optimizing infrastructure costs, or comparing different architectures, <strong>FLOPs</strong> (Floating Point Operations) and <strong>MFU</strong> (Model FLOPs Utilization) are essential metrics that provide hardware-independent ways to measure and optimize computational efficiency.</p> <p>This post builds upon two excellent foundational resources: <strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs comprehensive guide to Transformer FLOPs</a></strong> and <strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs practical guide to FLOPs in LLM training</a></strong>. While these posts cover dense Transformer architectures, this post extends their insights to modern Mixture-of-Experts (MoE) architectures, with a particular focus on OpenAI‚Äôs recently released GPT-OSS models.</p> <p>In this comprehensive analysis, we‚Äôll explore FLOP counting methodologies, dive deep into MFU calculations, and examine how these concepts apply to the latest generation of sparse models that are reshaping the efficiency landscape of large language models.</p> <blockquote> <p><strong>üõ†Ô∏è Practical Tools</strong>: Want to calculate FLOPs for your own models? Check out the <strong><a href="https://github.com/debjitpaul/flop_calculator">FLOP Calculator</a></strong> - an interactive tool that implements all the methodologies discussed in this post for both dense and MoE architectures.</p> </blockquote> <h2 id="-what-are-flops-and-why-do-they-matter">üî¢ What Are FLOPs and Why Do They Matter?</h2> <h3 id="defining-flops">Defining FLOPs</h3> <p>A <strong>FLOP</strong> (Floating Point Operation) represents a single computational operation like addition (<code class="language-plaintext highlighter-rouge">3.8 + 4.1</code>) or multiplication (<code class="language-plaintext highlighter-rouge">1.2 √ó 4.6</code>). When we talk about <strong>FLOPs</strong> (plural), we‚Äôre counting the total number of these atomic operations required for a specific task.</p> <p>For large-scale computations, we use:</p> <ul> <li><strong>GFLOPs</strong> = 1 billion FLOPs (10‚Åπ)</li> <li><strong>TFLOPs</strong> = 1 trillion FLOPs (10¬π¬≤)</li> <li><strong>PFLOPs</strong> = 1 quadrillion FLOPs (10¬π‚Åµ)</li> </ul> <h3 id="why-flops-matter">Why FLOPs Matter</h3> <p>FLOPs provide several crucial advantages:</p> <ol> <li><strong>Hardware Independence</strong>: Unlike wall-clock time, FLOPs offer consistent measurements across different hardware configurations</li> <li><strong>Reproducibility</strong>: Enable precise comparisons between different models and training setups</li> <li><strong>Cost Estimation</strong>: Help predict computational costs and resource requirements</li> <li><strong>Efficiency Analysis</strong>: Allow measurement of how well we utilize available hardware</li> </ol> <h2 id="-counting-flops-in-dense-transformers">üìê Counting FLOPs in Dense Transformers</h2> <h3 id="the-openai-method">The OpenAI Method</h3> <p>The seminal approach from OpenAI‚Äôs scaling laws paper [1] provides a clean approximation:</p> <p><strong>FLOPs per token ‚âà 6N</strong></p> <p>Where <code class="language-plaintext highlighter-rouge">N</code> is the number of non-embedding parameters. This factor of 6 accounts for:</p> <ul> <li><strong>2√ó</strong> for the forward pass (multiply-accumulate operations)</li> <li><strong>2√ó</strong> for the backward pass (gradients with respect to inputs)</li> <li><strong>2√ó</strong> for the backward pass (gradients with respect to parameters)</li> </ul> <p>This approximation, introduced by Kaplan et al. in their foundational work on neural language model scaling laws, provides a hardware-independent way to estimate computational requirements [1]. The elegance of this formula lies in its simplicity while capturing the essential computational structure of Transformer training.</p> <p>Let‚Äôs break down the forward pass components:</p> <table> <thead> <tr> <th>Operation</th> <th>Parameters</th> <th>FLOPs per Token</th> </tr> </thead> <tbody> <tr> <td><strong>Attention QKV</strong></td> <td><code class="language-plaintext highlighter-rouge">3 √ó d √ó d_model</code></td> <td><code class="language-plaintext highlighter-rouge">6 √ó L √ó d_model √ó d</code></td> </tr> <tr> <td><strong>Attention Scores</strong></td> <td>-</td> <td><code class="language-plaintext highlighter-rouge">4 √ó L √ó seq_len √ó d</code></td> </tr> <tr> <td><strong>Attention Project</strong></td> <td><code class="language-plaintext highlighter-rouge">d √ó d_model</code></td> <td><code class="language-plaintext highlighter-rouge">2 √ó L √ó d √ó d_model</code></td> </tr> <tr> <td><strong>Feedforward</strong></td> <td><code class="language-plaintext highlighter-rouge">8 √ó d_model¬≤</code></td> <td><code class="language-plaintext highlighter-rouge">16 √ó L √ó d_model¬≤</code></td> </tr> <tr> <td><strong>Total (approx)</strong></td> <td>-</td> <td><code class="language-plaintext highlighter-rouge">‚âà 2 √ó N</code></td> </tr> </tbody> </table> <p>Where:</p> <ul> <li><code class="language-plaintext highlighter-rouge">L</code> = number of layers</li> <li><code class="language-plaintext highlighter-rouge">seq_len</code> = sequence length</li> <li><code class="language-plaintext highlighter-rouge">d_model</code> = hidden dimension</li> <li><code class="language-plaintext highlighter-rouge">d</code> = attention dimension (<code class="language-plaintext highlighter-rouge">d_model</code> for most implementations)</li> </ul> <h3 id="the-deepmind-method">The DeepMind Method</h3> <p>DeepMind‚Äôs Chinchilla paper provides a more detailed accounting that includes embeddings, logits, and attention mechanics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">deepmind_flops_per_sequence</span><span class="p">(</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">ff_ratio</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">DeepMind method for forward pass FLOPs counting</span><span class="sh">"""</span>
    <span class="n">d_attn</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
    <span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">ff_ratio</span>

    <span class="c1"># Components
</span>    <span class="n">embeddings</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">vocab_size</span> <span class="o">*</span> <span class="n">d_model</span>
    <span class="n">attn_qkv</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_softmax</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">n_heads</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">attn_reduce</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span>
    <span class="n">attn_project</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_attn</span> <span class="o">*</span> <span class="n">n_heads</span><span class="p">)</span> <span class="o">*</span> <span class="n">d_model</span>

    <span class="n">total_attn</span> <span class="o">=</span> <span class="n">attn_qkv</span> <span class="o">+</span> <span class="n">attn_logits</span> <span class="o">+</span> <span class="n">attn_softmax</span> <span class="o">+</span> <span class="n">attn_reduce</span> <span class="o">+</span> <span class="n">attn_project</span>
    <span class="n">feedforward</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="p">(</span><span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">vocab_size</span>

    <span class="k">return</span> <span class="n">embeddings</span> <span class="o">+</span> <span class="n">n_layers</span> <span class="o">*</span> <span class="p">(</span><span class="n">total_attn</span> <span class="o">+</span> <span class="n">feedforward</span><span class="p">)</span> <span class="o">+</span> <span class="n">logits</span>
</code></pre></div></div> <h3 id="practical-example-gpt-3-scale-model">Practical Example: GPT-3 Scale Model</h3> <p>Let‚Äôs calculate FLOPs for a GPT-3 scale model:</p> <ul> <li><strong>Parameters</strong>: 175B non-embedding</li> <li><strong>Sequence length</strong>: 2048</li> <li><strong>Batch size</strong>: 32</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Using OpenAI approximation
</span><span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="mf">175e9</span>  <span class="c1"># 1.05 √ó 10^12 FLOPs
</span><span class="n">tokens_per_step</span> <span class="o">=</span> <span class="mi">32</span> <span class="o">*</span> <span class="mi">2048</span>  <span class="c1"># 65,536 tokens
</span><span class="n">flops_per_step</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">tokens_per_step</span>  <span class="c1"># 6.87 √ó 10^16 FLOPs
</span>
<span class="c1"># Forward + Backward
</span><span class="n">total_flops_per_step</span> <span class="o">=</span> <span class="n">flops_per_step</span> <span class="o">*</span> <span class="mi">2</span>  <span class="c1"># 1.37 √ó 10^17 FLOPs ‚âà 137 PFLOPs
</span></code></pre></div></div> <h2 id="-model-flops-utilization-mfu-the-gold-standard">‚ö° Model FLOPs Utilization (MFU): The Gold Standard</h2> <h3 id="understanding-mfu">Understanding MFU</h3> <p><strong>Model FLOPs Utilization (MFU)</strong> measures how efficiently we execute the theoretically necessary FLOPs for training, introduced in Google‚Äôs PaLM paper:</p> <p><strong>MFU = (Model FLOPs √ó Throughput) / Peak Hardware FLOPs</strong></p> <p>Where:</p> <ul> <li><strong>Model FLOPs</strong>: Theoretical FLOPs per token (using OpenAI‚Äôs 6N approximation)</li> <li><strong>Throughput</strong>: Observed tokens processed per second</li> <li><strong>Peak Hardware FLOPs</strong>: Theoretical maximum FLOP/s of your hardware</li> </ul> <h3 id="mfu-vs-hardware-flops-utilization-hfu">MFU vs. Hardware FLOPs Utilization (HFU)</h3> <table> <thead> <tr> <th>Metric</th> <th><strong>MFU</strong></th> <th><strong>HFU</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Includes</strong></td> <td>Only necessary model computations</td> <td>All computations (including overheads)</td> </tr> <tr> <td><strong>Use Case</strong></td> <td>Fair comparison across setups</td> <td>Implementation efficiency</td> </tr> <tr> <td><strong>Affected by</strong></td> <td>Model architecture, batch size</td> <td>Memory management, communication</td> </tr> </tbody> </table> <h3 id="calculating-mfu-a-practical-example">Calculating MFU: A Practical Example</h3> <p>Consider training a 7B parameter model on 8√óA100 GPUs:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model specifications
</span><span class="n">parameters</span> <span class="o">=</span> <span class="mf">7e9</span>
<span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">parameters</span>  <span class="c1"># 4.2 √ó 10^10
</span>
<span class="c1"># Hardware specifications
</span><span class="n">a100_peak_flops</span> <span class="o">=</span> <span class="mf">312e12</span>  <span class="c1"># 312 TFLOPs for bf16
</span><span class="n">total_peak_flops</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">a100_peak_flops</span>  <span class="c1"># 2.496 √ó 10^15
</span>
<span class="c1"># Measured throughput
</span><span class="n">tokens_per_second</span> <span class="o">=</span> <span class="mi">8000</span>

<span class="c1"># Calculate MFU
</span><span class="n">sustained_flops</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">tokens_per_second</span>  <span class="c1"># 3.36 √ó 10^14
</span><span class="n">mfu</span> <span class="o">=</span> <span class="n">sustained_flops</span> <span class="o">/</span> <span class="n">total_peak_flops</span>  <span class="c1"># 0.135 = 13.5%
</span></code></pre></div></div> <h3 id="typical-mfu-ranges">Typical MFU Ranges</h3> <p>Real-world MFU values vary significantly:</p> <ul> <li><strong>Small models (&lt; 1B)</strong>: 10-30%</li> <li><strong>Medium models (1B-10B)</strong>: 30-50%</li> <li><strong>Large models (10B-100B)</strong>: 45-65%</li> <li><strong>Very large models (100B+)</strong>: 50-70%</li> </ul> <p>Higher MFU in larger models is often due to better compute-to-communication ratios and improved memory bandwidth utilization.</p> <h2 id="-extending-to-mixture-of-experts-moe-architectures">üß† Extending to Mixture-of-Experts (MoE) Architectures</h2> <h3 id="moe-fundamentals">MoE Fundamentals</h3> <p>Mixture-of-Experts architectures replace dense feedforward networks with a router and multiple expert networks. Only a subset of experts (typically 1-2 out of 8-64) are activated per token, dramatically reducing computational costs while maintaining or improving model quality.</p> <h3 id="moe-flop-counting-challenges">MoE FLOP Counting Challenges</h3> <p>Traditional FLOP counting becomes more nuanced with MoE:</p> <ol> <li><strong>Routing Overhead</strong>: Additional FLOPs for expert selection</li> <li><strong>Variable Computation</strong>: Different tokens may use different experts</li> <li><strong>Load Balancing</strong>: Uneven expert utilization affects total FLOPs</li> <li><strong>Communication Costs</strong>: Expert routing across devices</li> </ol> <h3 id="moe-flop-formula">MoE FLOP Formula</h3> <p>For a MoE layer replacing a dense feedforward network:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">moe_flops_per_token</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">,</span> <span class="n">experts_per_token</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">load_balance_factor</span><span class="o">=</span><span class="mf">1.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Calculate FLOPs for MoE layer</span><span class="sh">"""</span>

    <span class="c1"># Router FLOPs (token ‚Üí expert probabilities)
</span>    <span class="n">router_flops</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">num_experts</span>

    <span class="c1"># Expert FLOPs (only active experts)
</span>    <span class="n">expert_flops</span> <span class="o">=</span> <span class="n">experts_per_token</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d_model</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">d_ff</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="c1"># Load balancing overhead
</span>    <span class="n">effective_expert_flops</span> <span class="o">=</span> <span class="n">expert_flops</span> <span class="o">*</span> <span class="n">load_balance_factor</span>

    <span class="k">return</span> <span class="n">router_flops</span> <span class="o">+</span> <span class="n">effective_expert_flops</span>

<span class="c1"># Example: 8 experts, top-2 routing
</span><span class="n">dense_ff_flops</span> <span class="o">=</span> <span class="mi">16</span> <span class="o">*</span> <span class="n">d_model</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># 4x expansion, two linear layers
</span><span class="n">moe_ff_flops</span> <span class="o">=</span> <span class="nf">moe_flops_per_token</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="o">*</span><span class="n">d_model</span><span class="p">)</span>

<span class="n">efficiency_gain</span> <span class="o">=</span> <span class="n">dense_ff_flops</span> <span class="o">/</span> <span class="n">moe_ff_flops</span>  <span class="c1"># Typically 2-4x
</span></code></pre></div></div> <h2 id="-case-study-openai-gpt-oss-models">üöÄ Case Study: OpenAI GPT-OSS Models</h2> <p>OpenAI‚Äôs GPT-OSS models provide an excellent real-world example of modern MoE architectures with several innovative features:</p> <h3 id="gpt-oss-architecture-overview">GPT-OSS Architecture Overview</h3> <p><strong>GPT-OSS-120B</strong>:</p> <ul> <li><strong>Total Parameters</strong>: 117B (5.1B active per token)</li> <li><strong>Architecture</strong>: MoE with native MXFP4 quantization</li> <li><strong>Activation</strong>: ~4.4% of total parameters per token</li> <li><strong>Memory</strong>: Fits on single H100 (80GB)</li> </ul> <p><strong>GPT-OSS-20B</strong>:</p> <ul> <li><strong>Total Parameters</strong>: 21B (3.6B active per token)</li> <li><strong>Activation</strong>: ~17% of total parameters per token</li> <li><strong>Memory</strong>: Runs in 16GB</li> </ul> <h3 id="flop-counting-for-gpt-oss">FLOP Counting for GPT-OSS</h3> <p>Let‚Äôs calculate FLOPs for GPT-OSS-120B:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">gpt_oss_flops_calculation</span><span class="p">():</span>
    <span class="c1"># GPT-OSS-120B specifications (estimated)
</span>    <span class="n">total_params</span> <span class="o">=</span> <span class="mf">117e9</span>
    <span class="n">active_params_per_token</span> <span class="o">=</span> <span class="mf">5.1e9</span>

    <span class="c1"># Traditional approach (if it were dense)
</span>    <span class="n">dense_flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">total_params</span>  <span class="c1"># 702 GFLOPs
</span>
    <span class="c1"># MoE approach (actual)
</span>    <span class="c1"># Attention layers remain dense
</span>    <span class="n">attention_params</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mf">0.3</span>  <span class="c1"># Estimated 30%
</span>    <span class="n">attention_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">attention_params</span>

    <span class="c1"># MoE feedforward layers
</span>    <span class="n">moe_params_per_token</span> <span class="o">=</span> <span class="n">active_params_per_token</span> <span class="o">-</span> <span class="n">attention_params</span>
    <span class="n">moe_flops</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">moe_params_per_token</span>

    <span class="c1"># Router overhead (small)
</span>    <span class="n">router_flops</span> <span class="o">=</span> <span class="n">total_params</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c1"># Estimated 0.1%
</span>
    <span class="n">total_flops_per_token</span> <span class="o">=</span> <span class="n">attention_flops</span> <span class="o">+</span> <span class="n">moe_flops</span> <span class="o">+</span> <span class="n">router_flops</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">dense_equivalent</span><span class="sh">'</span><span class="p">:</span> <span class="n">dense_flops_per_token</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">actual_moe</span><span class="sh">'</span><span class="p">:</span> <span class="n">total_flops_per_token</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">efficiency_gain</span><span class="sh">'</span><span class="p">:</span> <span class="n">dense_flops_per_token</span> <span class="o">/</span> <span class="n">total_flops_per_token</span>
    <span class="p">}</span>

<span class="n">results</span> <span class="o">=</span> <span class="nf">gpt_oss_flops_calculation</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dense equivalent: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">dense_equivalent</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">e</span><span class="si">}</span><span class="s"> FLOPs/token</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">MoE actual: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">actual_moe</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">e</span><span class="si">}</span><span class="s"> FLOPs/token</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Efficiency gain: </span><span class="si">{</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">efficiency_gain</span><span class="sh">'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">x</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="mxfp4-quantization-impact">MXFP4 Quantization Impact</h3> <p>GPT-OSS uses native MXFP4 quantization for MoE layers:</p> <ul> <li><strong>Memory</strong>: 4-bit storage vs 16-bit (4√ó reduction)</li> <li><strong>Compute</strong>: Specialized kernels maintain computational efficiency</li> <li><strong>Accuracy</strong>: Minimal degradation with proper scaling</li> </ul> <p>This affects FLOP counting considerations:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">quantized_moe_flops</span><span class="p">(</span><span class="n">base_flops</span><span class="p">,</span> <span class="n">quantization_overhead</span><span class="o">=</span><span class="mf">1.05</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    MXFP4 might have slight computational overhead
    but significant memory bandwidth benefits
    </span><span class="sh">"""</span>
    <span class="k">return</span> <span class="n">base_flops</span> <span class="o">*</span> <span class="n">quantization_overhead</span>
</code></pre></div></div> <h3 id="performance-analysis">Performance Analysis</h3> <p>Based on the GPT-OSS specifications, we can estimate performance characteristics:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Estimated GPT-OSS-120B on H100
</span><span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="mf">5.1e9</span>  <span class="c1"># 30.6 GFLOPs (active parameters)
</span><span class="n">h100_peak</span> <span class="o">=</span> <span class="mf">1980e12</span>  <span class="c1"># ~2 PFLOPs for int4 operations
</span><span class="n">target_throughput</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># tokens/second (estimated)
</span>
<span class="n">required_flops_rate</span> <span class="o">=</span> <span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">target_throughput</span>  <span class="c1"># 3.06 TFLOPs/s
</span><span class="n">mfu</span> <span class="o">=</span> <span class="n">required_flops_rate</span> <span class="o">/</span> <span class="n">h100_peak</span>  <span class="c1"># 0.15%
</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Estimated MFU for GPT-OSS-120B: </span><span class="si">{</span><span class="n">mfu</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><em>Note: These are rough estimates. Actual performance depends on implementation details, memory bandwidth, and other factors.</em></p> <h2 id="-practical-considerations-and-optimizations">üîß Practical Considerations and Optimizations</h2> <h3 id="memory-vs-compute-trade-offs">Memory vs. Compute Trade-offs</h3> <p>Modern LLM training involves several techniques that affect FLOP calculations:</p> <p><strong>Activation Checkpointing</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">checkpointing_flop_overhead</span><span class="p">(</span><span class="n">base_flops</span><span class="p">,</span> <span class="n">checkpoint_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Recompute activations during backward pass
    checkpoint_ratio: fraction of activations checkpointed
    </span><span class="sh">"""</span>
    <span class="n">recompute_flops</span> <span class="o">=</span> <span class="n">base_flops</span> <span class="o">*</span> <span class="n">checkpoint_ratio</span>
    <span class="k">return</span> <span class="n">base_flops</span> <span class="o">+</span> <span class="n">recompute_flops</span>  <span class="c1"># Up to 1.5√ó FLOPs
</span></code></pre></div></div> <p><strong>Gradient Accumulation</strong>:</p> <ul> <li>Doesn‚Äôt affect per-token FLOPs</li> <li>May affect MFU due to different memory access patterns</li> </ul> <h3 id="scaling-laws-and-flop-optimal-training">Scaling Laws and FLOP-Optimal Training</h3> <p>The relationship between model size, dataset size, and computational budget follows predictable scaling laws:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_optimal_scaling</span><span class="p">(</span><span class="n">compute_budget_flops</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Based on Chinchilla scaling laws
    Roughly: parameters ‚àù compute^0.5, tokens ‚àù compute^0.5
    </span><span class="sh">"""</span>
    <span class="n">optimal_params</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_budget_flops</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="n">optimal_tokens</span> <span class="o">=</span> <span class="p">(</span><span class="n">compute_budget_flops</span> <span class="o">/</span> <span class="mi">6</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>

    <span class="k">return</span> <span class="n">optimal_params</span><span class="p">,</span> <span class="n">optimal_tokens</span>

<span class="c1"># Example: 10^23 FLOPs budget
</span><span class="n">params</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="nf">compute_optimal_scaling</span><span class="p">(</span><span class="mf">1e23</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal: </span><span class="si">{</span><span class="n">params</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">B parameters, </span><span class="si">{</span><span class="n">tokens</span><span class="o">/</span><span class="mf">1e9</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s">B tokens</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="-benchmarking-and-measurement-tools">üìä Benchmarking and Measurement Tools</h2> <h3 id="measuring-mfu-in-practice">Measuring MFU in Practice</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">def</span> <span class="nf">measure_mfu</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Measure MFU during actual training</span><span class="sh">"""</span>
    <span class="n">device</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()).</span><span class="n">device</span>

    <span class="c1"># Create dummy batch
</span>    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50000</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Warm up
</span>    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">).</span><span class="n">loss</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_steps</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">).</span><span class="n">loss</span>
        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">synchronize</span><span class="p">()</span>
    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span>

    <span class="c1"># Calculate throughput
</span>    <span class="n">total_tokens</span> <span class="o">=</span> <span class="n">num_steps</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="n">throughput</span> <span class="o">=</span> <span class="n">total_tokens</span> <span class="o">/</span> <span class="p">(</span><span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">)</span>

    <span class="c1"># Calculate MFU
</span>    <span class="n">flops_per_token</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">*</span> <span class="n">model</span><span class="p">.</span><span class="nf">num_parameters</span><span class="p">()</span>
    <span class="n">peak_flops</span> <span class="o">=</span> <span class="nf">get_device_peak_flops</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">mfu</span> <span class="o">=</span> <span class="p">(</span><span class="n">flops_per_token</span> <span class="o">*</span> <span class="n">throughput</span><span class="p">)</span> <span class="o">/</span> <span class="n">peak_flops</span>

    <span class="k">return</span> <span class="n">mfu</span><span class="p">,</span> <span class="n">throughput</span>
</code></pre></div></div> <h3 id="interactive-flop-calculator">Interactive FLOP Calculator</h3> <p>For hands-on experimentation with these concepts, check out the <strong><a href="https://github.com/debjitpaul/flop_calculator">FLOP Calculator repository</a></strong>. This interactive tool implements:</p> <ul> <li>Dense Transformer FLOP calculations (OpenAI and DeepMind methods)</li> <li>MoE architecture FLOP counting with customizable parameters</li> <li>MFU measurement and comparison across different hardware</li> <li>Visualization of computational efficiency trade-offs</li> <li>Support for custom model configurations</li> </ul> <p>The calculator makes it easy to experiment with different architectural choices and understand their computational implications before committing to expensive training runs.</p> <h3 id="profiling-tools">Profiling Tools</h3> <ul> <li><strong>PyTorch Profiler</strong>: Built-in FLOP counting</li> <li><strong>DeepSpeed</strong>: MFU reporting in training logs</li> <li><strong>Weights &amp; Biases</strong>: Integration with hardware metrics</li> <li><strong>Custom counters</strong>: Framework-specific implementations</li> </ul> <h2 id="-future-directions-and-emerging-architectures">üîÆ Future Directions and Emerging Architectures</h2> <h3 id="beyond-traditional-flops">Beyond Traditional FLOPs</h3> <p>As architectures evolve, FLOP counting methodologies must adapt:</p> <p><strong>Sparse Attention Patterns</strong>:</p> <ul> <li>Linear attention: O(n) instead of O(n¬≤)</li> <li>Local attention: Reduced sequence length dependencies</li> <li>Mixture of attention heads: Different patterns per head</li> </ul> <p><strong>Conditional Computation</strong>:</p> <ul> <li>Early exit mechanisms</li> <li>Adaptive depth networks</li> <li>Token-wise routing</li> </ul> <p><strong>Hardware-Aware Metrics</strong>:</p> <ul> <li>Memory bandwidth utilization</li> <li>Integer operation efficiency</li> <li>Specialized accelerator metrics</li> </ul> <h3 id="the-road-ahead">The Road Ahead</h3> <p>The future of computational efficiency in LLMs likely involves:</p> <ol> <li><strong>Multi-modal MoE</strong>: Extending sparse computation to different modalities</li> <li><strong>Dynamic architectures</strong>: Runtime adaptation based on input complexity</li> <li><strong>Hardware co-design</strong>: Models designed for specific accelerators</li> <li><strong>Hybrid precision</strong>: Strategic use of different numerical formats</li> </ol> <h2 id="-conclusion">üèÅ Conclusion</h2> <p>Understanding FLOPs and MFU is crucial for anyone working with large-scale language models. As we‚Äôve seen:</p> <ul> <li><strong>FLOPs provide hardware-independent measurement</strong> of computational requirements</li> <li><strong>MFU enables fair comparison</strong> of training efficiency across different setups</li> <li><strong>MoE architectures complicate</strong> but don‚Äôt invalidate these fundamental concepts</li> <li><strong>Modern models like GPT-OSS</strong> showcase how innovative architectures and quantization can dramatically improve efficiency</li> </ul> <p>Whether you‚Äôre planning a training run, optimizing an existing system, or designing new architectures, these metrics provide essential insights into the computational reality of modern AI systems.</p> <p>As models continue to scale and new architectures emerge, the principles of FLOP counting and efficiency measurement will remain foundational tools for understanding and optimizing the computational landscape of artificial intelligence.</p> <hr/> <h2 id="-acknowledgments">üôè Acknowledgments</h2> <p>This blog post draws heavily from two excellent resources that provided the foundation for understanding FLOPs in LLM training. I want to give full credit to these authors for their pioneering work:</p> <p><strong>Primary Inspirations:</strong></p> <ul> <li> <p><strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs ‚ÄúTransformer FLOPs‚Äù</a></strong> - An exceptionally clear and comprehensive guide that masterfully explains FLOP counting methodologies, MFU calculations, and scaling behaviors. Much of the mathematical foundation and practical examples in this post are built upon Adam‚Äôs excellent work.</p> </li> <li> <p><strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs ‚ÄúFLOPs in LLM Training: The Ultimate Guide‚Äù</a></strong> - A practical and accessible guide that bridges the gap between theory and implementation, providing clear examples and real-world context for FLOP calculations.</p> </li> </ul> <p>Both posts were instrumental in shaping my understanding of computational efficiency in LLMs. This work extends their insights to MoE architectures and provides additional analysis of modern models like GPT-OSS, but the core concepts and methodologies owe much to these foundational resources.</p> <hr/> <h2 id="Ô∏è-tools-and-resources">üõ†Ô∏è Tools and Resources</h2> <p><strong>Interactive Calculator:</strong></p> <ul> <li><strong><a href="https://github.com/debjitpaul/flop_calculator">FLOP Calculator</a></strong>: Open-source tool for calculating FLOPs and MFU for both dense and MoE architectures</li> </ul> <h2 id="-references-and-further-reading">üìö References and Further Reading</h2> <p><strong>Foundational Papers:</strong></p> <ul> <li><strong>[1] <a href="https://arxiv.org/abs/2001.08361">OpenAI Scaling Laws</a></strong>: Kaplan, J., McCandlish, S., Henighan, T., et al. ‚ÄúScaling Laws for Neural Language Models‚Äù (2020) - Original FLOP counting methodology and the 6N approximation</li> <li><strong>[2] <a href="https://arxiv.org/abs/2203.15556">DeepMind Chinchilla</a></strong>: Hoffmann, J., Borgeaud, S., Mensch, A., et al. ‚ÄúTraining Compute-Optimal Large Language Models‚Äù (2022) - Detailed FLOP accounting and compute-optimal training</li> <li><strong>[3] <a href="https://arxiv.org/abs/2204.02311">Google PaLM</a></strong>: Chowdhery, A., Narang, S., Devlin, J., et al. ‚ÄúPaLM: Scaling Language Modeling with Pathways‚Äù (2022) - MFU methodology introduction</li> </ul> <p><strong>Key Blog Posts and Resources:</strong></p> <ul> <li><strong><a href="https://www.adamcasson.com/posts/transformer-flops">Adam Casson‚Äôs Transformer FLOPs</a></strong>: Comprehensive FLOP analysis with interactive calculator</li> <li><strong><a href="https://medium.com/@dpratishraj7991/flops-in-llm-training-the-ultimate-guide-fce22071ad48">Pratish Raj‚Äôs FLOPs Guide</a></strong>: Practical guide to FLOPs in LLM training</li> <li><strong><a href="https://github.com/openai/gpt-oss">GPT-OSS Repository</a></strong>: OpenAI‚Äôs open-source MoE implementation</li> </ul> <p><strong>Additional Technical Resources:</strong></p> <ul> <li><strong><a href="https://kipp.ly/blog/transformer-inference-arithmetic/">Transformer Inference Arithmetic</a></strong>: Kipp Bradford‚Äôs analysis of inference costs</li> <li><strong><a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr</a></strong>: Horace He‚Äôs guide to optimization fundamentals</li> </ul> <hr/> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">paul2024flops_mfu_moe</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
    <span class="na">url</span><span class="p">=</span><span class="s">{https://debjitpaul.github.io/blog/}</span>
<span class="p">}</span>
</code></pre></div></div> <hr/> <p><em>This analysis provides a comprehensive overview of computational efficiency in modern LLM training. For specific implementation details or model-specific calculations, always refer to the original papers and codebases.</em></p>]]></content><author><name></name></author><category term="research"/><category term="technical"/><category term="flops"/><category term="mfu"/><category term="llm-training"/><category term="moe"/><category term="gpt-oss"/><category term="computational-efficiency"/><category term="transformers"/><summary type="html"><![CDATA[A comprehensive guide to counting FLOPs in LLM training, measuring Model FLOPs Utilization (MFU), and extending these concepts to Mixture-of-Experts architectures with a deep dive into OpenAI's GPT-OSS models.]]></summary></entry><entry><title type="html">FIPO: Fallacy-Informed Preference Optimization - Steering LLMs Toward Logically Sound Arguments</title><link href="https://debjitpaul.github.io/blog/2025/fipo-outstanding-paper-award/" rel="alternate" type="text/html" title="FIPO: Fallacy-Informed Preference Optimization - Steering LLMs Toward Logically Sound Arguments"/><published>2025-01-15T00:00:00+00:00</published><updated>2025-01-15T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2025/fipo-outstanding-paper-award</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2025/fipo-outstanding-paper-award/"><![CDATA[<p>üèÜ <strong>Outstanding Paper Award Winner at NAACL 2025!</strong></p> <p>I‚Äôm thrilled to share that our paper ‚ÄúA Logical Fallacy-Informed Framework for Argument Generation‚Äù has received the <strong>Outstanding Paper Award</strong> at the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2025)!</p> <h2 id="-the-problem-llms-generate-fallacious-arguments">üö® The Problem: LLMs Generate Fallacious Arguments</h2> <p>Despite their remarkable capabilities, Large Language Models (LLMs) still struggle with generating logically sound arguments, often producing content that contains <strong>logical fallacies</strong> - errors in reasoning that undermine the validity of an argument. Our preliminary study revealed a startling finding: <strong>21% of arguments generated by ChatGPT contain logical fallacies</strong>.</p> <p>Consider these examples of fallacious vs. logically sound arguments:</p> <table> <thead> <tr> <th><strong>Topic</strong>: AI is bad for the job market (Support)</th> </tr> </thead> <tbody> <tr> <td>‚ùå <strong>Ad Hominem</strong>: ‚ÄúAI proponents are tech enthusiasts disconnected from real-world job market issues.‚Äù</td> </tr> <tr> <td>‚ùå <strong>Circular Reasoning</strong>: ‚ÄúAI is harmful to the employment sector because AI is bad for the job market.‚Äù</td> </tr> <tr> <td>‚ùå <strong>Appeal to Emotion</strong>: ‚ÄúAI will leave millions of families struggling and unemployed.‚Äù</td> </tr> <tr> <td>‚úÖ <strong>Logically Sound</strong>: ‚ÄúAI automates tasks, reducing employment opportunities by replacing humans in manufacturing and administrative jobs.‚Äù</td> </tr> </tbody> </table> <h2 id="-our-solution-fallacy-informed-preference-optimization-fipo">üí° Our Solution: Fallacy-Informed Preference Optimization (FIPO)</h2> <p>We introduce <strong>FIPO</strong>, a novel framework that helps steer LLMs toward generating logically sound arguments by making them explicitly aware of logical fallacies during training.</p> <h3 id="key-innovation-weighted-classification-loss">Key Innovation: Weighted Classification Loss</h3> <p>FIPO combines traditional preference optimization with a <strong>weighted cross-entropy classification loss</strong> that penalizes the model based on fallacy frequency, applying stronger penalties for misclassifying more common fallacies.</p> <p>The FIPO loss function is:</p> <p><strong>L<sub>FIPO</sub>(œÄ<sub>Œ∏</sub>) = L<sub>CPO</sub>(œÄ<sub>Œ∏</sub>) + ŒªL<sub>CLF</sub>(œÄ<sub>Œ∏</sub>)</strong></p> <p>Where the classification loss L<sub>CLF</sub> is defined as:</p> <table> <tbody> <tr> <td>**L<sub>CLF</sub>(œÄ<sub>Œ∏</sub>) = -E<sub>(t,s,y<sub>w</sub>,y<sub>l</sub>,k)‚àºD‚Äô</sub>[w<sub>0</sub> log P<sup>0</sup><sub>h<sub>Œ∏</sub></sub>(y<sub>w</sub></td> <td>t,s) + w<sub>k</sub> log P<sup>k</sup><sub>h<sub>Œ∏</sub></sub>(y<sub>l</sub></td> <td>t,s)]**</td> </tr> </tbody> </table> <p>Here:</p> <ul> <li><strong>w<sub>k</sub></strong>: Weights based on fallacy frequency in training data</li> <li><strong>P<sup>k</sup><sub>h<sub>Œ∏</sub></sub></strong>: Probability of fallacy type k from classification head</li> <li><strong>Œª = 0.3</strong>: Balancing parameter (optimized through experiments)</li> </ul> <h2 id="-the-13-types-of-logical-fallacies">üß† The 13 Types of Logical Fallacies</h2> <p>We define 13 categories of logical fallacies based on centuries of logical reasoning research dating back to Aristotle:</p> <h3 id="most-common-fallacies-in-our-dataset"><strong>Most Common Fallacies (in our dataset):</strong></h3> <ol> <li> <p><strong>Faulty Generalization (18.0%)</strong> - Drawing conclusions about all instances from limited examples</p> <ul> <li><em>Example: ‚ÄúI know someone who smoked cannabis and became successful. Therefore, everyone who smokes cannabis will be successful.‚Äù</em></li> </ul> </li> <li> <p><strong>Ad Hominem (12.3%)</strong> - Attacking the person making the argument rather than the argument itself</p> <ul> <li><em>Example: ‚ÄúThose climate scientists are just trying to get more funding for their research.‚Äù</em></li> </ul> </li> <li> <p><strong>Ad Populum (9.5%)</strong> - Argument based on what the majority believes</p> <ul> <li><em>Example: ‚ÄúMost people think this policy is good, so it must be correct.‚Äù</em></li> </ul> </li> <li> <p><strong>False Causality (8.8%)</strong> - Implying causal relationships without supporting evidence</p> <ul> <li><em>Example: ‚ÄúEver since we installed those wind turbines, there have been more bird deaths in the area.‚Äù</em></li> </ul> </li> <li> <p><strong>Circular Reasoning (7.0%)</strong> - The conclusion comes back to the premise without proving itself</p> <ul> <li><em>Example: ‚ÄúWe should trust the news because the news says we should trust it.‚Äù</em></li> </ul> </li> </ol> <h3 id="other-fallacy-types"><strong>Other Fallacy Types:</strong></h3> <ul> <li><strong>Appeal to Emotion (6.8%)</strong> - Manipulating emotions rather than using logic</li> <li><strong>Fallacy of Relevance (6.6%)</strong> - Introducing irrelevant information</li> <li><strong>Fallacy of Logic (6.2%)</strong> - Errors in logical structure</li> <li><strong>Intentional (5.8%)</strong> - Deliberately wrong arguments</li> <li><strong>False Dilemma (5.8%)</strong> - Presenting only two options when many exist</li> <li><strong>Fallacy of Extension (5.8%)</strong> - Attacking exaggerated versions of arguments</li> <li><strong>Fallacy of Credibility (5.4%)</strong> - Attacking speaker‚Äôs character</li> <li><strong>Equivocation (2.0%)</strong> - Using ambiguous language</li> </ul> <p><img src="https://via.placeholder.com/600x400/4CAF50/FFFFFF?text=Fallacy+Type+Distribution+Based+on+LOGIC+Dataset" alt="Fallacy Distribution"/></p> <h2 id="-methodology-4-step-framework">üî¨ Methodology: 4-Step Framework</h2> <p>Our approach involves four key steps:</p> <h3 id="step-1-supervised-fine-tuning-sft"><strong>Step 1: Supervised Fine-Tuning (SFT)</strong></h3> <p>Train the base model on the ExplaGraphs dataset containing topics, stances, and arguments.</p> <h3 id="step-2-preference-data-collection"><strong>Step 2: Preference Data Collection</strong></h3> <p>Generate fallacious arguments using ChatGPT for each original argument, creating preference pairs where:</p> <ul> <li><strong>Preferred (y<sub>w</sub>)</strong>: Original logically sound argument</li> <li><strong>Dispreferred (y<sub>l</sub>)</strong>: Generated fallacious argument with label k</li> </ul> <p>We generated <strong>7,872 fallacious arguments</strong> spanning all 13 fallacy types following the natural distribution found in real-world discourse.</p> <h3 id="step-3-preference-optimization"><strong>Step 3: Preference Optimization</strong></h3> <p>Apply standard preference optimization methods (DPO, PPO, CPO, KTO) using the preference dataset.</p> <h3 id="step-4-fipo-enhancement"><strong>Step 4: FIPO Enhancement</strong></h3> <p>Add our weighted classification loss to the best-performing preference method (CPO) to create FIPO.</p> <h2 id="-outstanding-results">üìä Outstanding Results</h2> <h3 id="dramatic-fallacy-reduction"><strong>Dramatic Fallacy Reduction</strong></h3> <table> <thead> <tr> <th>Model</th> <th>Baseline (SFT)</th> <th>Best Previous Method</th> <th><strong>FIPO</strong></th> <th><strong>Improvement</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Llama-2 (7B)</strong></td> <td>34.5%</td> <td>26.0% (PPO)</td> <td><strong>17.0%</strong></td> <td><strong>17.5% reduction</strong></td> </tr> <tr> <td><strong>Mistral (7B)</strong></td> <td>32.5%</td> <td>27.75% (KTO)</td> <td><strong>19.5%</strong></td> <td><strong>13.0% reduction</strong></td> </tr> </tbody> </table> <h3 id="quality-improvements-win-rate"><strong>Quality Improvements (Win-Rate)</strong></h3> <p>FIPO not only reduces fallacies but also maintains high argument quality:</p> <ul> <li><strong>Human Evaluation Win-Rate</strong>: 46% (vs. 40.3% loss rate for CPO)</li> <li><strong>GPT-4 Evaluation Win-Rate</strong>: 63.5% (highest among all methods)</li> <li><strong>Significant reduction in ‚Äúloss rate‚Äù</strong>: From 40.3% (CPO) to 23% (FIPO)</li> </ul> <h3 id="fallacy-specific-performance"><strong>Fallacy-Specific Performance</strong></h3> <p>FIPO excels particularly at reducing the most common fallacy types:</p> <table> <thead> <tr> <th>Fallacy Type</th> <th>Llama-2 SFT</th> <th>Llama-2 FIPO</th> <th><strong>Reduction</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Faulty Generalization</strong></td> <td>27.5%</td> <td><strong>7.0%</strong></td> <td><strong>-20.5%</strong></td> </tr> <tr> <td><strong>False Causality</strong></td> <td>2.5%</td> <td><strong>3.5%</strong></td> <td>Controlled</td> </tr> <tr> <td><strong>Appeal to Emotion</strong></td> <td>1.0%</td> <td><strong>2.5%</strong></td> <td>Controlled</td> </tr> </tbody> </table> <p>The weighted classification loss ensures the model focuses on the most problematic and frequent fallacies.</p> <h2 id="-human-evaluation-validation">üîç Human Evaluation Validation</h2> <h3 id="gpt-4-reliability-verification"><strong>GPT-4 Reliability Verification</strong></h3> <p>We validated GPT-4‚Äôs ability to identify fallacies through human annotation:</p> <ul> <li><strong>Randolph‚Äôs Œ∫ agreement</strong>: 0.640 (substantial agreement)</li> <li><strong>Majority agreement ratio</strong>: 95.5% between annotators and GPT-4</li> </ul> <h3 id="comparative-analysis"><strong>Comparative Analysis</strong></h3> <p>200 arguments were independently classified by our team and compared with GPT-4:</p> <ul> <li>Strong alignment on most fallacy types</li> <li>Main discrepancy: Fallacy of Relevance detection</li> <li>Some confusion between Faulty Generalization and False Causality (expected due to subtle differences)</li> </ul> <h2 id="-ablation-studies-confirm-design-choices">üß™ Ablation Studies Confirm Design Choices</h2> <p>We validated our design through rigorous ablation studies:</p> <table> <thead> <tr> <th>Approach</th> <th>Fallacy Rate</th> <th><strong>Analysis</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Uniform Dataset Distribution</strong></td> <td>37.5%</td> <td>Natural distribution is crucial</td> </tr> <tr> <td><strong>Unweighted Cross-Entropy</strong></td> <td>29.0%</td> <td>Weighting by frequency is essential</td> </tr> <tr> <td><strong>FIPO (Full Method)</strong></td> <td><strong>17.0%</strong></td> <td><strong>Best performance</strong></td> </tr> </tbody> </table> <p>These results confirm that both the <strong>natural fallacy distribution</strong> and <strong>weighted classification loss</strong> are essential components of FIPO.</p> <h2 id="-generalization-out-of-domain-performance">üåç Generalization: Out-of-Domain Performance</h2> <p>FIPO‚Äôs effectiveness extends beyond training data:</p> <p><strong>Debatepedia Dataset Results:</strong></p> <ul> <li><strong>Fallacy Rate</strong>: 45% (second-best, close to KTO‚Äôs 44%)</li> <li><strong>Win Rate</strong>: 62% (highest among all methods)</li> <li><strong>Excellent at reducing</strong>: False Causality and Fallacy of Relevance</li> </ul> <p>This demonstrates FIPO‚Äôs ability to generalize to new domains and topics.</p> <h2 id="-implementation-details">üíª Implementation Details</h2> <h3 id="base-models"><strong>Base Models</strong></h3> <ul> <li><strong>Llama-2 (7B)</strong> and <strong>Mistral (7B)</strong></li> <li><strong>LoRA fine-tuning</strong>: Reduces parameters from 7B to 8.3M (0.12%)</li> </ul> <h3 id="training-configuration"><strong>Training Configuration</strong></h3> <ul> <li><strong>Classification loss weight (Œª)</strong>: 0.3 (optimal balance)</li> <li><strong>Fallacy weights (w<sub>k</sub>)</strong>: Based on natural frequency distribution</li> <li><strong>Base method</strong>: CPO (best trade-off between win-rate and fallacy-rate)</li> </ul> <h3 id="key-hyperparameters"><strong>Key Hyperparameters</strong></h3> <ul> <li><strong>Learning rate</strong>: 2√ó10‚Åª‚Å¥</li> <li><strong>LoRA rank</strong>: 16, Œ± = 32</li> <li><strong>Training epochs</strong>: 3</li> <li><strong>Œ≤ (CPO)</strong>: 0.1</li> </ul> <h2 id="-impact-and-future-directions">üîÆ Impact and Future Directions</h2> <h3 id="immediate-impact"><strong>Immediate Impact</strong></h3> <ul> <li><strong>First work</strong> to systematically address logical fallacies in argument generation</li> <li><strong>Novel framework</strong> combining preference optimization with classification objectives</li> <li><strong>Significant performance gains</strong> across multiple models and evaluation metrics</li> </ul> <h3 id="broader-implications"><strong>Broader Implications</strong></h3> <ul> <li><strong>Trust and Safety</strong>: Reduces spread of logically flawed arguments</li> <li><strong>Educational Applications</strong>: Could help teach logical reasoning</li> <li><strong>Democratic Discourse</strong>: Promotes more sound public debate</li> </ul> <h3 id="future-research-directions"><strong>Future Research Directions</strong></h3> <ol> <li><strong>Scaling to larger models</strong>: Apply FIPO to models &gt;10B parameters</li> <li><strong>Multi-domain extension</strong>: Expand to scientific, legal, and technical arguments</li> <li><strong>Real-time fallacy detection</strong>: Interactive systems for argument evaluation</li> <li><strong>Cross-lingual fallacy analysis</strong>: Extend to non-English languages</li> <li><strong>Integration with fact-checking</strong>: Combine logical and factual verification</li> </ol> <h2 id="-key-takeaways">üéØ Key Takeaways</h2> <ol> <li><strong>LLMs have a significant logical fallacy problem</strong>: Even advanced models like ChatGPT generate fallacious arguments 21% of the time</li> <li><strong>Explicit fallacy awareness helps</strong>: Making models aware of specific fallacy types significantly improves argument quality</li> <li><strong>Weighted classification loss is crucial</strong>: Focusing on frequent fallacies yields better overall performance</li> <li><strong>Preference optimization isn‚Äôt enough alone</strong>: Standard methods like DPO and PPO help but miss fine-grained fallacy distinctions</li> <li><strong>FIPO provides substantial improvements</strong>: Up to 17.5% reduction in fallacy rates while maintaining argument quality</li> </ol> <h2 id="-resources-and-code">üìö Resources and Code</h2> <ul> <li><strong>üìÑ Paper</strong>: <a href="https://aclanthology.org/2025.naacl-long.374.pdf">A Logical Fallacy-Informed Framework for Argument Generation</a></li> <li><strong>üíª Code &amp; Data</strong>: Available at <a href="https://github.com/lucamouchel/Logical-Fallacies">github.com/lucamouchel/Logical-Fallacies</a></li> <li><strong>üèÜ Award Recognition</strong>: Outstanding Paper Award - NAACL 2025</li> </ul> <hr/> <h2 id="-acknowledgments">üë• Acknowledgments</h2> <p>This work was a collaborative effort with amazing researchers:</p> <ul> <li><strong>Luca Mouchel</strong> (Lead Author, Master Internship)</li> <li><strong>Shaobo Cui</strong>, <strong>Robert West</strong>, <strong>Antoine Bosselut</strong>, <strong>Boi Faltings</strong></li> <li><strong>EPFL, Switzerland</strong></li> </ul> <p>Special thanks to the <strong>ICT-48 Network of AI Research Excellence Center ‚ÄúTAILOR‚Äù</strong>, the <strong>Swiss National Science Foundation</strong>, and our other funding partners.</p> <hr/> <h2 id="-citation">üìñ Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">mouchel-etal-2025-logical</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">"A Logical Fallacy-Informed Framework for Argument Generation"</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">"Mouchel, Luca and Paul, Debjit and Cui, Shaobo and West, Robert and Bosselut, Antoine and Faltings, Boi"</span><span class="p">,</span>
    <span class="na">booktitle</span> <span class="p">=</span> <span class="s">"Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)"</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">"2025"</span><span class="p">,</span>
    <span class="na">pages</span> <span class="p">=</span> <span class="s">"7296--7314"</span><span class="p">,</span>
    <span class="na">publisher</span> <span class="p">=</span> <span class="s">"Association for Computational Linguistics"</span>
<span class="p">}</span>
</code></pre></div></div> <hr/> <h2 id="-discussion-questions">ü§î Discussion Questions</h2> <p><strong>For Researchers:</strong></p> <ul> <li>How might FIPO be adapted for other types of reasoning tasks beyond argument generation?</li> <li>What other fine-grained classification objectives could improve preference optimization?</li> </ul> <p><strong>For Practitioners:</strong></p> <ul> <li>How can we integrate fallacy detection into real-world applications like social media platforms or educational tools?</li> <li>What are the computational trade-offs when adding classification heads to large language models?</li> </ul> <p><strong>For Society:</strong></p> <ul> <li>How do we balance improving logical reasoning with maintaining diverse perspectives in AI-generated content?</li> <li>What role should AI play in moderating online discourse and identifying fallacious arguments?</li> </ul> <hr/> <p><em>This research represents a significant step toward more trustworthy and logically sound AI systems. As LLMs become increasingly prevalent in decision-making and public discourse, ensuring they generate well-reasoned arguments is not just a technical challenge‚Äîit‚Äôs a societal imperative.</em></p> <p><strong>üèÜ Proud to have this work recognized with the Outstanding Paper Award at NAACL 2025!</strong></p>]]></content><author><name></name></author><category term="research"/><category term="awards"/><category term="fipo"/><category term="logical-fallacies"/><category term="preference-optimization"/><category term="argument-generation"/><category term="outstanding-paper"/><summary type="html"><![CDATA[üèÜ Outstanding Paper Award Winner at NAACL 2025! Introducing FIPO, a novel framework that reduces logical fallacy errors in LLM-generated arguments by up to 17.5% through fallacy-informed preference optimization.]]></summary></entry><entry><title type="html">Making Reasoning Matter: Measuring Faithfulness in Chain-of-Thought Reasoning</title><link href="https://debjitpaul.github.io/blog/2024/reasoning-matter/" rel="alternate" type="text/html" title="Making Reasoning Matter: Measuring Faithfulness in Chain-of-Thought Reasoning"/><published>2024-02-23T00:00:00+00:00</published><updated>2024-02-23T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2024/reasoning-matter</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2024/reasoning-matter/"><![CDATA[<p>Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks when prompted to generate step-by-step explanations, known as Chain-of-Thought (CoT) reasoning. However, a critical question remains: <strong>Do these models actually use their generated reasoning steps to arrive at their final answers?</strong></p> <h2 id="the-problem">The Problem</h2> <p>While CoT prompting improves performance on many reasoning tasks, recent studies suggest that models might not always rely on their intermediate reasoning steps. This raises concerns about the <strong>faithfulness</strong> of the reasoning process - whether the generated explanations truly reflect the model‚Äôs decision-making process.</p> <h2 id="our-approach">Our Approach</h2> <p>In our paper <a href="https://arxiv.org/abs/2402.13950">‚ÄúMaking Reasoning Matter‚Äù</a>, we introduce a comprehensive framework to measure and improve reasoning faithfulness using <strong>causal mediation analysis</strong>.</p> <h3 id="key-contributions">Key Contributions</h3> <ol> <li><strong>Causal Framework</strong>: We develop a method to quantify how much intermediate reasoning steps causally influence final predictions</li> <li><strong>Extensive Evaluation</strong>: Analysis across 11 different language models on multiple reasoning tasks</li> <li><strong>Practical Improvements</strong>: Techniques to enhance reasoning faithfulness</li> </ol> <h2 id="key-findings">Key Findings</h2> <p>Our causal mediation analysis across different model families reveals several important insights:</p> <h3 id="model-behavior-varies-by-training-objective">Model Behavior Varies by Training Objective</h3> <ul> <li><strong>In-context learning</strong> and <strong>instruction-tuning</strong> improve alignment with reasoning chains</li> <li>Models trained with <strong>RLHF</strong> show more direct effects than indirect effects, suggesting potential issues with faithful reasoning</li> <li><strong>Larger models</strong> don‚Äôt automatically show better faithfulness</li> </ul> <h3 id="task-specific-patterns">Task-Specific Patterns</h3> <p>We evaluated on three types of reasoning tasks:</p> <ul> <li><strong>Mathematical Reasoning</strong> (GSM8K)</li> <li><strong>Strategic Reasoning</strong> (StrategyQA)</li> <li><strong>Causal Understanding</strong></li> </ul> <p>Results show that faithfulness varies significantly across task types, with mathematical reasoning showing different patterns compared to strategic reasoning tasks.</p> <h2 id="methodology">Methodology</h2> <p>Our approach uses <strong>causal mediation analysis</strong> to decompose the total effect of reasoning problems into:</p> <ol> <li><strong>Direct Effect</strong>: How much the problem directly influences the answer (bypassing reasoning)</li> <li><strong>Indirect Effect</strong>: How much the problem influences the answer through generated reasoning steps</li> </ol> <p>High indirect effect indicates faithful reasoning, while high direct effect suggests the model might be ignoring its reasoning steps.</p> <h2 id="implications">Implications</h2> <p>This work has important implications for:</p> <ul> <li><strong>Model Development</strong>: Understanding which training objectives promote faithful reasoning</li> <li><strong>Evaluation</strong>: Moving beyond accuracy to assess reasoning quality</li> <li><strong>Trust and Interpretability</strong>: Building more reliable and transparent AI systems</li> </ul> <h2 id="future-directions">Future Directions</h2> <p>Our findings open several avenues for future research:</p> <ul> <li>Developing training methods that promote faithfulness</li> <li>Creating better evaluation metrics for reasoning quality</li> <li>Understanding the relationship between model scale and reasoning faithfulness</li> </ul> <hr/> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">debjit2024frodo</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul and Robert West and Antoine Bosselut and Boi Faltings}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13950}</span><span class="p">,</span>
    <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
    <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Links:</strong></p> <ul> <li><a href="https://arxiv.org/abs/2402.13950">Paper on arXiv</a></li> <li><a href="/reasoningmatter/">Project Page</a></li> <li><a href="https://github.com/debjitpaul/reasoning-matter">Code Repository</a> <em>(if available)</em></li> </ul> <hr/> <p><em>This post is part of our ongoing research into making AI reasoning more transparent and reliable. For more updates on our work, follow our <a href="/blog/">research blog</a>.</em></p>]]></content><author><name></name></author><category term="research"/><category term="reasoning"/><category term="chain-of-thought"/><category term="LLMs"/><category term="faithfulness"/><category term="causal-analysis"/><summary type="html"><![CDATA[A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.]]></summary></entry><entry><title type="html">blog</title><link href="https://debjitpaul.github.io/blog/" rel="alternate" type="text/html" title="blog"/><published>2023-04-04T00:00:00+00:00</published><updated>2023-04-04T00:00:00+00:00</updated><id>https://debjitpaul.github.io/refiner</id><content type="html" xml:base="https://debjitpaul.github.io/blog/"><![CDATA[<div class="post"> <div class="header-bar"> <h1>Research Blog</h1> <h2>Blog posts on AI and NLP Research Topics</h2> </div> <div class="container featured-research" style="margin-bottom: 2rem;"> <h3 style="margin-bottom: 1rem; color: var(--global-theme-color);">üî¨ Featured Research Projects</h3> <div class="row mb-4"> <div class="col-md-6 mb-4"> <div class="card hoverable h-100"> <div class="card-body"> <h4 class="card-title">Making Reasoning Matter</h4> <p class="card-text">Measuring and improving faithfulness of chain-of-thought reasoning in large language models using causal mediation analysis.</p> <div class="research-highlights mb-3"> <h6>Key Contributions:</h6> <ul class="list-unstyled"> <li><i class="fas fa-check-circle text-success"></i> Causal framework for measuring reasoning faithfulness</li> <li><i class="fas fa-check-circle text-success"></i> Analysis across 11 different language models</li> <li><i class="fas fa-check-circle text-success"></i> Insights on training objectives and reasoning quality</li> </ul> </div> <div class="research-meta mb-2"> <span class="badge badge-primary">arXiv:2402.13950</span> <span class="badge badge-secondary">Chain-of-Thought</span> <span class="badge badge-secondary">Causal Analysis</span> </div> <div class="mt-3"> <a href="/reasoningmatter/" class="btn btn-primary btn-sm"> <i class="fas fa-info-circle"></i> Project Details </a> <a href="https://arxiv.org/abs/2402.13950" target="_blank" class="btn btn-outline-secondary btn-sm"> <i class="fas fa-external-link-alt"></i> Read Paper </a> </div> </div> </div> </div> <div class="col-md-6 mb-4"> <div class="card hoverable h-100"> <div class="card-body"> <h4 class="card-title">REFINER</h4> <p class="card-text">A framework for improving language models' reasoning through iterative feedback from critic models on intermediate representations.</p> <div class="research-highlights mb-3"> <h6>Key Features:</h6> <ul class="list-unstyled"> <li><i class="fas fa-check-circle text-success"></i> Generator-Critic architecture for iterative refinement</li> <li><i class="fas fa-check-circle text-success"></i> Structured feedback on reasoning steps</li> <li><i class="fas fa-check-circle text-success"></i> Human-compatible feedback integration</li> </ul> </div> <div class="research-meta mb-2"> <span class="badge badge-primary">arXiv:2304.01904</span> <span class="badge badge-secondary">Feedback Learning</span> <span class="badge badge-secondary">Reasoning</span> </div> <div class="mt-3"> <a href="/refiner/" class="btn btn-primary btn-sm"> <i class="fas fa-info-circle"></i> Project Details </a> <a href="https://arxiv.org/abs/2304.01904" target="_blank" class="btn btn-outline-secondary btn-sm"> <i class="fas fa-external-link-alt"></i> Read Paper </a> </div> </div> </div> </div> </div> <div class="row"> <div class="col-md-12"> <div class="card hoverable"> <div class="card-body"> <h5 class="card-title">üß† Research Focus Areas</h5> <div class="row"> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-brain text-primary"></i> AI Reasoning</h6> <p class="small text-muted">Chain-of-thought reasoning, causal inference, knowledge representation, and logical reasoning processes in AI systems.</p> </div> </div> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-comments text-info"></i> Feedback &amp; Learning</h6> <p class="small text-muted">Iterative feedback mechanisms, critic models, human-AI interaction, and preference optimization for better model alignment.</p> </div> </div> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-search text-success"></i> Model Faithfulness</h6> <p class="small text-muted">Measuring and improving the reliability, interpretability, and trustworthiness of AI decision-making processes.</p> </div> </div> </div> </div> </div> </div> </div> </div> <hr style="margin: 2rem 0;"/> <div class="tag-category-list mb-4"> <p><strong>Browse by topics:</strong></p> <ul class="p-0 m-0" style="display: flex; flex-wrap: wrap; list-style: none;"> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/moe" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> moe </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/gpt-oss" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> gpt-oss </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/transformers" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> transformers </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/faithfulness" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> faithfulness </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/code" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> code </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/causal-analysis" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> causal-analysis </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/chain-of-thought" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> chain-of-thought </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/category/external-services" class="badge badge-info"> <i class="fas fa-tag fa-sm"></i> external-services </a> </li> </ul> </div> <div class="container featured-posts mb-4"> <h4 style="margin-bottom: 1rem;">üìå Featured Posts</h4> <div class="row row-cols-2"> <div class="col mb-4"> <a href="/blog/2025/world-model/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">What Children Know That AI Doesn't: Learning Through Experience, Not Text</h5> <p class="card-text">Jean Piaget revealed how children construct understanding through action, surprise, and adaptation. His insights expose fundamental gaps in current AI‚Äîsystems that predict words, not reality. But a new paradigm is emerging.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 16 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> October 01, 2025 </p> </div> </div> </a> </div> <div class="col mb-4"> <a href="/blog/2025/compute/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">Understanding FLOPs, MFU, and Computational Efficiency in LLM Training: From Dense Transformers to MoE Architectures</h5> <p class="card-text">A comprehensive guide to counting FLOPs in LLM training, measuring Model FLOPs Utilization (MFU), and extending these concepts to Mixture-of-Experts architectures with a deep dive into OpenAI's GPT-OSS models.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 15 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> September 28, 2025 </p> </div> </div> </a> </div> <div class="col mb-4"> <a href="/blog/2025/fipo-outstanding-paper-award/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">FIPO: Fallacy-Informed Preference Optimization - Steering LLMs Toward Logically Sound Arguments</h5> <p class="card-text">üèÜ Outstanding Paper Award Winner at NAACL 2025! Introducing FIPO, a novel framework that reduces logical fallacy errors in LLM-generated arguments by up to 17.5% through fallacy-informed preference optimization.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 10 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> January 15, 2025 </p> </div> </div> </a> </div> <div class="col mb-4"> <a href="/blog/2024/reasoning-matter/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">Making Reasoning Matter: Measuring Faithfulness in Chain-of-Thought Reasoning</h5> <p class="card-text">A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 3 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> February 23, 2024 </p> </div> </div> </a> </div> </div> </div> <hr/> <h4 style="margin-bottom: 1rem;">üìù Recent Posts</h4> <ul class="post-list"> </ul> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Blog posts on AI and NLP Research Topics]]></summary></entry></feed>