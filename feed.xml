<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://debjitpaul.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debjitpaul.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-28T10:16:25+00:00</updated><id>https://debjitpaul.github.io/feed.xml</id><title type="html">blank</title><subtitle>Updating it, Work in Progress, My personal website</subtitle><entry><title type="html">Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning</title><link href="https://debjitpaul.github.io/blog/2024/reasoning-matter/" rel="alternate" type="text/html" title="Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning"/><published>2024-02-23T00:00:00+00:00</published><updated>2024-02-23T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2024/reasoning-matter</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2024/reasoning-matter/"><![CDATA[<h1 id="making-reasoning-matter-measuring-and-improving-faithfulness-of-chain-of-thought-reasoning">Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</h1> <p>Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks when prompted to generate step-by-step explanations, known as Chain-of-Thought (CoT) reasoning. However, a critical question remains: <strong>Do these models actually use their generated reasoning steps to arrive at their final answers?</strong></p> <h2 id="the-problem">The Problem</h2> <p>While CoT prompting improves performance on many reasoning tasks, recent studies suggest that models might not always rely on their intermediate reasoning steps. This raises concerns about the <strong>faithfulness</strong> of the reasoning process - whether the generated explanations truly reflect the model‚Äôs decision-making process.</p> <h2 id="our-approach">Our Approach</h2> <p>In our paper <a href="https://arxiv.org/abs/2402.13950">‚ÄúMaking Reasoning Matter‚Äù</a>, we introduce a comprehensive framework to measure and improve reasoning faithfulness using <strong>causal mediation analysis</strong>.</p> <h3 id="key-contributions">Key Contributions</h3> <ol> <li><strong>Causal Framework</strong>: We develop a method to quantify how much intermediate reasoning steps causally influence final predictions</li> <li><strong>Extensive Evaluation</strong>: Analysis across 11 different language models on multiple reasoning tasks</li> <li><strong>Practical Improvements</strong>: Techniques to enhance reasoning faithfulness</li> </ol> <h2 id="key-findings">Key Findings</h2> <p>Our causal mediation analysis across different model families reveals several important insights:</p> <h3 id="model-behavior-varies-by-training-objective">Model Behavior Varies by Training Objective</h3> <ul> <li><strong>In-context learning</strong> and <strong>instruction-tuning</strong> improve alignment with reasoning chains</li> <li>Models trained with <strong>RLHF</strong> show more direct effects than indirect effects, suggesting potential issues with faithful reasoning</li> <li><strong>Larger models</strong> don‚Äôt automatically show better faithfulness</li> </ul> <h3 id="task-specific-patterns">Task-Specific Patterns</h3> <p>We evaluated on three types of reasoning tasks:</p> <ul> <li><strong>Mathematical Reasoning</strong> (GSM8K)</li> <li><strong>Strategic Reasoning</strong> (StrategyQA)</li> <li><strong>Causal Understanding</strong></li> </ul> <p>Results show that faithfulness varies significantly across task types, with mathematical reasoning showing different patterns compared to strategic reasoning tasks.</p> <h2 id="methodology">Methodology</h2> <p>Our approach uses <strong>causal mediation analysis</strong> to decompose the total effect of reasoning problems into:</p> <ol> <li><strong>Direct Effect</strong>: How much the problem directly influences the answer (bypassing reasoning)</li> <li><strong>Indirect Effect</strong>: How much the problem influences the answer through generated reasoning steps</li> </ol> <p>High indirect effect indicates faithful reasoning, while high direct effect suggests the model might be ignoring its reasoning steps.</p> <h2 id="implications">Implications</h2> <p>This work has important implications for:</p> <ul> <li><strong>Model Development</strong>: Understanding which training objectives promote faithful reasoning</li> <li><strong>Evaluation</strong>: Moving beyond accuracy to assess reasoning quality</li> <li><strong>Trust and Interpretability</strong>: Building more reliable and transparent AI systems</li> </ul> <h2 id="future-directions">Future Directions</h2> <p>Our findings open several avenues for future research:</p> <ul> <li>Developing training methods that promote faithfulness</li> <li>Creating better evaluation metrics for reasoning quality</li> <li>Understanding the relationship between model scale and reasoning faithfulness</li> </ul> <hr/> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">debjit2024frodo</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul and Robert West and Antoine Bosselut and Boi Faltings}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13950}</span><span class="p">,</span>
    <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
    <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Links:</strong></p> <ul> <li><a href="https://arxiv.org/abs/2402.13950">Paper on arXiv</a></li> <li><a href="/reasoningmatter/">Project Page</a></li> <li><a href="https://github.com/debjitpaul/reasoning-matter">Code Repository</a> <em>(if available)</em></li> </ul> <hr/> <p><em>This post is part of our ongoing research into making AI reasoning more transparent and reliable. For more updates on our work, follow our <a href="/blog/">research blog</a>.</em></p>]]></content><author><name></name></author><category term="research"/><category term="reasoning"/><category term="chain-of-thought"/><category term="LLMs"/><category term="faithfulness"/><category term="causal-analysis"/><summary type="html"><![CDATA[A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.]]></summary></entry><entry><title type="html">blog</title><link href="https://debjitpaul.github.io/blog/" rel="alternate" type="text/html" title="blog"/><published>2023-04-04T00:00:00+00:00</published><updated>2023-04-04T00:00:00+00:00</updated><id>https://debjitpaul.github.io/refiner</id><content type="html" xml:base="https://debjitpaul.github.io/blog/"><![CDATA[<div class="post"> <div class="header-bar"> <h1>Research Blog</h1> <h2>Research updates, insights, and thoughts on AI reasoning and computational linguistics</h2> </div> <div class="container featured-research" style="margin-bottom: 2rem;"> <h3 style="margin-bottom: 1rem; color: var(--global-theme-color);">üî¨ Featured Research Projects</h3> <div class="row mb-4"> <div class="col-md-6 mb-4"> <div class="card hoverable h-100"> <div class="card-body"> <h4 class="card-title">Making Reasoning Matter</h4> <p class="card-text">Measuring and improving faithfulness of chain-of-thought reasoning in large language models using causal mediation analysis.</p> <div class="research-highlights mb-3"> <h6>Key Contributions:</h6> <ul class="list-unstyled"> <li><i class="fas fa-check-circle text-success"></i> Causal framework for measuring reasoning faithfulness</li> <li><i class="fas fa-check-circle text-success"></i> Analysis across 11 different language models</li> <li><i class="fas fa-check-circle text-success"></i> Insights on training objectives and reasoning quality</li> </ul> </div> <div class="research-meta mb-2"> <span class="badge badge-primary">arXiv:2402.13950</span> <span class="badge badge-secondary">Chain-of-Thought</span> <span class="badge badge-secondary">Causal Analysis</span> </div> <div class="mt-3"> <a href="/reasoningmatter/" class="btn btn-primary btn-sm"> <i class="fas fa-info-circle"></i> Project Details </a> <a href="https://arxiv.org/abs/2402.13950" target="_blank" class="btn btn-outline-secondary btn-sm"> <i class="fas fa-external-link-alt"></i> Read Paper </a> </div> </div> </div> </div> <div class="col-md-6 mb-4"> <div class="card hoverable h-100"> <div class="card-body"> <h4 class="card-title">REFINER</h4> <p class="card-text">A framework for improving language models' reasoning through iterative feedback from critic models on intermediate representations.</p> <div class="research-highlights mb-3"> <h6>Key Features:</h6> <ul class="list-unstyled"> <li><i class="fas fa-check-circle text-success"></i> Generator-Critic architecture for iterative refinement</li> <li><i class="fas fa-check-circle text-success"></i> Structured feedback on reasoning steps</li> <li><i class="fas fa-check-circle text-success"></i> Human-compatible feedback integration</li> </ul> </div> <div class="research-meta mb-2"> <span class="badge badge-primary">arXiv:2304.01904</span> <span class="badge badge-secondary">Feedback Learning</span> <span class="badge badge-secondary">Reasoning</span> </div> <div class="mt-3"> <a href="/refiner/" class="btn btn-primary btn-sm"> <i class="fas fa-info-circle"></i> Project Details </a> <a href="https://arxiv.org/abs/2304.01904" target="_blank" class="btn btn-outline-secondary btn-sm"> <i class="fas fa-external-link-alt"></i> Read Paper </a> </div> </div> </div> </div> </div> <div class="row"> <div class="col-md-12"> <div class="card hoverable"> <div class="card-body"> <h5 class="card-title">üß† Research Focus Areas</h5> <div class="row"> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-brain text-primary"></i> AI Reasoning</h6> <p class="small text-muted">Chain-of-thought reasoning, causal inference, knowledge representation, and logical reasoning processes in AI systems.</p> </div> </div> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-comments text-info"></i> Feedback &amp; Learning</h6> <p class="small text-muted">Iterative feedback mechanisms, critic models, human-AI interaction, and preference optimization for better model alignment.</p> </div> </div> <div class="col-md-4 mb-3"> <div class="research-area"> <h6><i class="fas fa-search text-success"></i> Model Faithfulness</h6> <p class="small text-muted">Measuring and improving the reliability, interpretability, and trustworthiness of AI decision-making processes.</p> </div> </div> </div> </div> </div> </div> </div> </div> <hr style="margin: 2rem 0;"/> <div class="tag-category-list mb-4"> <p><strong>Browse by topics:</strong></p> <ul class="p-0 m-0" style="display: flex; flex-wrap: wrap; list-style: none;"> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/formatting" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> formatting </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/images" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> images </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/links" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> links </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/math" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> math </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/code" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> code </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/tag/blockquotes" class="badge badge-light"> <i class="fas fa-hashtag fa-sm"></i> blockquotes </a> </li> <li style="margin-right: 15px; margin-bottom: 5px;"> <a href="/blog/category/external-services" class="badge badge-info"> <i class="fas fa-tag fa-sm"></i> external-services </a> </li> </ul> </div> <div class="container featured-posts mb-4"> <h4 style="margin-bottom: 1rem;">üìå Featured Posts</h4> <div class="row row-cols-2"> <div class="col mb-4"> <a href="/blog/2024/reasoning-matter/" style="text-decoration: none;"> <div class="card hoverable h-100"> <div class="card-body"> <div class="float-right"> <i class="fas fa-thumbtack fa-xs text-muted"></i> </div> <h5 class="card-title">Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning</h5> <p class="card-text">A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.</p> <p class="post-meta text-muted small"> <i class="fas fa-clock"></i> 3 min read &nbsp; ‚Ä¢ &nbsp; <i class="fas fa-calendar"></i> February 23, 2024 </p> </div> </div> </a> </div> </div> </div> <hr/> <h4 style="margin-bottom: 1rem;">üìù Recent Posts</h4> <ul class="post-list"> </ul> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Research updates, insights, and thoughts on AI reasoning and computational linguistics]]></summary></entry></feed>