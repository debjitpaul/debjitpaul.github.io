<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://debjitpaul.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://debjitpaul.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-27T21:46:01+00:00</updated><id>https://debjitpaul.github.io/feed.xml</id><title type="html">blank</title><subtitle>Updating it, Work in Progress, My personal website</subtitle><entry><title type="html">Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning</title><link href="https://debjitpaul.github.io/blog/2024/reasoning-matter/" rel="alternate" type="text/html" title="Making Reasoning Matter - Measuring Faithfulness in Chain-of-Thought Reasoning"/><published>2024-02-23T00:00:00+00:00</published><updated>2024-02-23T00:00:00+00:00</updated><id>https://debjitpaul.github.io/blog/2024/reasoning-matter</id><content type="html" xml:base="https://debjitpaul.github.io/blog/2024/reasoning-matter/"><![CDATA[<h1 id="making-reasoning-matter-measuring-and-improving-faithfulness-of-chain-of-thought-reasoning">Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning</h1> <p>Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks when prompted to generate step-by-step explanations, known as Chain-of-Thought (CoT) reasoning. However, a critical question remains: <strong>Do these models actually use their generated reasoning steps to arrive at their final answers?</strong></p> <h2 id="the-problem">The Problem</h2> <p>While CoT prompting improves performance on many reasoning tasks, recent studies suggest that models might not always rely on their intermediate reasoning steps. This raises concerns about the <strong>faithfulness</strong> of the reasoning process - whether the generated explanations truly reflect the model’s decision-making process.</p> <h2 id="our-approach">Our Approach</h2> <p>In our paper <a href="https://arxiv.org/abs/2402.13950">“Making Reasoning Matter”</a>, we introduce a comprehensive framework to measure and improve reasoning faithfulness using <strong>causal mediation analysis</strong>.</p> <h3 id="key-contributions">Key Contributions</h3> <ol> <li><strong>Causal Framework</strong>: We develop a method to quantify how much intermediate reasoning steps causally influence final predictions</li> <li><strong>Extensive Evaluation</strong>: Analysis across 11 different language models on multiple reasoning tasks</li> <li><strong>Practical Improvements</strong>: Techniques to enhance reasoning faithfulness</li> </ol> <h2 id="key-findings">Key Findings</h2> <p>Our causal mediation analysis across different model families reveals several important insights:</p> <h3 id="model-behavior-varies-by-training-objective">Model Behavior Varies by Training Objective</h3> <ul> <li><strong>In-context learning</strong> and <strong>instruction-tuning</strong> improve alignment with reasoning chains</li> <li>Models trained with <strong>RLHF</strong> show more direct effects than indirect effects, suggesting potential issues with faithful reasoning</li> <li><strong>Larger models</strong> don’t automatically show better faithfulness</li> </ul> <h3 id="task-specific-patterns">Task-Specific Patterns</h3> <p>We evaluated on three types of reasoning tasks:</p> <ul> <li><strong>Mathematical Reasoning</strong> (GSM8K)</li> <li><strong>Strategic Reasoning</strong> (StrategyQA)</li> <li><strong>Causal Understanding</strong></li> </ul> <p>Results show that faithfulness varies significantly across task types, with mathematical reasoning showing different patterns compared to strategic reasoning tasks.</p> <h2 id="methodology">Methodology</h2> <p>Our approach uses <strong>causal mediation analysis</strong> to decompose the total effect of reasoning problems into:</p> <ol> <li><strong>Direct Effect</strong>: How much the problem directly influences the answer (bypassing reasoning)</li> <li><strong>Indirect Effect</strong>: How much the problem influences the answer through generated reasoning steps</li> </ol> <p>High indirect effect indicates faithful reasoning, while high direct effect suggests the model might be ignoring its reasoning steps.</p> <h2 id="implications">Implications</h2> <p>This work has important implications for:</p> <ul> <li><strong>Model Development</strong>: Understanding which training objectives promote faithful reasoning</li> <li><strong>Evaluation</strong>: Moving beyond accuracy to assess reasoning quality</li> <li><strong>Trust and Interpretability</strong>: Building more reliable and transparent AI systems</li> </ul> <h2 id="future-directions">Future Directions</h2> <p>Our findings open several avenues for future research:</p> <ul> <li>Developing training methods that promote faithfulness</li> <li>Creating better evaluation metrics for reasoning quality</li> <li>Understanding the relationship between model scale and reasoning faithfulness</li> </ul> <hr/> <p><strong>Citation:</strong></p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">debjit2024frodo</span><span class="p">,</span>
    <span class="na">title</span><span class="p">=</span><span class="s">{Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning}</span><span class="p">,</span>
    <span class="na">author</span><span class="p">=</span><span class="s">{Debjit Paul and Robert West and Antoine Bosselut and Boi Faltings}</span><span class="p">,</span>
    <span class="na">year</span><span class="p">=</span><span class="s">{2024}</span><span class="p">,</span>
    <span class="na">eprint</span><span class="p">=</span><span class="s">{2402.13950}</span><span class="p">,</span>
    <span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span>
    <span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CL}</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Links:</strong></p> <ul> <li><a href="https://arxiv.org/abs/2402.13950">Paper on arXiv</a></li> <li><a href="/reasoningmatter/">Project Page</a></li> <li><a href="https://github.com/debjitpaul/reasoning-matter">Code Repository</a> <em>(if available)</em></li> </ul> <hr/> <p><em>This post is part of our ongoing research into making AI reasoning more transparent and reliable. For more updates on our work, follow our <a href="/blog/">research blog</a>.</em></p>]]></content><author><name></name></author><category term="research"/><category term="reasoning"/><category term="chain-of-thought"/><category term="LLMs"/><category term="faithfulness"/><category term="causal-analysis"/><summary type="html"><![CDATA[A deep dive into our latest research on evaluating and improving the faithfulness of chain-of-thought reasoning in large language models.]]></summary></entry></feed>